{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DogFaceNet: test file with MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import skimage as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = (28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing\n",
    "- Get the dataset from folders\n",
    "- Associate the corresponding classes\n",
    "- Resized the dataset\n",
    "- Shuffle the dataset?\n",
    "- Divide the dataset into validation, training and testing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "# x_train -= 127.5\n",
    "# x_test -= 127.5\n",
    "# x_train *= 1./128\n",
    "# x_test *= 1./128\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New method\n",
    "- We divide the validation and test set from the training set with the classic division method: 85 percent training, 10 validating, 5 testing.\n",
    "- We then computes pairs of images in the validation set and testing set:\n",
    " - Some of these pairs are images of the same dog and some are picture of different dogs\n",
    " - We create a two lists:\n",
    "  - A list a images containing the pairs: two successive images are a pair of images. For example, image 0 and is a pair, image 2 and 3 is another pair, etc...\n",
    "  - A list of boolean called 'issame' indicating if a pair is a pair of images of the same dog or a pair of different dogs. For example, if image 0 and image 1 are showing the same dog value 0 and 1 in the list will be True. On the other hand if the image 2 and 3 represent two different dogs the value 2 and 3 in the list will be at False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f1fcb60240>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sk.io.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the number from zero to six for training\n",
    "# and from seven to nine for testing\n",
    "keep_test = np.empty(len(y_test),dtype=bool)\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] in np.arange(0,8):\n",
    "        keep_test[i] = True\n",
    "    else:\n",
    "        keep_test[i] = False\n",
    "keep_train = np.empty(len(y_train),dtype=bool)\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] in np.arange(0,8):\n",
    "        keep_train[i] = True\n",
    "    else:\n",
    "        keep_train[i] = False\n",
    "    \n",
    "\n",
    "images_valid = np.vstack((x_train[np.logical_not(keep_train)],x_test[np.logical_not(keep_test)]))\n",
    "labels_valid = np.append(y_train[np.logical_not(keep_train)],y_test[np.logical_not(keep_test)])\n",
    "\n",
    "x_train = x_train[keep_train]\n",
    "y_train = y_train[keep_train]\n",
    "x_test = x_test[keep_test]\n",
    "y_test = y_test[keep_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs: 10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786f48a7b9fb4459b69da840e98309f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of same images: 2538\n",
      "Number of validation images: 13783\n"
     ]
    }
   ],
   "source": [
    "w, h = SIZE\n",
    "\n",
    "# print(\"Number of validation images: \" + str(len(labels_valid)))\n",
    "# print(\"Number of training images: \" + str(len(labels_train)))\n",
    "# print(\"Number of classes in the training set: \" + str(max(labels_train) - min(labels_train)+1))\n",
    "\n",
    "\n",
    "# Creates the pairs\n",
    "\n",
    "nbof_pairs = (len(images_valid)//2)*2 # it has to be multiple of 2\n",
    "nbof_pairs = 10000\n",
    "print(\"Number of pairs: \" + str(nbof_pairs))\n",
    "\n",
    "pairs = np.empty((nbof_pairs,w,h))\n",
    "issame = np.empty(nbof_pairs, dtype=int)\n",
    "y_pairs = np.empty(nbof_pairs)\n",
    "\n",
    "nbof_same = 0\n",
    "\n",
    "for i in tqdm_notebook(range(0,nbof_pairs,2)):\n",
    "    ## alea_issame will decide if the new pair will be a pair of same dog images or a pair of different\n",
    "    alea_issame = np.random.rand()\n",
    "\n",
    "    if alea_issame < 0.5: # Then it will be a pair of same dogs\n",
    "        # we randomly choose a dog\n",
    "        choice = np.random.randint(len(labels_valid))\n",
    "        \n",
    "        # we extract the images of this class\n",
    "        chosen_images = list(images_valid[np.equal(labels_valid,labels_valid[choice])])\n",
    "        chosen_labels = list(labels_valid[np.equal(labels_valid,labels_valid[choice])])\n",
    "        \n",
    "        while len(labels_valid[np.equal(labels_valid,labels_valid[choice])]) < 2:\n",
    "            choice = np.random.randint(len(labels_valid))\n",
    "            chosen_images = list(images_valid[np.equal(labels_valid,labels_valid[choice])])\n",
    "            chosen_labels = list(labels_valid[np.equal(labels_valid,labels_valid[choice])])\n",
    "            \n",
    "        # we then randomly choose two pictures of this class\n",
    "        choice1 = np.random.randint(len(chosen_images))\n",
    "        pairs[i] = chosen_images[choice1]\n",
    "        y_pairs[i] = chosen_labels[choice1]\n",
    "        \n",
    "        save = np.copy(chosen_images)\n",
    "        chosen_images = chosen_images[:choice1] + chosen_images[choice1+1:]\n",
    "        if len(chosen_images) == 0:\n",
    "            print(\"Bug!\")\n",
    "            print(save)\n",
    "        choice2 = np.random.randint(len(chosen_images))\n",
    "        pairs[i+1] = chosen_images[choice2]\n",
    "        y_pairs[i+1] = chosen_labels[choice2]\n",
    "        \n",
    "        issame[i] = issame[i+1] = 1\n",
    "        \n",
    "        nbof_same += 1\n",
    "        \n",
    "    else: # Then it will be a pair of different dogs\n",
    "        # we randomly choose two dogs\n",
    "        choice1 = np.random.randint(len(labels_valid))\n",
    "        \n",
    "        # we extract the images of the class\n",
    "        chosen_images = list(images_valid[np.equal(labels_valid,labels_valid[choice1])])\n",
    "        chosen_labels = list(labels_valid[np.equal(labels_valid,labels_valid[choice1])])\n",
    "        \n",
    "        # we choose an image of this class\n",
    "        choice = np.random.randint(len(chosen_images))\n",
    "        #print(choice)\n",
    "        pairs[i] = chosen_images[choice]\n",
    "        y_pairs[i] = chosen_labels[choice]\n",
    "        \n",
    "        choice2 = np.random.randint(len(labels_valid))\n",
    "        \n",
    "        # check if we have two different classes\n",
    "        while labels_valid[choice2] == labels_valid[choice1]:\n",
    "            choice2 = np.random.randint(len(labels_valid))\n",
    "        \n",
    "        chosen_images = list(images_valid[np.equal(labels_valid,labels_valid[choice2])])\n",
    "        chosen_labels = list(labels_valid[np.equal(labels_valid,labels_valid[choice2])])\n",
    "        \n",
    "        # we choose an image of this class\n",
    "        choice = np.random.randint(len(chosen_images))\n",
    "        \n",
    "        pairs[i+1] = chosen_images[choice]\n",
    "        y_pairs[i+1] = chosen_labels[choice]\n",
    "        \n",
    "        issame[i] = issame[i+1] = 0\n",
    "\n",
    "print(\"Number of same images: \" + str(nbof_same))\n",
    "print(\"Number of validation images: \" + str(len(labels_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all the pairs\n",
    "for i in range(0,len(y_pairs),2):\n",
    "    if issame[i] == 0:\n",
    "        if y_pairs[i]==y_pairs[i+1]:\n",
    "            print(y_pairs[i],y_pairs[i+1],issame[i])\n",
    "    if issame[i] == 1:\n",
    "        if y_pairs[i]!=y_pairs[i+1]:\n",
    "            print(y_pairs[i],y_pairs[i+1],issame[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 1 1 1 1 1]\n",
      "[8. 9. 8. 8. 8. 8. 9. 9. 9. 9.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x1080 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check some pairs\n",
    "s = 0\n",
    "n = 5\n",
    "print(issame[2*s:(n+s)*2])\n",
    "print(y_pairs[2*s:(n+s)*2])\n",
    "fig = plt.figure(figsize=(5,3*n))\n",
    "for i in range(s,s+n):\n",
    "    plt.subplot(n,2,2*(i-s)+1)\n",
    "    plt.imshow(pairs[2*i])\n",
    "    plt.subplot(n,2,2*(i-s)+2)\n",
    "    plt.imshow(pairs[2*i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48200, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.expand_dims(x_train, -1)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8017, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pairs = np.expand_dims(pairs, -1)\n",
    "print(pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_pairs = tf.keras.utils.to_categorical(y_pairs)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shapes: \n",
      "(60000, 28, 28, 1)\n",
      "(60000,)\n",
      "(60000,)\n",
      "Train output shape: \n",
      "(60000,)\n",
      "(60000, 2)\n",
      "Valid input shape: \n",
      "(10000, 28, 28, 1)\n",
      "(10000,)\n",
      "(10000,)\n",
      "Valid output shape: \n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes\n",
    "# Train inputs\n",
    "print(\"Train input shapes: \")\n",
    "print(images_train.shape)\n",
    "print(labels_train.shape)\n",
    "print(issame_train_in.shape)\n",
    "\n",
    "print(\"Train output shape: \")\n",
    "print(labels_train.shape)\n",
    "print(issame_train_out.shape)\n",
    "\n",
    "print(\"Valid input shape: \")\n",
    "print(pairs.shape)\n",
    "print(labels_valid.shape)\n",
    "print(issame_in.shape)\n",
    "\n",
    "print(\"Valid output shape: \")\n",
    "print(labels_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "- Define the ArcFace layer\n",
    "- Define the dummy model first\n",
    "- Compile it with the softmax loss and and Adam optimizer\n",
    "- Then use transfer learning with a more complex model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Arcface layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "import math\n",
    "\n",
    "# Arcface should only be used for training\n",
    "class Arcface(Layer):\n",
    "\n",
    "    def __init__(self, out_num, s = 64., m = 0.5, **kwargs):\n",
    "        self.out_num = out_num\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        super(Arcface, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape, initializer='uniform'):\n",
    "        assert isinstance(input_shape, list)\n",
    "        \n",
    "        shape = tf.TensorShape((input_shape[0][-1],self.out_num))\n",
    "        print(shape)\n",
    "        \n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                                 shape=shape,\n",
    "                                                 initializer=initializer,\n",
    "                                                 dtype=tf.float32,\n",
    "                                                 trainable=True)\n",
    "        super(Arcface, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        assert isinstance(x, list)\n",
    "        emb, labels = x\n",
    "        #labels_sq = tf.squeeze(labels,1)\n",
    "        #labels_sq = tf.reshape(labels,[None])\n",
    "        #labels_int = tf.cast(labels_sq,tf.int32, name='labels_int')\n",
    "        #print(labels_int.shape)\n",
    "        #mask = tf.one_hot(labels_int, depth=self.out_num, name='one_hot_mask')\n",
    "        mask = labels\n",
    "        #mask = tf.squeeze(mask,1)\n",
    "        #print(mask.shape)\n",
    "        #mask_shape = mask.shape.as_list()\n",
    "        #mask = tf.reshape(mask, (None,mask_shape[-1]))\n",
    "        #print(mask.shape)\n",
    "        def train_output():\n",
    "            cos_m = math.cos(self.m)\n",
    "            sin_m = math.sin(self.m)\n",
    "            mm = sin_m * self.m  # issue 1\n",
    "            threshold = math.cos(math.pi - self.m)\n",
    "\n",
    "            # inputs and weights norm\n",
    "            embedding_norm = tf.norm(emb, axis=1, keepdims=True)\n",
    "            embedding = tf.div(emb, embedding_norm, name='norm_embedding')\n",
    "\n",
    "            weights_norm = tf.norm(self.kernel, axis=0, keepdims=True)\n",
    "            weights = tf.div(self.kernel, weights_norm, name='norm_weights')\n",
    "            # cos(theta+m)\n",
    "            cos_t = tf.matmul(embedding, weights, name='cos_t')\n",
    "            print(cos_t.shape)\n",
    "            cos_t2 = tf.square(cos_t, name='cos_2')\n",
    "            sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n",
    "            sin_t = tf.sqrt(sin_t2, name='sin_t')\n",
    "            cos_mt = self.s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n",
    "\n",
    "            # this condition controls the theta+m should be in range [0, pi]\n",
    "            #      0<=theta+m<=pi\n",
    "            #     -m<=theta<=pi-m\n",
    "            cond_v = cos_t - threshold\n",
    "            cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n",
    "\n",
    "            keep_val = self.s*(cos_t - mm)\n",
    "            cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n",
    "\n",
    "            \n",
    "            # mask = tf.squeeze(mask, 1)\n",
    "            inv_mask = tf.subtract(1., mask, name='inverse_mask')\n",
    "\n",
    "            s_cos_t = tf.multiply(self.s, cos_t, name='scalar_cos_t')\n",
    "            mul1 = tf.multiply(s_cos_t, inv_mask)\n",
    "            print(mul1.shape)\n",
    "            mul2 = tf.multiply(cos_mt_temp, mask)\n",
    "            print(mul2.shape)\n",
    "            output = tf.add(mul1, mul2, name='arcface_loss_output')\n",
    "            print(output.shape)\n",
    "            print(cos_mt_temp.shape)\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        def valid_output():\n",
    "            return mask\n",
    "        \n",
    "        return K.in_train_phase(train_output,valid_output,training=training)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        shape_emb, shape_lab = input_shape\n",
    "        shape_emb[-1] = self.out_num\n",
    "        return tf.TensorShape(shape_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class Cosine(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(Cosine, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        shape = tf.TensorShape((input_shape[1],self.output_dim))\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=shape,\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(Cosine, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        # Compute the euclidian norm\n",
    "        x_norm = tf.norm(x, axis=1, keepdims=True)\n",
    "        x = tf.div(x, x_norm, name='x_norm')\n",
    "\n",
    "        W_norm = tf.norm(self.kernel, axis=0, keepdims=True)\n",
    "        W = tf.div(self.kernel, W_norm, name='norm_weights')\n",
    "        return K.dot(x, W)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class Cosine(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(Cosine, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        shape = tf.TensorShape((input_shape[-1],self.output_dim))\n",
    "\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=shape,\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(Cosine, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x / tf.norm(x,axis=-1,keepdims=True)\n",
    "        w = self.kernel / tf.norm(self.kernel,axis=0,keepdims=True)\n",
    "        return K.dot(x, w)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "m = 0.5\n",
    "s = 64.\n",
    "out_num = 8\n",
    "\n",
    "def cosineLoss(y_true,y_pred):\n",
    "    # Compute the softmax like \n",
    "    s_cos = tf.multiply(y_pred, s)\n",
    "    s_cos_m = tf.add(s_cos, -m)\n",
    "    exp_s_cos = K.exp(s_cos)\n",
    "    exp_s_cos_m = K.exp(s_cos_m)\n",
    "    \n",
    "    mask = tf.cast(y_true, tf.float32)\n",
    "    m_mask = tf.multiply(mask,m)\n",
    "    s_cos_m_mask = tf.subtract(s_cos, m_mask)\n",
    "    den = K.sum(s_cos_m_mask,1)\n",
    "    den_stacked = K.stack([den]*out_num,axis=-1)\n",
    "    \n",
    "    out = tf.divide(exp_s_cos_m,den_stacked)\n",
    "    # Compute the categorical crossentropy\n",
    "    return K.categorical_crossentropy(y_true,out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outputs: 8\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "m = 0.5\n",
    "s = 64.\n",
    "out_num = 8\n",
    "\n",
    "print(\"Number of outputs: \" + str(out_num))\n",
    "\n",
    "def cosineLoss(y_true,y_pred):\n",
    "    # Compute the softmax like \n",
    "    e_s_m = K.exp(s*y_pred - m)\n",
    "    e_s = K.exp(s*y_pred)\n",
    "    \n",
    "    mask = K.cast(y_true, tf.float32)\n",
    "    inv_mask = 1. - mask\n",
    "    den = e_s_m * mask + e_s * inv_mask\n",
    "    den = K.sum(den,-1,keepdims=True)\n",
    "\n",
    "    out = e_s_m / den\n",
    "    # Compute the categorical crossentropy\n",
    "    return K.categorical_crossentropy(y_true,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0.35\n",
    "s = 128\n",
    "out_num = 10\n",
    "\n",
    "def arcfaceLoss(y_true,y_pred):\n",
    "    # Compute the softmax like \n",
    "    cos_t = y_pred\n",
    "    sin_t = tf.sqrt(tf.subtract(1.,tf.square(cos_t)))\n",
    "    \n",
    "    cos_m = tf.cos(m)\n",
    "    sin_m = tf.sin(m)\n",
    "    \n",
    "    cos_t_m = tf.subtract(tf.multiply(cos_t,cos_m),tf.multiply(sin_t,sin_m))\n",
    "    \n",
    "    s_cos_t_m = tf.multiply(cos_t_m, s)\n",
    "    \n",
    "    exp_s_cos_t_m = K.exp(s_cos_t_m)\n",
    "    exp_s_cos_t = K.exp(tf.multiply(cos_t,s))\n",
    "    \n",
    "    mask = tf.cast(y_true, tf.float32)\n",
    "    \n",
    "    inv_mask = tf.subtract(1.,mask)\n",
    "    \n",
    "    den = tf.multiply(mask,exp_s_cos_t_m)\n",
    "    den = tf.add(den,tf.multiply(inv_mask,exp_s_cos_t))\n",
    "    den = K.sum(den,1)\n",
    "    den_stacked = K.stack([den]*out_num,axis=-1)\n",
    "    \n",
    "    out = tf.divide(exp_s_cos_t_m,den_stacked)\n",
    "\n",
    "    # Compute the categorical crossentropy\n",
    "    return K.categorical_crossentropy(y_true,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the validation layer:\n",
    "- The bigger the validation batch the better it is (no less than 64 pictures -> 32 pairs)\n",
    "- It computes the ROC curve\n",
    "- Finds the best threshold\n",
    "- Returns a list of 2D vectors [1,0] if the pair was the same dog, [0,1] if it was a different dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# Should only be used for validating\n",
    "class Validation(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Validation, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        self.emb_shape = input_shape[0]\n",
    "        super(Validation, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        \"\"\"\n",
    "        Inputs: a tuple containing the embeddings and the issame list\n",
    "        - embeddings: shape=(batch_size, embedding_size), type=float\n",
    "        - issame: shape=(batch_size), type=bool\n",
    "        \n",
    "        Outputs: a tensor of shape=(batch_size,2), the ouput is either [1,0] (is same) or [0,1] (is different)\n",
    "        \"\"\"\n",
    "        assert isinstance(x, list)\n",
    "        \n",
    "        embeddings, iss = x\n",
    "        \n",
    "        \n",
    "        \n",
    "        def train_output():\n",
    "            return iss\n",
    "        \n",
    "        def valid_output():\n",
    "            issame = tf.squeeze(iss)\n",
    "            #self.emb_shape = embeddings.shape\n",
    "            emb = tf.math.l2_normalize(embeddings,0)\n",
    "            # emb contains a list of pictures\n",
    "            # pictures with an even index are first pictures of the pairs\n",
    "            # pictures with an odd index are second pictures of the pairs\n",
    "            emb1 = embeddings[0::2]\n",
    "            emb2 = embeddings[1::2]\n",
    "            #emb1, emb2 = tf.split(embeddings, [32,32],0)\n",
    "            \n",
    "          # Compute the distance for each pair of vector\n",
    "            dist = tf.reduce_sum(tf.squared_difference(emb1,emb2),1)\n",
    "            dist = tf.reshape(tf.stack([dist,dist], axis=-1), [-1])\n",
    "            actual_issame_bool = tf.cast(issame,dtype=tf.bool)\n",
    "\n",
    "            def fn(t):\n",
    "                less = tf.less(dist,t)\n",
    "                acc = tf.logical_not(tf.logical_xor(less,actual_issame_bool))\n",
    "                acc = tf.cast(acc,tf.float32)\n",
    "                out = tf.reshape(tf.reduce_sum(acc),[])\n",
    "                return out\n",
    "\n",
    "\n",
    "            thresholds = tf.range(0,1,0.001)\n",
    "            apply_t = tf.map_fn(fn, thresholds)\n",
    "            best_t = tf.argmax(apply_t)\n",
    "\n",
    "            best = thresholds[best_t]\n",
    "\n",
    "          # Redo the manipulation with the best threshold\n",
    "            less = tf.less(dist,best)\n",
    "            less = tf.cast(less,tf.float32)\n",
    "            less = tf.expand_dims(less,1) # <- bug fixed\n",
    "            return less\n",
    "\n",
    "            \n",
    "        return K.in_train_phase(train_output,valid_output,training=training)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        emb_shape, _ = input_shape\n",
    "        return (emb_shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# Should only be used for validating\n",
    "class Validation(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Validation, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        super(Validation, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        \"\"\"\n",
    "        Inputs: a tuple containing the embeddings and the issame list\n",
    "        - embeddings: shape=(batch_size, embedding_size), type=float\n",
    "        - issame: shape=(batch_size), type=bool\n",
    "        \n",
    "        Outputs: a tensor of shape=(batch_size,2), the ouput is either [1,0] (is same) or [0,1] (is different)\n",
    "        \"\"\"\n",
    "        assert isinstance(x, list)\n",
    "        \n",
    "        embeddings, iss = x\n",
    "        \n",
    "        \n",
    "        \n",
    "        def train_output():\n",
    "            return iss\n",
    "        \n",
    "        def valid_output():\n",
    "            emb = tf.math.l2_normalize(embeddings,0)\n",
    "            # emb contains a list of pictures\n",
    "            # pictures with an even index are first pictures of the pairs\n",
    "            # pictures with an odd index are second pictures of the pairs\n",
    "            emb1 = embeddings[0::2]\n",
    "            emb2 = embeddings[1::2]\n",
    "            \n",
    "          # Compute the distance for each pair of vector\n",
    "            dist = tf.reduce_sum(tf.squared_difference(emb1,emb2),1)\n",
    "            dist = tf.reshape(tf.stack([dist,dist], axis=-1), [-1])\n",
    "            less = tf.less(dist,0.01)\n",
    "            less = tf.cast(less,tf.float32)\n",
    "            less = tf.expand_dims(less,1) # <- bug fixed\n",
    "            return less\n",
    "\n",
    "            \n",
    "        return K.in_train_phase(train_output,valid_output,training=training)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        emb_shape, _ = input_shape\n",
    "        emb_shape[-1] = 1\n",
    "        return emb_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try to use the functional API\n",
    "#tf.reset_default_graph()\n",
    "def net(inputs_shapes, emb_size=4):\n",
    "    images_shape, labels_shape = inputs_shapes\n",
    "    input_image = tf.keras.Input(images_shape,name='image_input')\n",
    "    x = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(input_image)\n",
    "    x = tf.keras.layers.Conv2D(64, (3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    emb = tf.keras.layers.Dense(3, activity_regularizer='l2')(x)\n",
    "    \n",
    "    out = tf.keras.layers.Dense(8, kernel_regularizer='l2', use_bias=False, name='output')(emb)\n",
    "    model = tf.keras.Model(inputs=[input_image], outputs=out)\n",
    "    return model\n",
    "\n",
    "w, h = SIZE\n",
    "inputs_shapes = [(w, h, 1,),(8,)]\n",
    "model = net(inputs_shapes)\n",
    "model.compile(tf.keras.optimizers.Adam(),loss={'output':cosineLoss},metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use the functional API\n",
    "#tf.reset_default_graph()\n",
    "def net(inputs_shapes, emb_size=4):\n",
    "    images_shape, labels_shape = inputs_shapes\n",
    "    input_image = tf.keras.Input(images_shape,name='image_input')\n",
    "    x = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(input_image)\n",
    "    x = tf.keras.layers.Conv2D(64, (3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    emb = tf.keras.layers.Dense(3)(x)\n",
    "    \n",
    "    out = Cosine(8, name='output')(emb)\n",
    "    model = tf.keras.Model(inputs=[input_image], outputs=out)\n",
    "    return model\n",
    "\n",
    "w, h = SIZE\n",
    "inputs_shapes = [(w, h, 1,),(8,)]\n",
    "model = net(inputs_shapes)\n",
    "model.compile(tf.keras.optimizers.Adam(),loss={'output':cosineLoss},metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use the functional API\n",
    "#tf.reset_default_graph()\n",
    "def net(inputs_shapes, emb_size=4):\n",
    "    images_shape, labels_shape = inputs_shapes\n",
    "    input_image = tf.keras.Input(images_shape,name='image_input')\n",
    "    x = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(input_image)\n",
    "    x = tf.keras.layers.Conv2D(64, (3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    \n",
    "    out = tf.keras.layers.Dense(10, activation='softmax', name='output')(x)\n",
    "    model = tf.keras.Model(inputs=[input_image], outputs=out)\n",
    "    return model\n",
    "\n",
    "w, h = SIZE\n",
    "inputs_shapes = [(w, h, 1,),(10,)]\n",
    "model = net(inputs_shapes)\n",
    "model.compile(tf.keras.optimizers.Adam(),loss={'output':'categorical_crossentropy'},metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48200 samples, validate on 8017 samples\n",
      "Epoch 1/5\n",
      "48200/48200 [==============================] - ETA: 2:33 - loss: 0.2308 - acc: 0.960 - ETA: 45s - loss: 0.2355 - acc: 0.966 - ETA: 33s - loss: 0.1905 - acc: 0.97 - ETA: 28s - loss: 0.1709 - acc: 0.97 - ETA: 22s - loss: 0.1471 - acc: 0.97 - ETA: 21s - loss: 0.1669 - acc: 0.97 - ETA: 19s - loss: 0.1715 - acc: 0.96 - ETA: 17s - loss: 0.1654 - acc: 0.97 - ETA: 17s - loss: 0.1610 - acc: 0.97 - ETA: 15s - loss: 0.1549 - acc: 0.97 - ETA: 14s - loss: 0.1466 - acc: 0.97 - ETA: 14s - loss: 0.1474 - acc: 0.97 - ETA: 13s - loss: 0.1501 - acc: 0.97 - ETA: 13s - loss: 0.1474 - acc: 0.97 - ETA: 13s - loss: 0.1423 - acc: 0.97 - ETA: 12s - loss: 0.1483 - acc: 0.97 - ETA: 12s - loss: 0.1447 - acc: 0.97 - ETA: 12s - loss: 0.1496 - acc: 0.97 - ETA: 12s - loss: 0.1499 - acc: 0.97 - ETA: 11s - loss: 0.1501 - acc: 0.97 - ETA: 11s - loss: 0.1462 - acc: 0.97 - ETA: 11s - loss: 0.1442 - acc: 0.97 - ETA: 11s - loss: 0.1478 - acc: 0.97 - ETA: 11s - loss: 0.1475 - acc: 0.97 - ETA: 10s - loss: 0.1484 - acc: 0.97 - ETA: 10s - loss: 0.1468 - acc: 0.97 - ETA: 10s - loss: 0.1489 - acc: 0.97 - ETA: 10s - loss: 0.1470 - acc: 0.97 - ETA: 10s - loss: 0.1539 - acc: 0.97 - ETA: 10s - loss: 0.1512 - acc: 0.97 - ETA: 10s - loss: 0.1520 - acc: 0.97 - ETA: 9s - loss: 0.1512 - acc: 0.9747 - ETA: 9s - loss: 0.1546 - acc: 0.974 - ETA: 9s - loss: 0.1536 - acc: 0.975 - ETA: 9s - loss: 0.1522 - acc: 0.974 - ETA: 9s - loss: 0.1509 - acc: 0.975 - ETA: 9s - loss: 0.1521 - acc: 0.974 - ETA: 9s - loss: 0.1513 - acc: 0.974 - ETA: 9s - loss: 0.1511 - acc: 0.975 - ETA: 8s - loss: 0.1493 - acc: 0.975 - ETA: 8s - loss: 0.1476 - acc: 0.975 - ETA: 8s - loss: 0.1470 - acc: 0.975 - ETA: 8s - loss: 0.1474 - acc: 0.975 - ETA: 8s - loss: 0.1483 - acc: 0.975 - ETA: 8s - loss: 0.1454 - acc: 0.975 - ETA: 8s - loss: 0.1452 - acc: 0.975 - ETA: 8s - loss: 0.1450 - acc: 0.975 - ETA: 8s - loss: 0.1470 - acc: 0.975 - ETA: 7s - loss: 0.1469 - acc: 0.975 - ETA: 7s - loss: 0.1462 - acc: 0.975 - ETA: 7s - loss: 0.1512 - acc: 0.975 - ETA: 7s - loss: 0.1490 - acc: 0.975 - ETA: 7s - loss: 0.1502 - acc: 0.975 - ETA: 7s - loss: 0.1485 - acc: 0.975 - ETA: 7s - loss: 0.1481 - acc: 0.975 - ETA: 7s - loss: 0.1481 - acc: 0.975 - ETA: 7s - loss: 0.1475 - acc: 0.975 - ETA: 7s - loss: 0.1467 - acc: 0.975 - ETA: 7s - loss: 0.1480 - acc: 0.975 - ETA: 6s - loss: 0.1479 - acc: 0.975 - ETA: 6s - loss: 0.1478 - acc: 0.975 - ETA: 6s - loss: 0.1488 - acc: 0.975 - ETA: 6s - loss: 0.1482 - acc: 0.975 - ETA: 6s - loss: 0.1483 - acc: 0.975 - ETA: 6s - loss: 0.1468 - acc: 0.975 - ETA: 6s - loss: 0.1480 - acc: 0.975 - ETA: 6s - loss: 0.1492 - acc: 0.975 - ETA: 6s - loss: 0.1490 - acc: 0.975 - ETA: 6s - loss: 0.1491 - acc: 0.975 - ETA: 6s - loss: 0.1495 - acc: 0.975 - ETA: 6s - loss: 0.1500 - acc: 0.975 - ETA: 6s - loss: 0.1499 - acc: 0.974 - ETA: 5s - loss: 0.1487 - acc: 0.975 - ETA: 5s - loss: 0.1486 - acc: 0.975 - ETA: 5s - loss: 0.1495 - acc: 0.974 - ETA: 5s - loss: 0.1496 - acc: 0.974 - ETA: 5s - loss: 0.1524 - acc: 0.974 - ETA: 5s - loss: 0.1529 - acc: 0.974 - ETA: 5s - loss: 0.1532 - acc: 0.974 - ETA: 5s - loss: 0.1556 - acc: 0.973 - ETA: 5s - loss: 0.1567 - acc: 0.973 - ETA: 5s - loss: 0.1562 - acc: 0.973 - ETA: 5s - loss: 0.1558 - acc: 0.973 - ETA: 5s - loss: 0.1560 - acc: 0.973 - ETA: 5s - loss: 0.1575 - acc: 0.973 - ETA: 5s - loss: 0.1581 - acc: 0.973 - ETA: 5s - loss: 0.1586 - acc: 0.973 - ETA: 5s - loss: 0.1582 - acc: 0.973 - ETA: 4s - loss: 0.1587 - acc: 0.973 - ETA: 4s - loss: 0.1590 - acc: 0.973 - ETA: 4s - loss: 0.1591 - acc: 0.973 - ETA: 4s - loss: 0.1584 - acc: 0.973 - ETA: 4s - loss: 0.1591 - acc: 0.973 - ETA: 4s - loss: 0.1593 - acc: 0.973 - ETA: 4s - loss: 0.1607 - acc: 0.973 - ETA: 4s - loss: 0.1606 - acc: 0.973 - ETA: 4s - loss: 0.1600 - acc: 0.973 - ETA: 4s - loss: 0.1617 - acc: 0.973 - ETA: 4s - loss: 0.1632 - acc: 0.972 - ETA: 4s - loss: 0.1627 - acc: 0.973 - ETA: 4s - loss: 0.1641 - acc: 0.972 - ETA: 3s - loss: 0.1643 - acc: 0.972 - ETA: 3s - loss: 0.1652 - acc: 0.972 - ETA: 3s - loss: 0.1644 - acc: 0.972 - ETA: 3s - loss: 0.1651 - acc: 0.972 - ETA: 3s - loss: 0.1651 - acc: 0.972 - ETA: 3s - loss: 0.1655 - acc: 0.972 - ETA: 3s - loss: 0.1655 - acc: 0.972 - ETA: 3s - loss: 0.1654 - acc: 0.972 - ETA: 3s - loss: 0.1656 - acc: 0.972 - ETA: 3s - loss: 0.1655 - acc: 0.972 - ETA: 3s - loss: 0.1650 - acc: 0.972 - ETA: 3s - loss: 0.1643 - acc: 0.972 - ETA: 3s - loss: 0.1640 - acc: 0.972 - ETA: 3s - loss: 0.1645 - acc: 0.972 - ETA: 3s - loss: 0.1647 - acc: 0.972 - ETA: 3s - loss: 0.1643 - acc: 0.972 - ETA: 3s - loss: 0.1645 - acc: 0.972 - ETA: 2s - loss: 0.1663 - acc: 0.972 - ETA: 2s - loss: 0.1662 - acc: 0.972 - ETA: 2s - loss: 0.1660 - acc: 0.972 - ETA: 2s - loss: 0.1658 - acc: 0.972 - ETA: 2s - loss: 0.1651 - acc: 0.972 - ETA: 2s - loss: 0.1651 - acc: 0.972 - ETA: 2s - loss: 0.1645 - acc: 0.972 - ETA: 2s - loss: 0.1646 - acc: 0.972 - ETA: 2s - loss: 0.1651 - acc: 0.972 - ETA: 2s - loss: 0.1643 - acc: 0.972 - ETA: 2s - loss: 0.1656 - acc: 0.972 - ETA: 2s - loss: 0.1649 - acc: 0.972 - ETA: 2s - loss: 0.1647 - acc: 0.972 - ETA: 1s - loss: 0.1650 - acc: 0.972 - ETA: 1s - loss: 0.1645 - acc: 0.972 - ETA: 1s - loss: 0.1645 - acc: 0.972 - ETA: 1s - loss: 0.1651 - acc: 0.972 - ETA: 1s - loss: 0.1652 - acc: 0.972 - ETA: 1s - loss: 0.1656 - acc: 0.972 - ETA: 1s - loss: 0.1657 - acc: 0.972 - ETA: 1s - loss: 0.1656 - acc: 0.972 - ETA: 1s - loss: 0.1657 - acc: 0.972 - ETA: 1s - loss: 0.1659 - acc: 0.972 - ETA: 1s - loss: 0.1659 - acc: 0.972 - ETA: 1s - loss: 0.1650 - acc: 0.972 - ETA: 1s - loss: 0.1645 - acc: 0.972 - ETA: 1s - loss: 0.1636 - acc: 0.972 - ETA: 1s - loss: 0.1636 - acc: 0.972 - ETA: 0s - loss: 0.1647 - acc: 0.972 - ETA: 0s - loss: 0.1652 - acc: 0.972 - ETA: 0s - loss: 0.1655 - acc: 0.972 - ETA: 0s - loss: 0.1656 - acc: 0.972 - ETA: 0s - loss: 0.1665 - acc: 0.972 - ETA: 0s - loss: 0.1668 - acc: 0.972 - ETA: 0s - loss: 0.1670 - acc: 0.972 - ETA: 0s - loss: 0.1674 - acc: 0.972 - ETA: 0s - loss: 0.1668 - acc: 0.972 - ETA: 0s - loss: 0.1660 - acc: 0.972 - ETA: 0s - loss: 0.1663 - acc: 0.972 - ETA: 0s - loss: 0.1668 - acc: 0.972 - ETA: 0s - loss: 0.1663 - acc: 0.972 - ETA: 0s - loss: 0.1658 - acc: 0.972 - ETA: 0s - loss: 0.1660 - acc: 0.972 - 12s 247us/step - loss: 0.1661 - acc: 0.9725 - val_loss: 0.0965 - val_acc: 0.9869\n",
      "Epoch 2/5\n",
      "48200/48200 [==============================] - ETA: 11s - loss: 0.2078 - acc: 0.98 - ETA: 11s - loss: 0.1747 - acc: 0.97 - ETA: 12s - loss: 0.1507 - acc: 0.98 - ETA: 12s - loss: 0.1481 - acc: 0.97 - ETA: 12s - loss: 0.1231 - acc: 0.98 - ETA: 11s - loss: 0.1092 - acc: 0.98 - ETA: 11s - loss: 0.1145 - acc: 0.98 - ETA: 11s - loss: 0.1072 - acc: 0.98 - ETA: 11s - loss: 0.1303 - acc: 0.97 - ETA: 10s - loss: 0.1228 - acc: 0.97 - ETA: 10s - loss: 0.1212 - acc: 0.97 - ETA: 10s - loss: 0.1272 - acc: 0.97 - ETA: 10s - loss: 0.1238 - acc: 0.97 - ETA: 10s - loss: 0.1183 - acc: 0.97 - ETA: 10s - loss: 0.1220 - acc: 0.97 - ETA: 10s - loss: 0.1241 - acc: 0.97 - ETA: 10s - loss: 0.1246 - acc: 0.97 - ETA: 9s - loss: 0.1336 - acc: 0.9779 - ETA: 9s - loss: 0.1312 - acc: 0.978 - ETA: 9s - loss: 0.1276 - acc: 0.978 - ETA: 9s - loss: 0.1277 - acc: 0.978 - ETA: 9s - loss: 0.1246 - acc: 0.978 - ETA: 9s - loss: 0.1302 - acc: 0.978 - ETA: 9s - loss: 0.1270 - acc: 0.978 - ETA: 9s - loss: 0.1257 - acc: 0.978 - ETA: 9s - loss: 0.1228 - acc: 0.979 - ETA: 9s - loss: 0.1234 - acc: 0.978 - ETA: 9s - loss: 0.1302 - acc: 0.978 - ETA: 9s - loss: 0.1293 - acc: 0.978 - ETA: 9s - loss: 0.1290 - acc: 0.978 - ETA: 9s - loss: 0.1343 - acc: 0.977 - ETA: 8s - loss: 0.1362 - acc: 0.977 - ETA: 8s - loss: 0.1359 - acc: 0.977 - ETA: 8s - loss: 0.1335 - acc: 0.977 - ETA: 8s - loss: 0.1329 - acc: 0.977 - ETA: 8s - loss: 0.1308 - acc: 0.977 - ETA: 8s - loss: 0.1323 - acc: 0.977 - ETA: 8s - loss: 0.1338 - acc: 0.977 - ETA: 8s - loss: 0.1344 - acc: 0.977 - ETA: 8s - loss: 0.1332 - acc: 0.977 - ETA: 8s - loss: 0.1335 - acc: 0.977 - ETA: 8s - loss: 0.1353 - acc: 0.977 - ETA: 8s - loss: 0.1352 - acc: 0.977 - ETA: 8s - loss: 0.1362 - acc: 0.977 - ETA: 8s - loss: 0.1355 - acc: 0.977 - ETA: 8s - loss: 0.1334 - acc: 0.977 - ETA: 8s - loss: 0.1342 - acc: 0.977 - ETA: 8s - loss: 0.1338 - acc: 0.977 - ETA: 7s - loss: 0.1324 - acc: 0.977 - ETA: 7s - loss: 0.1322 - acc: 0.978 - ETA: 7s - loss: 0.1352 - acc: 0.977 - ETA: 7s - loss: 0.1368 - acc: 0.977 - ETA: 7s - loss: 0.1367 - acc: 0.977 - ETA: 7s - loss: 0.1357 - acc: 0.977 - ETA: 7s - loss: 0.1340 - acc: 0.978 - ETA: 7s - loss: 0.1352 - acc: 0.978 - ETA: 7s - loss: 0.1355 - acc: 0.978 - ETA: 7s - loss: 0.1355 - acc: 0.977 - ETA: 7s - loss: 0.1367 - acc: 0.977 - ETA: 7s - loss: 0.1355 - acc: 0.977 - ETA: 7s - loss: 0.1372 - acc: 0.977 - ETA: 6s - loss: 0.1369 - acc: 0.977 - ETA: 6s - loss: 0.1356 - acc: 0.977 - ETA: 6s - loss: 0.1350 - acc: 0.977 - ETA: 6s - loss: 0.1353 - acc: 0.977 - ETA: 6s - loss: 0.1350 - acc: 0.977 - ETA: 6s - loss: 0.1342 - acc: 0.977 - ETA: 6s - loss: 0.1348 - acc: 0.977 - ETA: 6s - loss: 0.1361 - acc: 0.977 - ETA: 6s - loss: 0.1355 - acc: 0.977 - ETA: 6s - loss: 0.1340 - acc: 0.977 - ETA: 6s - loss: 0.1341 - acc: 0.977 - ETA: 6s - loss: 0.1343 - acc: 0.977 - ETA: 6s - loss: 0.1341 - acc: 0.977 - ETA: 6s - loss: 0.1340 - acc: 0.977 - ETA: 6s - loss: 0.1333 - acc: 0.977 - ETA: 6s - loss: 0.1340 - acc: 0.977 - ETA: 5s - loss: 0.1337 - acc: 0.977 - ETA: 5s - loss: 0.1340 - acc: 0.978 - ETA: 5s - loss: 0.1341 - acc: 0.978 - ETA: 5s - loss: 0.1343 - acc: 0.978 - ETA: 5s - loss: 0.1338 - acc: 0.978 - ETA: 5s - loss: 0.1343 - acc: 0.978 - ETA: 5s - loss: 0.1360 - acc: 0.978 - ETA: 5s - loss: 0.1354 - acc: 0.977 - ETA: 5s - loss: 0.1375 - acc: 0.977 - ETA: 5s - loss: 0.1381 - acc: 0.977 - ETA: 5s - loss: 0.1385 - acc: 0.977 - ETA: 5s - loss: 0.1384 - acc: 0.977 - ETA: 5s - loss: 0.1374 - acc: 0.977 - ETA: 5s - loss: 0.1370 - acc: 0.977 - ETA: 4s - loss: 0.1371 - acc: 0.977 - ETA: 4s - loss: 0.1382 - acc: 0.977 - ETA: 4s - loss: 0.1381 - acc: 0.977 - ETA: 4s - loss: 0.1390 - acc: 0.977 - ETA: 4s - loss: 0.1380 - acc: 0.977 - ETA: 4s - loss: 0.1376 - acc: 0.977 - ETA: 4s - loss: 0.1374 - acc: 0.977 - ETA: 4s - loss: 0.1386 - acc: 0.977 - ETA: 4s - loss: 0.1398 - acc: 0.977 - ETA: 4s - loss: 0.1408 - acc: 0.977 - ETA: 4s - loss: 0.1402 - acc: 0.977 - ETA: 4s - loss: 0.1402 - acc: 0.977 - ETA: 4s - loss: 0.1400 - acc: 0.977 - ETA: 4s - loss: 0.1395 - acc: 0.977 - ETA: 4s - loss: 0.1394 - acc: 0.977 - ETA: 4s - loss: 0.1387 - acc: 0.977 - ETA: 4s - loss: 0.1387 - acc: 0.977 - ETA: 4s - loss: 0.1396 - acc: 0.977 - ETA: 3s - loss: 0.1399 - acc: 0.977 - ETA: 3s - loss: 0.1393 - acc: 0.977 - ETA: 3s - loss: 0.1393 - acc: 0.977 - ETA: 3s - loss: 0.1385 - acc: 0.977 - ETA: 3s - loss: 0.1386 - acc: 0.977 - ETA: 3s - loss: 0.1388 - acc: 0.977 - ETA: 3s - loss: 0.1381 - acc: 0.977 - ETA: 3s - loss: 0.1396 - acc: 0.977 - ETA: 3s - loss: 0.1396 - acc: 0.977 - ETA: 3s - loss: 0.1401 - acc: 0.977 - ETA: 3s - loss: 0.1396 - acc: 0.977 - ETA: 3s - loss: 0.1396 - acc: 0.977 - ETA: 3s - loss: 0.1393 - acc: 0.977 - ETA: 3s - loss: 0.1408 - acc: 0.977 - ETA: 2s - loss: 0.1416 - acc: 0.977 - ETA: 2s - loss: 0.1421 - acc: 0.977 - ETA: 2s - loss: 0.1422 - acc: 0.977 - ETA: 2s - loss: 0.1417 - acc: 0.977 - ETA: 2s - loss: 0.1421 - acc: 0.977 - ETA: 2s - loss: 0.1415 - acc: 0.977 - ETA: 2s - loss: 0.1416 - acc: 0.977 - ETA: 2s - loss: 0.1412 - acc: 0.977 - ETA: 2s - loss: 0.1428 - acc: 0.977 - ETA: 2s - loss: 0.1432 - acc: 0.977 - ETA: 2s - loss: 0.1434 - acc: 0.977 - ETA: 2s - loss: 0.1426 - acc: 0.977 - ETA: 2s - loss: 0.1426 - acc: 0.977 - ETA: 2s - loss: 0.1426 - acc: 0.977 - ETA: 2s - loss: 0.1420 - acc: 0.977 - ETA: 1s - loss: 0.1426 - acc: 0.977 - ETA: 1s - loss: 0.1427 - acc: 0.977 - ETA: 1s - loss: 0.1425 - acc: 0.977 - ETA: 1s - loss: 0.1421 - acc: 0.977 - ETA: 1s - loss: 0.1422 - acc: 0.977 - ETA: 1s - loss: 0.1426 - acc: 0.977 - ETA: 1s - loss: 0.1420 - acc: 0.977 - ETA: 1s - loss: 0.1415 - acc: 0.977 - ETA: 1s - loss: 0.1415 - acc: 0.977 - ETA: 1s - loss: 0.1410 - acc: 0.977 - ETA: 1s - loss: 0.1409 - acc: 0.977 - ETA: 1s - loss: 0.1404 - acc: 0.977 - ETA: 1s - loss: 0.1396 - acc: 0.977 - ETA: 0s - loss: 0.1399 - acc: 0.977 - ETA: 0s - loss: 0.1395 - acc: 0.977 - ETA: 0s - loss: 0.1389 - acc: 0.977 - ETA: 0s - loss: 0.1388 - acc: 0.977 - ETA: 0s - loss: 0.1381 - acc: 0.977 - ETA: 0s - loss: 0.1384 - acc: 0.977 - ETA: 0s - loss: 0.1381 - acc: 0.977 - ETA: 0s - loss: 0.1378 - acc: 0.977 - ETA: 0s - loss: 0.1384 - acc: 0.977 - ETA: 0s - loss: 0.1383 - acc: 0.977 - ETA: 0s - loss: 0.1381 - acc: 0.977 - ETA: 0s - loss: 0.1381 - acc: 0.978 - ETA: 0s - loss: 0.1377 - acc: 0.978 - ETA: 0s - loss: 0.1373 - acc: 0.978 - ETA: 0s - loss: 0.1378 - acc: 0.978 - ETA: 0s - loss: 0.1381 - acc: 0.977 - 12s 245us/step - loss: 0.1378 - acc: 0.9779 - val_loss: 0.0886 - val_acc: 0.9868\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48200/48200 [==============================] - ETA: 11s - loss: 0.1466 - acc: 0.97 - ETA: 11s - loss: 0.2041 - acc: 0.97 - ETA: 10s - loss: 0.1752 - acc: 0.97 - ETA: 10s - loss: 0.1377 - acc: 0.97 - ETA: 10s - loss: 0.1277 - acc: 0.97 - ETA: 10s - loss: 0.1447 - acc: 0.97 - ETA: 11s - loss: 0.1279 - acc: 0.98 - ETA: 11s - loss: 0.1186 - acc: 0.98 - ETA: 11s - loss: 0.1136 - acc: 0.98 - ETA: 10s - loss: 0.1205 - acc: 0.98 - ETA: 10s - loss: 0.1145 - acc: 0.98 - ETA: 10s - loss: 0.1183 - acc: 0.98 - ETA: 10s - loss: 0.1167 - acc: 0.97 - ETA: 10s - loss: 0.1122 - acc: 0.98 - ETA: 10s - loss: 0.1199 - acc: 0.98 - ETA: 10s - loss: 0.1177 - acc: 0.97 - ETA: 10s - loss: 0.1188 - acc: 0.97 - ETA: 9s - loss: 0.1219 - acc: 0.9801 - ETA: 9s - loss: 0.1228 - acc: 0.980 - ETA: 9s - loss: 0.1224 - acc: 0.980 - ETA: 9s - loss: 0.1204 - acc: 0.980 - ETA: 9s - loss: 0.1221 - acc: 0.980 - ETA: 9s - loss: 0.1187 - acc: 0.980 - ETA: 9s - loss: 0.1187 - acc: 0.980 - ETA: 9s - loss: 0.1208 - acc: 0.980 - ETA: 9s - loss: 0.1206 - acc: 0.980 - ETA: 9s - loss: 0.1187 - acc: 0.980 - ETA: 9s - loss: 0.1190 - acc: 0.980 - ETA: 9s - loss: 0.1189 - acc: 0.980 - ETA: 8s - loss: 0.1194 - acc: 0.980 - ETA: 8s - loss: 0.1189 - acc: 0.980 - ETA: 8s - loss: 0.1157 - acc: 0.981 - ETA: 8s - loss: 0.1186 - acc: 0.981 - ETA: 8s - loss: 0.1184 - acc: 0.980 - ETA: 8s - loss: 0.1182 - acc: 0.980 - ETA: 8s - loss: 0.1165 - acc: 0.981 - ETA: 8s - loss: 0.1168 - acc: 0.981 - ETA: 8s - loss: 0.1185 - acc: 0.980 - ETA: 8s - loss: 0.1179 - acc: 0.980 - ETA: 8s - loss: 0.1192 - acc: 0.980 - ETA: 8s - loss: 0.1219 - acc: 0.980 - ETA: 8s - loss: 0.1213 - acc: 0.980 - ETA: 7s - loss: 0.1205 - acc: 0.980 - ETA: 7s - loss: 0.1188 - acc: 0.980 - ETA: 7s - loss: 0.1194 - acc: 0.980 - ETA: 7s - loss: 0.1174 - acc: 0.981 - ETA: 7s - loss: 0.1147 - acc: 0.981 - ETA: 7s - loss: 0.1139 - acc: 0.981 - ETA: 7s - loss: 0.1129 - acc: 0.981 - ETA: 7s - loss: 0.1122 - acc: 0.981 - ETA: 7s - loss: 0.1129 - acc: 0.981 - ETA: 7s - loss: 0.1144 - acc: 0.980 - ETA: 7s - loss: 0.1146 - acc: 0.980 - ETA: 7s - loss: 0.1147 - acc: 0.980 - ETA: 7s - loss: 0.1143 - acc: 0.980 - ETA: 7s - loss: 0.1131 - acc: 0.980 - ETA: 7s - loss: 0.1150 - acc: 0.980 - ETA: 6s - loss: 0.1148 - acc: 0.980 - ETA: 6s - loss: 0.1139 - acc: 0.980 - ETA: 6s - loss: 0.1150 - acc: 0.980 - ETA: 6s - loss: 0.1163 - acc: 0.980 - ETA: 6s - loss: 0.1190 - acc: 0.980 - ETA: 6s - loss: 0.1178 - acc: 0.980 - ETA: 6s - loss: 0.1182 - acc: 0.980 - ETA: 6s - loss: 0.1187 - acc: 0.980 - ETA: 6s - loss: 0.1193 - acc: 0.980 - ETA: 6s - loss: 0.1200 - acc: 0.980 - ETA: 6s - loss: 0.1206 - acc: 0.980 - ETA: 6s - loss: 0.1198 - acc: 0.980 - ETA: 6s - loss: 0.1202 - acc: 0.980 - ETA: 6s - loss: 0.1212 - acc: 0.980 - ETA: 6s - loss: 0.1208 - acc: 0.980 - ETA: 6s - loss: 0.1194 - acc: 0.980 - ETA: 6s - loss: 0.1200 - acc: 0.980 - ETA: 5s - loss: 0.1195 - acc: 0.980 - ETA: 5s - loss: 0.1197 - acc: 0.980 - ETA: 5s - loss: 0.1189 - acc: 0.980 - ETA: 5s - loss: 0.1187 - acc: 0.980 - ETA: 5s - loss: 0.1194 - acc: 0.980 - ETA: 5s - loss: 0.1205 - acc: 0.980 - ETA: 5s - loss: 0.1200 - acc: 0.980 - ETA: 5s - loss: 0.1190 - acc: 0.980 - ETA: 5s - loss: 0.1180 - acc: 0.980 - ETA: 5s - loss: 0.1172 - acc: 0.980 - ETA: 5s - loss: 0.1171 - acc: 0.980 - ETA: 5s - loss: 0.1169 - acc: 0.980 - ETA: 5s - loss: 0.1164 - acc: 0.980 - ETA: 5s - loss: 0.1165 - acc: 0.980 - ETA: 5s - loss: 0.1155 - acc: 0.980 - ETA: 4s - loss: 0.1157 - acc: 0.980 - ETA: 4s - loss: 0.1159 - acc: 0.980 - ETA: 4s - loss: 0.1151 - acc: 0.980 - ETA: 4s - loss: 0.1150 - acc: 0.980 - ETA: 4s - loss: 0.1145 - acc: 0.980 - ETA: 4s - loss: 0.1151 - acc: 0.980 - ETA: 4s - loss: 0.1151 - acc: 0.980 - ETA: 4s - loss: 0.1158 - acc: 0.980 - ETA: 4s - loss: 0.1155 - acc: 0.980 - ETA: 4s - loss: 0.1160 - acc: 0.980 - ETA: 4s - loss: 0.1158 - acc: 0.980 - ETA: 4s - loss: 0.1172 - acc: 0.980 - ETA: 4s - loss: 0.1172 - acc: 0.980 - ETA: 3s - loss: 0.1170 - acc: 0.980 - ETA: 3s - loss: 0.1167 - acc: 0.980 - ETA: 3s - loss: 0.1164 - acc: 0.980 - ETA: 3s - loss: 0.1157 - acc: 0.980 - ETA: 3s - loss: 0.1165 - acc: 0.980 - ETA: 3s - loss: 0.1161 - acc: 0.980 - ETA: 3s - loss: 0.1161 - acc: 0.980 - ETA: 3s - loss: 0.1155 - acc: 0.980 - ETA: 3s - loss: 0.1157 - acc: 0.980 - ETA: 3s - loss: 0.1159 - acc: 0.980 - ETA: 3s - loss: 0.1158 - acc: 0.980 - ETA: 3s - loss: 0.1159 - acc: 0.980 - ETA: 3s - loss: 0.1153 - acc: 0.980 - ETA: 3s - loss: 0.1158 - acc: 0.980 - ETA: 3s - loss: 0.1159 - acc: 0.980 - ETA: 3s - loss: 0.1159 - acc: 0.980 - ETA: 3s - loss: 0.1152 - acc: 0.980 - ETA: 2s - loss: 0.1147 - acc: 0.980 - ETA: 2s - loss: 0.1160 - acc: 0.980 - ETA: 2s - loss: 0.1156 - acc: 0.980 - ETA: 2s - loss: 0.1160 - acc: 0.980 - ETA: 2s - loss: 0.1157 - acc: 0.980 - ETA: 2s - loss: 0.1160 - acc: 0.980 - ETA: 2s - loss: 0.1159 - acc: 0.980 - ETA: 2s - loss: 0.1157 - acc: 0.980 - ETA: 2s - loss: 0.1154 - acc: 0.980 - ETA: 2s - loss: 0.1158 - acc: 0.980 - ETA: 2s - loss: 0.1153 - acc: 0.980 - ETA: 2s - loss: 0.1159 - acc: 0.980 - ETA: 2s - loss: 0.1163 - acc: 0.980 - ETA: 2s - loss: 0.1161 - acc: 0.980 - ETA: 1s - loss: 0.1157 - acc: 0.980 - ETA: 1s - loss: 0.1152 - acc: 0.980 - ETA: 1s - loss: 0.1160 - acc: 0.980 - ETA: 1s - loss: 0.1161 - acc: 0.980 - ETA: 1s - loss: 0.1168 - acc: 0.980 - ETA: 1s - loss: 0.1169 - acc: 0.980 - ETA: 1s - loss: 0.1173 - acc: 0.979 - ETA: 1s - loss: 0.1171 - acc: 0.980 - ETA: 1s - loss: 0.1170 - acc: 0.980 - ETA: 1s - loss: 0.1171 - acc: 0.980 - ETA: 1s - loss: 0.1180 - acc: 0.979 - ETA: 1s - loss: 0.1190 - acc: 0.979 - ETA: 1s - loss: 0.1190 - acc: 0.979 - ETA: 1s - loss: 0.1190 - acc: 0.979 - ETA: 1s - loss: 0.1189 - acc: 0.979 - ETA: 0s - loss: 0.1188 - acc: 0.979 - ETA: 0s - loss: 0.1192 - acc: 0.979 - ETA: 0s - loss: 0.1190 - acc: 0.979 - ETA: 0s - loss: 0.1192 - acc: 0.979 - ETA: 0s - loss: 0.1191 - acc: 0.979 - ETA: 0s - loss: 0.1192 - acc: 0.979 - ETA: 0s - loss: 0.1191 - acc: 0.979 - ETA: 0s - loss: 0.1194 - acc: 0.979 - ETA: 0s - loss: 0.1192 - acc: 0.979 - ETA: 0s - loss: 0.1188 - acc: 0.979 - ETA: 0s - loss: 0.1187 - acc: 0.979 - ETA: 0s - loss: 0.1185 - acc: 0.979 - ETA: 0s - loss: 0.1188 - acc: 0.979 - ETA: 0s - loss: 0.1186 - acc: 0.979 - 12s 239us/step - loss: 0.1194 - acc: 0.9793 - val_loss: 0.0906 - val_acc: 0.9872\n",
      "Epoch 4/5\n",
      "48200/48200 [==============================] - ETA: 11s - loss: 0.0671 - acc: 0.99 - ETA: 11s - loss: 0.1038 - acc: 0.98 - ETA: 11s - loss: 0.1461 - acc: 0.97 - ETA: 11s - loss: 0.1395 - acc: 0.97 - ETA: 11s - loss: 0.1427 - acc: 0.97 - ETA: 11s - loss: 0.1270 - acc: 0.97 - ETA: 10s - loss: 0.1535 - acc: 0.97 - ETA: 10s - loss: 0.1469 - acc: 0.97 - ETA: 10s - loss: 0.1418 - acc: 0.97 - ETA: 10s - loss: 0.1376 - acc: 0.97 - ETA: 10s - loss: 0.1427 - acc: 0.97 - ETA: 10s - loss: 0.1463 - acc: 0.97 - ETA: 10s - loss: 0.1455 - acc: 0.97 - ETA: 10s - loss: 0.1372 - acc: 0.97 - ETA: 9s - loss: 0.1413 - acc: 0.9773 - ETA: 9s - loss: 0.1367 - acc: 0.977 - ETA: 9s - loss: 0.1344 - acc: 0.977 - ETA: 9s - loss: 0.1353 - acc: 0.977 - ETA: 9s - loss: 0.1357 - acc: 0.977 - ETA: 9s - loss: 0.1344 - acc: 0.976 - ETA: 9s - loss: 0.1341 - acc: 0.976 - ETA: 9s - loss: 0.1335 - acc: 0.976 - ETA: 9s - loss: 0.1301 - acc: 0.977 - ETA: 9s - loss: 0.1342 - acc: 0.977 - ETA: 9s - loss: 0.1349 - acc: 0.977 - ETA: 9s - loss: 0.1300 - acc: 0.977 - ETA: 9s - loss: 0.1284 - acc: 0.977 - ETA: 8s - loss: 0.1255 - acc: 0.977 - ETA: 8s - loss: 0.1241 - acc: 0.978 - ETA: 8s - loss: 0.1223 - acc: 0.978 - ETA: 8s - loss: 0.1219 - acc: 0.978 - ETA: 8s - loss: 0.1199 - acc: 0.978 - ETA: 8s - loss: 0.1176 - acc: 0.979 - ETA: 8s - loss: 0.1150 - acc: 0.979 - ETA: 8s - loss: 0.1136 - acc: 0.979 - ETA: 8s - loss: 0.1130 - acc: 0.979 - ETA: 8s - loss: 0.1126 - acc: 0.979 - ETA: 8s - loss: 0.1145 - acc: 0.979 - ETA: 8s - loss: 0.1137 - acc: 0.979 - ETA: 8s - loss: 0.1123 - acc: 0.980 - ETA: 8s - loss: 0.1109 - acc: 0.980 - ETA: 8s - loss: 0.1079 - acc: 0.980 - ETA: 8s - loss: 0.1072 - acc: 0.981 - ETA: 8s - loss: 0.1084 - acc: 0.981 - ETA: 8s - loss: 0.1105 - acc: 0.981 - ETA: 8s - loss: 0.1086 - acc: 0.981 - ETA: 7s - loss: 0.1105 - acc: 0.981 - ETA: 7s - loss: 0.1101 - acc: 0.981 - ETA: 7s - loss: 0.1099 - acc: 0.981 - ETA: 7s - loss: 0.1101 - acc: 0.981 - ETA: 7s - loss: 0.1107 - acc: 0.981 - ETA: 7s - loss: 0.1092 - acc: 0.981 - ETA: 7s - loss: 0.1098 - acc: 0.981 - ETA: 7s - loss: 0.1105 - acc: 0.981 - ETA: 7s - loss: 0.1095 - acc: 0.981 - ETA: 7s - loss: 0.1119 - acc: 0.981 - ETA: 7s - loss: 0.1132 - acc: 0.981 - ETA: 7s - loss: 0.1122 - acc: 0.981 - ETA: 7s - loss: 0.1128 - acc: 0.981 - ETA: 6s - loss: 0.1109 - acc: 0.981 - ETA: 6s - loss: 0.1104 - acc: 0.981 - ETA: 6s - loss: 0.1109 - acc: 0.981 - ETA: 6s - loss: 0.1096 - acc: 0.981 - ETA: 6s - loss: 0.1097 - acc: 0.981 - ETA: 6s - loss: 0.1119 - acc: 0.981 - ETA: 6s - loss: 0.1109 - acc: 0.981 - ETA: 6s - loss: 0.1114 - acc: 0.981 - ETA: 6s - loss: 0.1122 - acc: 0.981 - ETA: 6s - loss: 0.1112 - acc: 0.981 - ETA: 6s - loss: 0.1135 - acc: 0.981 - ETA: 6s - loss: 0.1137 - acc: 0.981 - ETA: 6s - loss: 0.1130 - acc: 0.981 - ETA: 5s - loss: 0.1141 - acc: 0.980 - ETA: 5s - loss: 0.1135 - acc: 0.981 - ETA: 5s - loss: 0.1142 - acc: 0.980 - ETA: 5s - loss: 0.1140 - acc: 0.980 - ETA: 5s - loss: 0.1130 - acc: 0.980 - ETA: 5s - loss: 0.1122 - acc: 0.980 - ETA: 5s - loss: 0.1125 - acc: 0.980 - ETA: 5s - loss: 0.1116 - acc: 0.981 - ETA: 5s - loss: 0.1118 - acc: 0.981 - ETA: 5s - loss: 0.1124 - acc: 0.981 - ETA: 5s - loss: 0.1120 - acc: 0.981 - ETA: 5s - loss: 0.1118 - acc: 0.981 - ETA: 5s - loss: 0.1121 - acc: 0.981 - ETA: 5s - loss: 0.1118 - acc: 0.981 - ETA: 4s - loss: 0.1129 - acc: 0.981 - ETA: 4s - loss: 0.1123 - acc: 0.981 - ETA: 4s - loss: 0.1121 - acc: 0.981 - ETA: 4s - loss: 0.1123 - acc: 0.981 - ETA: 4s - loss: 0.1113 - acc: 0.981 - ETA: 4s - loss: 0.1112 - acc: 0.981 - ETA: 4s - loss: 0.1121 - acc: 0.981 - ETA: 4s - loss: 0.1128 - acc: 0.981 - ETA: 4s - loss: 0.1115 - acc: 0.981 - ETA: 4s - loss: 0.1114 - acc: 0.981 - ETA: 4s - loss: 0.1107 - acc: 0.981 - ETA: 4s - loss: 0.1103 - acc: 0.981 - ETA: 4s - loss: 0.1099 - acc: 0.981 - ETA: 4s - loss: 0.1104 - acc: 0.981 - ETA: 3s - loss: 0.1108 - acc: 0.981 - ETA: 3s - loss: 0.1112 - acc: 0.981 - ETA: 3s - loss: 0.1107 - acc: 0.981 - ETA: 3s - loss: 0.1112 - acc: 0.981 - ETA: 3s - loss: 0.1111 - acc: 0.981 - ETA: 3s - loss: 0.1120 - acc: 0.981 - ETA: 3s - loss: 0.1117 - acc: 0.981 - ETA: 3s - loss: 0.1119 - acc: 0.981 - ETA: 3s - loss: 0.1124 - acc: 0.981 - ETA: 3s - loss: 0.1123 - acc: 0.981 - ETA: 3s - loss: 0.1113 - acc: 0.981 - ETA: 3s - loss: 0.1108 - acc: 0.981 - ETA: 3s - loss: 0.1106 - acc: 0.981 - ETA: 3s - loss: 0.1103 - acc: 0.981 - ETA: 2s - loss: 0.1105 - acc: 0.981 - ETA: 2s - loss: 0.1112 - acc: 0.981 - ETA: 2s - loss: 0.1115 - acc: 0.981 - ETA: 2s - loss: 0.1117 - acc: 0.981 - ETA: 2s - loss: 0.1113 - acc: 0.981 - ETA: 2s - loss: 0.1118 - acc: 0.981 - ETA: 2s - loss: 0.1112 - acc: 0.981 - ETA: 2s - loss: 0.1116 - acc: 0.981 - ETA: 2s - loss: 0.1113 - acc: 0.981 - ETA: 2s - loss: 0.1111 - acc: 0.981 - ETA: 2s - loss: 0.1106 - acc: 0.981 - ETA: 2s - loss: 0.1110 - acc: 0.981 - ETA: 2s - loss: 0.1112 - acc: 0.981 - ETA: 2s - loss: 0.1110 - acc: 0.981 - ETA: 2s - loss: 0.1104 - acc: 0.981 - ETA: 1s - loss: 0.1109 - acc: 0.981 - ETA: 1s - loss: 0.1103 - acc: 0.981 - ETA: 1s - loss: 0.1109 - acc: 0.981 - ETA: 1s - loss: 0.1120 - acc: 0.981 - ETA: 1s - loss: 0.1118 - acc: 0.981 - ETA: 1s - loss: 0.1126 - acc: 0.981 - ETA: 1s - loss: 0.1121 - acc: 0.981 - ETA: 1s - loss: 0.1118 - acc: 0.981 - ETA: 1s - loss: 0.1120 - acc: 0.981 - ETA: 1s - loss: 0.1122 - acc: 0.981 - ETA: 1s - loss: 0.1121 - acc: 0.981 - ETA: 1s - loss: 0.1124 - acc: 0.981 - ETA: 1s - loss: 0.1122 - acc: 0.981 - ETA: 1s - loss: 0.1123 - acc: 0.981 - ETA: 0s - loss: 0.1123 - acc: 0.981 - ETA: 0s - loss: 0.1122 - acc: 0.981 - ETA: 0s - loss: 0.1124 - acc: 0.981 - ETA: 0s - loss: 0.1122 - acc: 0.981 - ETA: 0s - loss: 0.1124 - acc: 0.981 - ETA: 0s - loss: 0.1124 - acc: 0.981 - ETA: 0s - loss: 0.1129 - acc: 0.981 - ETA: 0s - loss: 0.1132 - acc: 0.981 - ETA: 0s - loss: 0.1131 - acc: 0.981 - ETA: 0s - loss: 0.1129 - acc: 0.981 - ETA: 0s - loss: 0.1127 - acc: 0.981 - ETA: 0s - loss: 0.1126 - acc: 0.981 - ETA: 0s - loss: 0.1123 - acc: 0.981 - ETA: 0s - loss: 0.1122 - acc: 0.981 - ETA: 0s - loss: 0.1119 - acc: 0.981 - ETA: 0s - loss: 0.1118 - acc: 0.981 - ETA: 0s - loss: 0.1118 - acc: 0.981 - ETA: 0s - loss: 0.1117 - acc: 0.981 - 12s 239us/step - loss: 0.1116 - acc: 0.9813 - val_loss: 0.0684 - val_acc: 0.9896\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48200/48200 [==============================] - ETA: 11s - loss: 0.1470 - acc: 0.98 - ETA: 11s - loss: 0.1403 - acc: 0.97 - ETA: 11s - loss: 0.1047 - acc: 0.97 - ETA: 11s - loss: 0.0904 - acc: 0.98 - ETA: 11s - loss: 0.1151 - acc: 0.98 - ETA: 11s - loss: 0.1262 - acc: 0.98 - ETA: 11s - loss: 0.1123 - acc: 0.98 - ETA: 11s - loss: 0.1108 - acc: 0.98 - ETA: 10s - loss: 0.1020 - acc: 0.98 - ETA: 10s - loss: 0.1017 - acc: 0.98 - ETA: 10s - loss: 0.1046 - acc: 0.98 - ETA: 10s - loss: 0.1079 - acc: 0.98 - ETA: 10s - loss: 0.1109 - acc: 0.98 - ETA: 10s - loss: 0.1207 - acc: 0.98 - ETA: 10s - loss: 0.1164 - acc: 0.98 - ETA: 10s - loss: 0.1171 - acc: 0.98 - ETA: 10s - loss: 0.1121 - acc: 0.98 - ETA: 10s - loss: 0.1149 - acc: 0.98 - ETA: 10s - loss: 0.1130 - acc: 0.98 - ETA: 10s - loss: 0.1168 - acc: 0.97 - ETA: 10s - loss: 0.1176 - acc: 0.97 - ETA: 9s - loss: 0.1130 - acc: 0.9797 - ETA: 10s - loss: 0.1100 - acc: 0.98 - ETA: 9s - loss: 0.1102 - acc: 0.9802 - ETA: 9s - loss: 0.1098 - acc: 0.980 - ETA: 9s - loss: 0.1088 - acc: 0.980 - ETA: 9s - loss: 0.1112 - acc: 0.980 - ETA: 9s - loss: 0.1122 - acc: 0.980 - ETA: 9s - loss: 0.1157 - acc: 0.980 - ETA: 9s - loss: 0.1132 - acc: 0.980 - ETA: 9s - loss: 0.1128 - acc: 0.980 - ETA: 9s - loss: 0.1155 - acc: 0.980 - ETA: 9s - loss: 0.1166 - acc: 0.980 - ETA: 9s - loss: 0.1165 - acc: 0.980 - ETA: 9s - loss: 0.1135 - acc: 0.981 - ETA: 9s - loss: 0.1128 - acc: 0.981 - ETA: 9s - loss: 0.1135 - acc: 0.981 - ETA: 8s - loss: 0.1136 - acc: 0.981 - ETA: 8s - loss: 0.1153 - acc: 0.980 - ETA: 8s - loss: 0.1145 - acc: 0.980 - ETA: 8s - loss: 0.1136 - acc: 0.980 - ETA: 8s - loss: 0.1130 - acc: 0.980 - ETA: 8s - loss: 0.1128 - acc: 0.981 - ETA: 8s - loss: 0.1122 - acc: 0.981 - ETA: 8s - loss: 0.1133 - acc: 0.980 - ETA: 8s - loss: 0.1114 - acc: 0.980 - ETA: 8s - loss: 0.1109 - acc: 0.980 - ETA: 8s - loss: 0.1101 - acc: 0.980 - ETA: 8s - loss: 0.1091 - acc: 0.981 - ETA: 8s - loss: 0.1123 - acc: 0.980 - ETA: 8s - loss: 0.1106 - acc: 0.980 - ETA: 7s - loss: 0.1097 - acc: 0.980 - ETA: 7s - loss: 0.1144 - acc: 0.980 - ETA: 7s - loss: 0.1131 - acc: 0.980 - ETA: 7s - loss: 0.1125 - acc: 0.980 - ETA: 7s - loss: 0.1132 - acc: 0.980 - ETA: 7s - loss: 0.1131 - acc: 0.980 - ETA: 7s - loss: 0.1151 - acc: 0.980 - ETA: 7s - loss: 0.1146 - acc: 0.980 - ETA: 7s - loss: 0.1128 - acc: 0.980 - ETA: 7s - loss: 0.1128 - acc: 0.980 - ETA: 7s - loss: 0.1128 - acc: 0.980 - ETA: 6s - loss: 0.1127 - acc: 0.980 - ETA: 6s - loss: 0.1115 - acc: 0.980 - ETA: 6s - loss: 0.1122 - acc: 0.980 - ETA: 6s - loss: 0.1114 - acc: 0.981 - ETA: 6s - loss: 0.1102 - acc: 0.981 - ETA: 6s - loss: 0.1098 - acc: 0.981 - ETA: 6s - loss: 0.1104 - acc: 0.981 - ETA: 6s - loss: 0.1111 - acc: 0.981 - ETA: 6s - loss: 0.1108 - acc: 0.981 - ETA: 6s - loss: 0.1114 - acc: 0.981 - ETA: 6s - loss: 0.1109 - acc: 0.981 - ETA: 6s - loss: 0.1097 - acc: 0.981 - ETA: 6s - loss: 0.1084 - acc: 0.981 - ETA: 5s - loss: 0.1088 - acc: 0.981 - ETA: 5s - loss: 0.1082 - acc: 0.981 - ETA: 5s - loss: 0.1079 - acc: 0.981 - ETA: 5s - loss: 0.1078 - acc: 0.981 - ETA: 5s - loss: 0.1092 - acc: 0.981 - ETA: 5s - loss: 0.1087 - acc: 0.981 - ETA: 5s - loss: 0.1082 - acc: 0.981 - ETA: 5s - loss: 0.1073 - acc: 0.981 - ETA: 5s - loss: 0.1073 - acc: 0.981 - ETA: 5s - loss: 0.1072 - acc: 0.981 - ETA: 5s - loss: 0.1078 - acc: 0.981 - ETA: 5s - loss: 0.1072 - acc: 0.981 - ETA: 5s - loss: 0.1071 - acc: 0.981 - ETA: 5s - loss: 0.1075 - acc: 0.981 - ETA: 4s - loss: 0.1073 - acc: 0.981 - ETA: 4s - loss: 0.1069 - acc: 0.981 - ETA: 4s - loss: 0.1065 - acc: 0.981 - ETA: 4s - loss: 0.1067 - acc: 0.981 - ETA: 4s - loss: 0.1065 - acc: 0.981 - ETA: 4s - loss: 0.1064 - acc: 0.981 - ETA: 4s - loss: 0.1076 - acc: 0.981 - ETA: 4s - loss: 0.1080 - acc: 0.981 - ETA: 4s - loss: 0.1083 - acc: 0.981 - ETA: 4s - loss: 0.1076 - acc: 0.981 - ETA: 4s - loss: 0.1072 - acc: 0.981 - ETA: 4s - loss: 0.1073 - acc: 0.982 - ETA: 4s - loss: 0.1083 - acc: 0.981 - ETA: 3s - loss: 0.1081 - acc: 0.981 - ETA: 3s - loss: 0.1083 - acc: 0.981 - ETA: 3s - loss: 0.1079 - acc: 0.981 - ETA: 3s - loss: 0.1072 - acc: 0.982 - ETA: 3s - loss: 0.1067 - acc: 0.982 - ETA: 3s - loss: 0.1081 - acc: 0.981 - ETA: 3s - loss: 0.1093 - acc: 0.981 - ETA: 3s - loss: 0.1092 - acc: 0.981 - ETA: 3s - loss: 0.1087 - acc: 0.981 - ETA: 3s - loss: 0.1086 - acc: 0.981 - ETA: 3s - loss: 0.1081 - acc: 0.981 - ETA: 3s - loss: 0.1089 - acc: 0.981 - ETA: 3s - loss: 0.1093 - acc: 0.981 - ETA: 2s - loss: 0.1087 - acc: 0.981 - ETA: 2s - loss: 0.1089 - acc: 0.981 - ETA: 2s - loss: 0.1093 - acc: 0.981 - ETA: 2s - loss: 0.1100 - acc: 0.981 - ETA: 2s - loss: 0.1095 - acc: 0.981 - ETA: 2s - loss: 0.1093 - acc: 0.981 - ETA: 2s - loss: 0.1096 - acc: 0.981 - ETA: 2s - loss: 0.1099 - acc: 0.981 - ETA: 2s - loss: 0.1098 - acc: 0.981 - ETA: 2s - loss: 0.1096 - acc: 0.981 - ETA: 2s - loss: 0.1110 - acc: 0.981 - ETA: 2s - loss: 0.1135 - acc: 0.980 - ETA: 2s - loss: 0.1131 - acc: 0.980 - ETA: 2s - loss: 0.1153 - acc: 0.980 - ETA: 1s - loss: 0.1155 - acc: 0.980 - ETA: 1s - loss: 0.1163 - acc: 0.980 - ETA: 1s - loss: 0.1168 - acc: 0.980 - ETA: 1s - loss: 0.1176 - acc: 0.980 - ETA: 1s - loss: 0.1177 - acc: 0.979 - ETA: 1s - loss: 0.1179 - acc: 0.979 - ETA: 1s - loss: 0.1192 - acc: 0.979 - ETA: 1s - loss: 0.1192 - acc: 0.979 - ETA: 1s - loss: 0.1199 - acc: 0.979 - ETA: 1s - loss: 0.1203 - acc: 0.979 - ETA: 1s - loss: 0.1202 - acc: 0.979 - ETA: 1s - loss: 0.1214 - acc: 0.979 - ETA: 1s - loss: 0.1223 - acc: 0.979 - ETA: 1s - loss: 0.1227 - acc: 0.979 - ETA: 0s - loss: 0.1230 - acc: 0.979 - ETA: 0s - loss: 0.1241 - acc: 0.979 - ETA: 0s - loss: 0.1237 - acc: 0.979 - ETA: 0s - loss: 0.1242 - acc: 0.979 - ETA: 0s - loss: 0.1235 - acc: 0.979 - ETA: 0s - loss: 0.1236 - acc: 0.979 - ETA: 0s - loss: 0.1239 - acc: 0.979 - ETA: 0s - loss: 0.1235 - acc: 0.979 - ETA: 0s - loss: 0.1243 - acc: 0.979 - ETA: 0s - loss: 0.1240 - acc: 0.979 - ETA: 0s - loss: 0.1239 - acc: 0.979 - ETA: 0s - loss: 0.1235 - acc: 0.979 - ETA: 0s - loss: 0.1238 - acc: 0.979 - 11s 235us/step - loss: 0.1235 - acc: 0.9792 - val_loss: 0.1446 - val_acc: 0.9825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x212c22549e8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [x_train],\n",
    "    [y_train],\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    validation_data=([x_test],[y_test])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The piece of code below returns the predicted accuracy of the model on the validation set. The validation set is composed of pairs of images and the model has to decide if this pair of images represents the same images or not. The accuracy evaluates its ability to correctly indentify a good pair in the model or correctly rejecting a bad one. We compute the distance between two embeddings vectors to compute the model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x212b991bfd0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x212b9918080>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x212b9918240>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x212b9918ba8>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x212b99187b8>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x212b9918860>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x212b9924ef0>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x212b9962d30>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x212b9962f60>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = tf.keras.Model(model.layers[0].input, model.layers[-2].output)\n",
    "mod.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.222469   2.2506027  2.660291 ]\n",
      "[-1.2795771  1.4958158  2.2156096]\n",
      "[-0.92281145  2.2994103   1.825941  ]\n",
      "[-1.604319   1.5813861  2.9419658]\n",
      "[-0.25879228  1.0201457   1.8521013 ]\n",
      "[-1.8367114  2.278872   3.148376 ]\n",
      "[-0.6717158   0.36145443  1.0905459 ]\n",
      "[-1.0725374  1.3340237  1.9719963]\n",
      "[-1.069055   1.5998819  1.9537525]\n",
      "[-1.8593919  2.2822096  3.9964497]\n",
      "[-2.418324   2.1673317  3.5004356]\n",
      "[-2.13125    2.0738149  3.6639445]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    if np.argmax(y_train[i])==4:\n",
    "        print(predict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.035328  , -3.7085228 ,  0.9583403 ],\n",
       "       [ 0.30475223, -3.8464775 , -2.4406123 ],\n",
       "       [-1.222469  ,  2.2506027 ,  2.660291  ],\n",
       "       ...,\n",
       "       [ 8.959661  , -4.08996   , -3.5287194 ],\n",
       "       [-1.6217207 , -6.745757  ,  4.4861174 ],\n",
       "       [-2.7662365 , -1.4557369 , -0.3838599 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict=mod.predict(x_train)\n",
    "print(len(predict[0]))\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1400000000000001\n",
      "0.7166\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(predict)):\n",
    "    predict[i] = predict[i]/np.linalg.norm(predict[i])\n",
    "\n",
    "# Normalizes\n",
    "#emb = tf.math.l2_normalize(emb_,0)\n",
    "\n",
    "# Separates the pairs\n",
    "emb1 = predict[0::2]\n",
    "emb2 = predict[1::2]\n",
    "\n",
    "# Computes distance between pairs\n",
    "diff = np.square(emb1-emb2)\n",
    "dist = np.sum(diff,1)\n",
    "\n",
    "best = 0\n",
    "best_t = 0\n",
    "thresholds = np.arange(0,4,0.01)\n",
    "for i in range(len(thresholds)):\n",
    "    less = np.less(dist, thresholds[i])\n",
    "    acc = np.logical_not(np.logical_xor(less, issame[0::2]))\n",
    "    acc = acc.astype(float)\n",
    "    out = np.sum(acc)\n",
    "    out = out/len(acc)\n",
    "    if out > best:\n",
    "        best_t = thresholds[i]\n",
    "        best = out\n",
    "\n",
    "print(best_t)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For out=8\n",
      "[ 0.6789282  -0.20618166 -0.7046599 ]\n",
      "[ 0.01885007 -0.8925208  -0.45061222]\n",
      "[-0.37101886 -0.927535    0.04498752]\n",
      "[ 0.7092516 -0.275286  -0.6489836]\n",
      "[ 0.4349809   0.79985225 -0.41355532]\n",
      "[ 0.09678148 -0.86734384 -0.48820907]\n",
      "[ 0.95458746 -0.01850358 -0.29735595]\n",
      "[ 0.27384564 -0.811192   -0.51669735]\n",
      "[-0.38721666  0.8883745  -0.24668595]\n",
      "[ 0.38524598 -0.8787014  -0.2819031 ]\n",
      "[0.06257431 0.9519171  0.29989713]\n",
      "[ 0.69298357 -0.3861992  -0.60878897]\n",
      "[ 0.37679875 -0.76613104 -0.5206399 ]\n",
      "[ 0.16391426 -0.91319406 -0.37310693]\n",
      "[ 0.5471594   0.59842885 -0.5852346 ]\n",
      "[ 0.90900916  0.04971976 -0.41379988]\n",
      "[ 0.8012651   0.15588956 -0.577644  ]\n",
      "[0.17459138 0.3657722  0.9141819 ]\n",
      "[ 0.44113278  0.5636355  -0.6983673 ]\n",
      "[-0.01781899 -0.93439007 -0.35580558]\n",
      "[-0.39170986 -0.7252056   0.56625104]\n",
      "[-0.4807252  -0.84071404  0.24920523]\n",
      "[-0.10991403  0.77207804  0.62595075]\n",
      "[ 0.8042612   0.4046466  -0.43522993]\n",
      "[ 0.6808763   0.53806525 -0.49688348]\n",
      "[-0.19593625 -0.97986597 -0.03836231]\n",
      "[-0.85305536 -0.5010146  -0.14587976]\n",
      "[ 0.55608356 -0.58634704 -0.5890401 ]\n",
      "[-0.6797449  -0.70071244  0.21667698]\n",
      "[ 0.49976906  0.65418214 -0.5676941 ]\n",
      "[ 0.40642375 -0.87170565 -0.27376795]\n",
      "[ 0.46408883 -0.5877876  -0.66266674]\n",
      "[ 0.01538304 -0.904089   -0.4270674 ]\n",
      "[ 0.32089454 -0.8922546   0.3176608 ]\n",
      "[ 0.7748759  -0.11252462 -0.6220173 ]\n",
      "[ 0.5885659   0.5484428  -0.59397036]\n",
      "[ 0.6000847  -0.21927735  0.7692957 ]\n",
      "[ 0.5087191   0.15766352 -0.8463729 ]\n",
      "[ 0.5972444   0.49634686 -0.630031  ]\n",
      "[-0.3702126  0.7208534 -0.5859293]\n",
      "[ 0.7750224 -0.3811463 -0.5040514]\n",
      "[ 0.4272155   0.72849166 -0.5355247 ]\n",
      "[ 0.56305754  0.58752155 -0.5811924 ]\n",
      "[ 0.57883054  0.6432248  -0.5012156 ]\n",
      "[-0.05366481 -0.99228776  0.11173613]\n",
      "For out=9\n",
      "[0.6187602  0.40369278 0.67391974]\n",
      "[0.5650259  0.21115635 0.79759556]\n",
      "[0.40705454 0.5370682  0.73882633]\n",
      "[-0.33450192  0.3997483   0.8534107 ]\n",
      "[-0.05587399  0.84026974  0.53928185]\n",
      "[0.54747766 0.39305815 0.7387648 ]\n",
      "[-0.20995767  0.41860887  0.8835635 ]\n",
      "[-0.1547298  0.5076044  0.8475827]\n",
      "[ 0.5310765  -0.72105867  0.4450081 ]\n",
      "[-0.38878298  0.23649092  0.89046043]\n",
      "[0.48237106 0.5521136  0.68006516]\n",
      "[0.6047154 0.4652727 0.6464059]\n",
      "[ 0.7758148  -0.6290089   0.04959109]\n",
      "[-0.2431251  0.6301811  0.7374023]\n",
      "[0.27483904 0.34525537 0.897364  ]\n",
      "[0.25326508 0.54107356 0.8019329 ]\n",
      "[-0.1730298   0.59554696  0.78446454]\n",
      "[0.4749074  0.4738546  0.74156904]\n",
      "[0.5393388  0.18571821 0.821354  ]\n",
      "[-0.03033677  0.39762333  0.9170471 ]\n",
      "[0.5597618  0.44807363 0.69706297]\n",
      "[-0.3522731  0.4507455  0.8202025]\n",
      "[-0.03586073 -0.08897814  0.9953878 ]\n",
      "[0.96161026 0.12797745 0.24274975]\n",
      "[0.70493984 0.20558871 0.6788174 ]\n",
      "[-0.25914374 -0.73865336  0.6222827 ]\n",
      "[-0.14827742  0.35199344  0.92418313]\n",
      "[0.02390355 0.7337391  0.6790107 ]\n",
      "[ 0.28922996 -0.75156957 -0.59286535]\n",
      "[-0.16701242  0.47795635  0.8623599 ]\n",
      "[0.72914857 0.18189633 0.65973943]\n",
      "[0.6675146  0.38029554 0.64015585]\n",
      "[-0.2922964   0.7004128   0.65114105]\n",
      "[-0.4775776  -0.01178533  0.87851053]\n",
      "[-0.09888244 -0.06482077  0.99298567]\n",
      "[0.64544725 0.39928344 0.65113014]\n",
      "[-0.47665638 -0.43640834  0.76311624]\n",
      "[0.59029627 0.45532924 0.66650254]\n",
      "[0.36173353 0.29835972 0.8832499 ]\n",
      "[-0.28410664  0.5741214   0.76789844]\n",
      "[0.4530853  0.48133028 0.75035644]\n",
      "[0.5596762  0.44883043 0.69664466]\n",
      "[-0.34669018  0.5850073   0.7331933 ]\n",
      "[0.3695403  0.5279102  0.76469004]\n",
      "[0.0026267  0.45059168 0.89272624]\n",
      "[0.4503543  0.24386124 0.858902  ]\n",
      "[ 0.03492937 -0.5729827   0.8188228 ]\n",
      "[0.6007986  0.36848179 0.7094098 ]\n",
      "[-0.22689962  0.5792133   0.78296137]\n",
      "[0.66938597 0.43537536 0.60197234]\n",
      "[-0.07228711  0.72844094  0.68128437]\n",
      "[-0.15031804 -0.84533024 -0.512661  ]\n",
      "[ 0.4238166  -0.14913942  0.8933851 ]\n",
      "[-0.20057115  0.5191812   0.83079606]\n",
      "[-0.28098342  0.6433215   0.7121697 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"For out=8\")\n",
    "for i in range(100):\n",
    "    if y_pairs[i][8]==1:\n",
    "        print(predict[i])\n",
    "print(\"For out=9\")\n",
    "for i in range(100):\n",
    "    if y_pairs[i][9]==1:\n",
    "        print(predict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\guillaume\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        nan         nan         nan]\n",
      " [        nan         nan         nan]\n",
      " [        nan         nan         nan]\n",
      " [        nan         nan         nan]\n",
      " [        nan         nan         nan]\n",
      " [        nan         nan         nan]\n",
      " [        nan         nan         nan]\n",
      " [        nan         nan         nan]\n",
      " [ 0.28930569 -0.46553033 -0.10190845]\n",
      " [ 0.36381942  0.33873959  0.75985286]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x212c0e61128>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "emb_size = 3\n",
    "\n",
    "mean = np.zeros((10,emb_size))\n",
    "nb = np.zeros(10)\n",
    "y_pairs = y_pairs.astype(int)\n",
    "y_int = np.empty(len(y_pairs),dtype=int)\n",
    "for i in range(len(y_pairs)):\n",
    "    idx = 0\n",
    "    while (y_pairs[i][idx]!=1):\n",
    "        idx += 1\n",
    "    y_int[i] = int(idx)\n",
    "\n",
    "for i in range(len(predict)):\n",
    "    mean[y_int[i]] += predict[i]\n",
    "    nb[y_int[i]] += 1\n",
    "\n",
    "for i in range(len(mean)):\n",
    "    mean[i] /= nb[i]\n",
    "print(mean)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.axis('equal')\n",
    "# for i in range(10):\n",
    "#     y = predict[np.equal(i,y_int)]\n",
    "#     ax.scatter(y[:,0],y[:,1],y[:,2])\n",
    "ax.scatter(mean[:,0],mean[:,1],mean[:,2])\n",
    "# plt.axis('equal')\n",
    "# for i in range(1,4,2):\n",
    "#     y = predict[np.equal(i,y_int)]\n",
    "#     plt.plot(y[:,0],y[:,1],'o')\n",
    "#plt.plot(mean[:,0],mean[:,1],'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0885650146484375\n"
     ]
    }
   ],
   "source": [
    "print(np.sum([np.sum(predict[i] for i in range(len(predict)))])/len(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "0.46860551834106445\n",
      "0.63699996\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "emb_raw = tf.constant(predict)\n",
    "\n",
    "actual_issame = tf.constant(issame)\n",
    "emb = tf.math.l2_normalize(emb_raw,0)\n",
    "\n",
    "# Normalizes\n",
    "#emb = tf.math.l2_normalize(emb_,0)\n",
    "\n",
    "# Separates the pairs\n",
    "emb1 = emb[0::2]\n",
    "emb2 = emb[1::2]\n",
    "\n",
    "# Computes distance between pairs\n",
    "diff = tf.squared_difference(emb1,emb2)\n",
    "dist = tf.reduce_sum(diff,1)\n",
    "\n",
    "dist = tf.reshape(tf.stack([dist,dist], axis=-1), [-1])\n",
    "print(dist.shape)\n",
    "best_threshold = 0\n",
    "#for t in np.arange(0,1,0.001):\n",
    "#t = 0.01\n",
    "\n",
    "actual_issame_bool = tf.cast(actual_issame,dtype=tf.bool)\n",
    "\n",
    "def fn(t):\n",
    "    less = tf.less(dist,t)\n",
    "\n",
    "    acc = tf.logical_not(tf.logical_xor(less,actual_issame_bool))\n",
    "    acc = tf.cast(acc,tf.float32)\n",
    "    \n",
    "    out = tf.reshape(tf.reduce_sum(acc),[])\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "thresholds = tf.range(0,0.1,0.001)\n",
    "apply_t = tf.map_fn(fn, thresholds)\n",
    "best_t = tf.argmax(apply_t)\n",
    "\n",
    "best = thresholds[best_t]\n",
    "\n",
    "# Redo the manipulation with the best threshold\n",
    "less = tf.less(dist,best)\n",
    "#less = tf.cast(less,tf.float32)\n",
    "\n",
    "acc = tf.logical_not(tf.logical_xor(less,actual_issame_bool))\n",
    "acc = tf.cast(acc,tf.float32)\n",
    "\n",
    "out = tf.reshape(tf.reduce_sum(acc),[])\n",
    "out = tf.divide(out,len(predict))\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    t1 = time.time()\n",
    "    dist_,best_ = sess.run([out,best])\n",
    "    t2 = time.time()\n",
    "    print(t2-t1)\n",
    "    print(dist_)\n",
    "    print(best_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(x_train[10]))\n",
    "print(y_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "54656/60000 [==========================>...] - ETA: 11:32 - loss: 2.3099 - acc: 0.11 - ETA: 3:58 - loss: 2.2466 - acc: 0.1615 - ETA: 2:28 - loss: 2.1419 - acc: 0.232 - ETA: 1:49 - loss: 2.0485 - acc: 0.275 - ETA: 1:27 - loss: 1.9879 - acc: 0.308 - ETA: 1:14 - loss: 1.8592 - acc: 0.366 - ETA: 1:04 - loss: 1.8003 - acc: 0.385 - ETA: 57s - loss: 1.7449 - acc: 0.409 - ETA: 51s - loss: 1.6686 - acc: 0.43 - ETA: 47s - loss: 1.5879 - acc: 0.46 - ETA: 44s - loss: 1.5306 - acc: 0.48 - ETA: 41s - loss: 1.4794 - acc: 0.50 - ETA: 38s - loss: 1.4109 - acc: 0.53 - ETA: 36s - loss: 1.3590 - acc: 0.54 - ETA: 34s - loss: 1.3086 - acc: 0.56 - ETA: 33s - loss: 1.2620 - acc: 0.58 - ETA: 31s - loss: 1.2199 - acc: 0.59 - ETA: 30s - loss: 1.1920 - acc: 0.60 - ETA: 29s - loss: 1.1640 - acc: 0.61 - ETA: 28s - loss: 1.1249 - acc: 0.63 - ETA: 27s - loss: 1.0899 - acc: 0.64 - ETA: 26s - loss: 1.0601 - acc: 0.65 - ETA: 26s - loss: 1.0361 - acc: 0.66 - ETA: 25s - loss: 1.0081 - acc: 0.67 - ETA: 24s - loss: 0.9818 - acc: 0.68 - ETA: 24s - loss: 0.9605 - acc: 0.68 - ETA: 23s - loss: 0.9412 - acc: 0.69 - ETA: 23s - loss: 0.9216 - acc: 0.69 - ETA: 22s - loss: 0.9006 - acc: 0.70 - ETA: 22s - loss: 0.8858 - acc: 0.71 - ETA: 21s - loss: 0.8746 - acc: 0.71 - ETA: 21s - loss: 0.8568 - acc: 0.72 - ETA: 20s - loss: 0.8420 - acc: 0.72 - ETA: 20s - loss: 0.8289 - acc: 0.73 - ETA: 20s - loss: 0.8141 - acc: 0.73 - ETA: 19s - loss: 0.8010 - acc: 0.74 - ETA: 19s - loss: 0.7870 - acc: 0.74 - ETA: 19s - loss: 0.7740 - acc: 0.75 - ETA: 18s - loss: 0.7643 - acc: 0.75 - ETA: 18s - loss: 0.7507 - acc: 0.75 - ETA: 18s - loss: 0.7409 - acc: 0.76 - ETA: 18s - loss: 0.7315 - acc: 0.76 - ETA: 17s - loss: 0.7189 - acc: 0.77 - ETA: 17s - loss: 0.7098 - acc: 0.77 - ETA: 17s - loss: 0.7014 - acc: 0.77 - ETA: 17s - loss: 0.6917 - acc: 0.77 - ETA: 16s - loss: 0.6834 - acc: 0.78 - ETA: 16s - loss: 0.6741 - acc: 0.78 - ETA: 16s - loss: 0.6651 - acc: 0.78 - ETA: 16s - loss: 0.6573 - acc: 0.79 - ETA: 16s - loss: 0.6486 - acc: 0.79 - ETA: 15s - loss: 0.6423 - acc: 0.79 - ETA: 15s - loss: 0.6353 - acc: 0.79 - ETA: 15s - loss: 0.6293 - acc: 0.79 - ETA: 15s - loss: 0.6233 - acc: 0.80 - ETA: 15s - loss: 0.6159 - acc: 0.80 - ETA: 15s - loss: 0.6084 - acc: 0.80 - ETA: 14s - loss: 0.6011 - acc: 0.80 - ETA: 14s - loss: 0.5949 - acc: 0.81 - ETA: 14s - loss: 0.5885 - acc: 0.81 - ETA: 14s - loss: 0.5833 - acc: 0.81 - ETA: 14s - loss: 0.5779 - acc: 0.81 - ETA: 14s - loss: 0.5716 - acc: 0.81 - ETA: 13s - loss: 0.5645 - acc: 0.82 - ETA: 13s - loss: 0.5587 - acc: 0.82 - ETA: 13s - loss: 0.5539 - acc: 0.82 - ETA: 13s - loss: 0.5482 - acc: 0.82 - ETA: 13s - loss: 0.5449 - acc: 0.82 - ETA: 13s - loss: 0.5405 - acc: 0.82 - ETA: 13s - loss: 0.5360 - acc: 0.83 - ETA: 13s - loss: 0.5311 - acc: 0.83 - ETA: 12s - loss: 0.5256 - acc: 0.83 - ETA: 12s - loss: 0.5213 - acc: 0.83 - ETA: 12s - loss: 0.5168 - acc: 0.83 - ETA: 12s - loss: 0.5130 - acc: 0.83 - ETA: 12s - loss: 0.5082 - acc: 0.83 - ETA: 12s - loss: 0.5036 - acc: 0.84 - ETA: 12s - loss: 0.4985 - acc: 0.84 - ETA: 12s - loss: 0.4947 - acc: 0.84 - ETA: 11s - loss: 0.4913 - acc: 0.84 - ETA: 11s - loss: 0.4868 - acc: 0.84 - ETA: 11s - loss: 0.4847 - acc: 0.84 - ETA: 11s - loss: 0.4824 - acc: 0.84 - ETA: 11s - loss: 0.4800 - acc: 0.84 - ETA: 11s - loss: 0.4771 - acc: 0.85 - ETA: 11s - loss: 0.4728 - acc: 0.85 - ETA: 11s - loss: 0.4696 - acc: 0.85 - ETA: 11s - loss: 0.4664 - acc: 0.85 - ETA: 10s - loss: 0.4628 - acc: 0.85 - ETA: 10s - loss: 0.4599 - acc: 0.85 - ETA: 10s - loss: 0.4571 - acc: 0.85 - ETA: 10s - loss: 0.4541 - acc: 0.85 - ETA: 10s - loss: 0.4508 - acc: 0.85 - ETA: 10s - loss: 0.4475 - acc: 0.85 - ETA: 10s - loss: 0.4450 - acc: 0.86 - ETA: 10s - loss: 0.4415 - acc: 0.86 - ETA: 10s - loss: 0.4390 - acc: 0.86 - ETA: 10s - loss: 0.4366 - acc: 0.86 - ETA: 9s - loss: 0.4340 - acc: 0.8644 - ETA: 9s - loss: 0.4314 - acc: 0.865 - ETA: 9s - loss: 0.4284 - acc: 0.866 - ETA: 9s - loss: 0.4270 - acc: 0.866 - ETA: 9s - loss: 0.4248 - acc: 0.867 - ETA: 9s - loss: 0.4217 - acc: 0.868 - ETA: 9s - loss: 0.4191 - acc: 0.869 - ETA: 9s - loss: 0.4178 - acc: 0.869 - ETA: 9s - loss: 0.4153 - acc: 0.870 - ETA: 9s - loss: 0.4137 - acc: 0.870 - ETA: 9s - loss: 0.4111 - acc: 0.871 - ETA: 8s - loss: 0.4091 - acc: 0.872 - ETA: 8s - loss: 0.4068 - acc: 0.873 - ETA: 8s - loss: 0.4051 - acc: 0.873 - ETA: 8s - loss: 0.4029 - acc: 0.874 - ETA: 8s - loss: 0.4018 - acc: 0.874 - ETA: 8s - loss: 0.3999 - acc: 0.875 - ETA: 8s - loss: 0.3977 - acc: 0.876 - ETA: 8s - loss: 0.3955 - acc: 0.876 - ETA: 8s - loss: 0.3937 - acc: 0.877 - ETA: 8s - loss: 0.3917 - acc: 0.878 - ETA: 8s - loss: 0.3897 - acc: 0.878 - ETA: 8s - loss: 0.3872 - acc: 0.879 - ETA: 7s - loss: 0.3857 - acc: 0.880 - ETA: 7s - loss: 0.3836 - acc: 0.880 - ETA: 7s - loss: 0.3815 - acc: 0.881 - ETA: 7s - loss: 0.3795 - acc: 0.882 - ETA: 7s - loss: 0.3774 - acc: 0.882 - ETA: 7s - loss: 0.3759 - acc: 0.883 - ETA: 7s - loss: 0.3740 - acc: 0.883 - ETA: 7s - loss: 0.3718 - acc: 0.884 - ETA: 7s - loss: 0.3701 - acc: 0.885 - ETA: 7s - loss: 0.3686 - acc: 0.885 - ETA: 7s - loss: 0.3672 - acc: 0.886 - ETA: 7s - loss: 0.3653 - acc: 0.886 - ETA: 7s - loss: 0.3634 - acc: 0.887 - ETA: 6s - loss: 0.3614 - acc: 0.888 - ETA: 6s - loss: 0.3607 - acc: 0.888 - ETA: 6s - loss: 0.3592 - acc: 0.888 - ETA: 6s - loss: 0.3575 - acc: 0.889 - ETA: 6s - loss: 0.3561 - acc: 0.889 - ETA: 6s - loss: 0.3544 - acc: 0.890 - ETA: 6s - loss: 0.3526 - acc: 0.890 - ETA: 6s - loss: 0.3513 - acc: 0.891 - ETA: 6s - loss: 0.3495 - acc: 0.891 - ETA: 6s - loss: 0.3482 - acc: 0.892 - ETA: 6s - loss: 0.3465 - acc: 0.892 - ETA: 6s - loss: 0.3449 - acc: 0.893 - ETA: 6s - loss: 0.3439 - acc: 0.893 - ETA: 5s - loss: 0.3425 - acc: 0.894 - ETA: 5s - loss: 0.3417 - acc: 0.894 - ETA: 5s - loss: 0.3402 - acc: 0.894 - ETA: 5s - loss: 0.3386 - acc: 0.895 - ETA: 5s - loss: 0.3373 - acc: 0.895 - ETA: 5s - loss: 0.3361 - acc: 0.896 - ETA: 5s - loss: 0.3345 - acc: 0.896 - ETA: 5s - loss: 0.3334 - acc: 0.897 - ETA: 5s - loss: 0.3321 - acc: 0.897 - ETA: 5s - loss: 0.3307 - acc: 0.898 - ETA: 5s - loss: 0.3292 - acc: 0.898 - ETA: 5s - loss: 0.3283 - acc: 0.898 - ETA: 5s - loss: 0.3270 - acc: 0.899 - ETA: 4s - loss: 0.3258 - acc: 0.899 - ETA: 4s - loss: 0.3248 - acc: 0.899 - ETA: 4s - loss: 0.3236 - acc: 0.900 - ETA: 4s - loss: 0.3222 - acc: 0.900 - ETA: 4s - loss: 0.3209 - acc: 0.901 - ETA: 4s - loss: 0.3196 - acc: 0.901 - ETA: 4s - loss: 0.3189 - acc: 0.901 - ETA: 4s - loss: 0.3176 - acc: 0.902 - ETA: 4s - loss: 0.3166 - acc: 0.902 - ETA: 4s - loss: 0.3153 - acc: 0.903 - ETA: 4s - loss: 0.3143 - acc: 0.903 - ETA: 4s - loss: 0.3133 - acc: 0.903 - ETA: 4s - loss: 0.3125 - acc: 0.904 - ETA: 4s - loss: 0.3114 - acc: 0.904 - ETA: 4s - loss: 0.3103 - acc: 0.904 - ETA: 3s - loss: 0.3092 - acc: 0.905 - ETA: 3s - loss: 0.3078 - acc: 0.905 - ETA: 3s - loss: 0.3066 - acc: 0.905 - ETA: 3s - loss: 0.3053 - acc: 0.906 - ETA: 3s - loss: 0.3043 - acc: 0.906 - ETA: 3s - loss: 0.3035 - acc: 0.906 - ETA: 3s - loss: 0.3026 - acc: 0.907 - ETA: 3s - loss: 0.3013 - acc: 0.907 - ETA: 3s - loss: 0.3002 - acc: 0.907 - ETA: 3s - loss: 0.2993 - acc: 0.908 - ETA: 3s - loss: 0.2986 - acc: 0.908 - ETA: 3s - loss: 0.2976 - acc: 0.908 - ETA: 3s - loss: 0.2965 - acc: 0.909 - ETA: 3s - loss: 0.2953 - acc: 0.909 - ETA: 2s - loss: 0.2947 - acc: 0.909 - ETA: 2s - loss: 0.2938 - acc: 0.909 - ETA: 2s - loss: 0.2926 - acc: 0.910 - ETA: 2s - loss: 0.2918 - acc: 0.910 - ETA: 2s - loss: 0.2909 - acc: 0.910 - ETA: 2s - loss: 0.2897 - acc: 0.911 - ETA: 2s - loss: 0.2889 - acc: 0.911 - ETA: 2s - loss: 0.2881 - acc: 0.911 - ETA: 2s - loss: 0.2870 - acc: 0.911 - ETA: 2s - loss: 0.2864 - acc: 0.912 - ETA: 2s - loss: 0.2856 - acc: 0.912 - ETA: 2s - loss: 0.2848 - acc: 0.912 - ETA: 2s - loss: 0.2840 - acc: 0.913 - ETA: 2s - loss: 0.2831 - acc: 0.913 - ETA: 2s - loss: 0.2825 - acc: 0.913 - ETA: 1s - loss: 0.2817 - acc: 0.913 - ETA: 1s - loss: 0.2808 - acc: 0.913 - ETA: 1s - loss: 0.2799 - acc: 0.914 - ETA: 1s - loss: 0.2790 - acc: 0.914 - ETA: 1s - loss: 0.2781 - acc: 0.914 - ETA: 1s - loss: 0.2777 - acc: 0.914 - ETA: 1s - loss: 0.2771 - acc: 0.915 - ETA: 1s - loss: 0.2763 - acc: 0.915 - ETA: 1s - loss: 0.2755 - acc: 0.915 - ETA: 1s - loss: 0.2748 - acc: 0.915960000/60000 [==============================] - ETA: 1s - loss: 0.2741 - acc: 0.916 - ETA: 1s - loss: 0.2733 - acc: 0.916 - ETA: 1s - loss: 0.2727 - acc: 0.916 - ETA: 1s - loss: 0.2721 - acc: 0.916 - ETA: 1s - loss: 0.2717 - acc: 0.916 - ETA: 0s - loss: 0.2709 - acc: 0.917 - ETA: 0s - loss: 0.2702 - acc: 0.917 - ETA: 0s - loss: 0.2694 - acc: 0.917 - ETA: 0s - loss: 0.2688 - acc: 0.917 - ETA: 0s - loss: 0.2681 - acc: 0.918 - ETA: 0s - loss: 0.2673 - acc: 0.918 - ETA: 0s - loss: 0.2664 - acc: 0.918 - ETA: 0s - loss: 0.2656 - acc: 0.918 - ETA: 0s - loss: 0.2650 - acc: 0.919 - ETA: 0s - loss: 0.2642 - acc: 0.919 - ETA: 0s - loss: 0.2637 - acc: 0.919 - ETA: 0s - loss: 0.2630 - acc: 0.919 - ETA: 0s - loss: 0.2624 - acc: 0.919 - ETA: 0s - loss: 0.2616 - acc: 0.920 - ETA: 0s - loss: 0.2611 - acc: 0.920 - 16s 266us/step - loss: 0.2606 - acc: 0.9204 - val_loss: 0.0568 - val_acc: 0.9810\n",
      "Epoch 2/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0504 - acc: 0.98 - ETA: 13s - loss: 0.1103 - acc: 0.96 - ETA: 13s - loss: 0.0983 - acc: 0.96 - ETA: 13s - loss: 0.1153 - acc: 0.96 - ETA: 13s - loss: 0.1209 - acc: 0.96 - ETA: 13s - loss: 0.1286 - acc: 0.96 - ETA: 13s - loss: 0.1285 - acc: 0.96 - ETA: 13s - loss: 0.1204 - acc: 0.96 - ETA: 13s - loss: 0.1226 - acc: 0.96 - ETA: 13s - loss: 0.1176 - acc: 0.96 - ETA: 12s - loss: 0.1161 - acc: 0.96 - ETA: 12s - loss: 0.1137 - acc: 0.96 - ETA: 12s - loss: 0.1101 - acc: 0.97 - ETA: 12s - loss: 0.1088 - acc: 0.97 - ETA: 12s - loss: 0.1094 - acc: 0.97 - ETA: 12s - loss: 0.1077 - acc: 0.97 - ETA: 12s - loss: 0.1085 - acc: 0.97 - ETA: 12s - loss: 0.1103 - acc: 0.97 - ETA: 12s - loss: 0.1104 - acc: 0.97 - ETA: 12s - loss: 0.1082 - acc: 0.97 - ETA: 12s - loss: 0.1089 - acc: 0.97 - ETA: 12s - loss: 0.1073 - acc: 0.97 - ETA: 12s - loss: 0.1065 - acc: 0.97 - ETA: 12s - loss: 0.1074 - acc: 0.97 - ETA: 12s - loss: 0.1059 - acc: 0.97 - ETA: 12s - loss: 0.1064 - acc: 0.97 - ETA: 12s - loss: 0.1073 - acc: 0.96 - ETA: 12s - loss: 0.1048 - acc: 0.97 - ETA: 11s - loss: 0.1059 - acc: 0.97 - ETA: 11s - loss: 0.1063 - acc: 0.97 - ETA: 11s - loss: 0.1074 - acc: 0.96 - ETA: 11s - loss: 0.1068 - acc: 0.97 - ETA: 11s - loss: 0.1049 - acc: 0.97 - ETA: 11s - loss: 0.1041 - acc: 0.97 - ETA: 11s - loss: 0.1035 - acc: 0.97 - ETA: 11s - loss: 0.1030 - acc: 0.97 - ETA: 11s - loss: 0.1019 - acc: 0.97 - ETA: 11s - loss: 0.1021 - acc: 0.97 - ETA: 11s - loss: 0.1017 - acc: 0.97 - ETA: 11s - loss: 0.0998 - acc: 0.97 - ETA: 11s - loss: 0.1002 - acc: 0.97 - ETA: 11s - loss: 0.1000 - acc: 0.97 - ETA: 11s - loss: 0.1009 - acc: 0.97 - ETA: 11s - loss: 0.1006 - acc: 0.97 - ETA: 10s - loss: 0.0999 - acc: 0.97 - ETA: 10s - loss: 0.0994 - acc: 0.97 - ETA: 10s - loss: 0.0988 - acc: 0.97 - ETA: 10s - loss: 0.0995 - acc: 0.97 - ETA: 10s - loss: 0.0999 - acc: 0.97 - ETA: 10s - loss: 0.0990 - acc: 0.97 - ETA: 10s - loss: 0.0988 - acc: 0.97 - ETA: 10s - loss: 0.0980 - acc: 0.97 - ETA: 10s - loss: 0.0973 - acc: 0.97 - ETA: 10s - loss: 0.0971 - acc: 0.97 - ETA: 10s - loss: 0.0966 - acc: 0.97 - ETA: 10s - loss: 0.0968 - acc: 0.97 - ETA: 10s - loss: 0.0976 - acc: 0.97 - ETA: 10s - loss: 0.0981 - acc: 0.97 - ETA: 10s - loss: 0.0978 - acc: 0.97 - ETA: 10s - loss: 0.0981 - acc: 0.97 - ETA: 10s - loss: 0.0998 - acc: 0.97 - ETA: 9s - loss: 0.0998 - acc: 0.9700 - ETA: 9s - loss: 0.0996 - acc: 0.970 - ETA: 9s - loss: 0.0996 - acc: 0.970 - ETA: 9s - loss: 0.0992 - acc: 0.970 - ETA: 9s - loss: 0.0998 - acc: 0.970 - ETA: 9s - loss: 0.0995 - acc: 0.970 - ETA: 9s - loss: 0.0992 - acc: 0.970 - ETA: 9s - loss: 0.0987 - acc: 0.970 - ETA: 9s - loss: 0.0986 - acc: 0.970 - ETA: 9s - loss: 0.0989 - acc: 0.970 - ETA: 9s - loss: 0.0992 - acc: 0.970 - ETA: 9s - loss: 0.0993 - acc: 0.970 - ETA: 9s - loss: 0.0988 - acc: 0.970 - ETA: 9s - loss: 0.0986 - acc: 0.970 - ETA: 9s - loss: 0.0982 - acc: 0.971 - ETA: 9s - loss: 0.0981 - acc: 0.971 - ETA: 9s - loss: 0.0982 - acc: 0.971 - ETA: 8s - loss: 0.0985 - acc: 0.971 - ETA: 8s - loss: 0.0980 - acc: 0.971 - ETA: 8s - loss: 0.0980 - acc: 0.971 - ETA: 8s - loss: 0.0981 - acc: 0.971 - ETA: 8s - loss: 0.0986 - acc: 0.971 - ETA: 8s - loss: 0.0980 - acc: 0.971 - ETA: 8s - loss: 0.0977 - acc: 0.971 - ETA: 8s - loss: 0.0975 - acc: 0.971 - ETA: 8s - loss: 0.0976 - acc: 0.971 - ETA: 8s - loss: 0.0975 - acc: 0.971 - ETA: 8s - loss: 0.0980 - acc: 0.971 - ETA: 8s - loss: 0.0975 - acc: 0.971 - ETA: 8s - loss: 0.0974 - acc: 0.971 - ETA: 8s - loss: 0.0975 - acc: 0.971 - ETA: 8s - loss: 0.0973 - acc: 0.971 - ETA: 8s - loss: 0.0972 - acc: 0.971 - ETA: 8s - loss: 0.0969 - acc: 0.971 - ETA: 7s - loss: 0.0971 - acc: 0.971 - ETA: 7s - loss: 0.0974 - acc: 0.971 - ETA: 7s - loss: 0.0973 - acc: 0.971 - ETA: 7s - loss: 0.0972 - acc: 0.971 - ETA: 7s - loss: 0.0966 - acc: 0.971 - ETA: 7s - loss: 0.0961 - acc: 0.972 - ETA: 7s - loss: 0.0959 - acc: 0.972 - ETA: 7s - loss: 0.0961 - acc: 0.972 - ETA: 7s - loss: 0.0962 - acc: 0.972 - ETA: 7s - loss: 0.0956 - acc: 0.972 - ETA: 7s - loss: 0.0957 - acc: 0.972 - ETA: 7s - loss: 0.0954 - acc: 0.972 - ETA: 7s - loss: 0.0950 - acc: 0.972 - ETA: 7s - loss: 0.0951 - acc: 0.972 - ETA: 7s - loss: 0.0954 - acc: 0.972 - ETA: 7s - loss: 0.0955 - acc: 0.972 - ETA: 7s - loss: 0.0959 - acc: 0.972 - ETA: 6s - loss: 0.0957 - acc: 0.972 - ETA: 6s - loss: 0.0953 - acc: 0.972 - ETA: 6s - loss: 0.0953 - acc: 0.972 - ETA: 6s - loss: 0.0958 - acc: 0.972 - ETA: 6s - loss: 0.0955 - acc: 0.972 - ETA: 6s - loss: 0.0961 - acc: 0.972 - ETA: 6s - loss: 0.0960 - acc: 0.972 - ETA: 6s - loss: 0.0958 - acc: 0.972 - ETA: 6s - loss: 0.0954 - acc: 0.972 - ETA: 6s - loss: 0.0953 - acc: 0.972 - ETA: 6s - loss: 0.0951 - acc: 0.972 - ETA: 6s - loss: 0.0951 - acc: 0.972 - ETA: 6s - loss: 0.0949 - acc: 0.972 - ETA: 6s - loss: 0.0951 - acc: 0.972 - ETA: 6s - loss: 0.0948 - acc: 0.972 - ETA: 6s - loss: 0.0944 - acc: 0.972 - ETA: 6s - loss: 0.0945 - acc: 0.972 - ETA: 6s - loss: 0.0942 - acc: 0.972 - ETA: 5s - loss: 0.0941 - acc: 0.972 - ETA: 5s - loss: 0.0939 - acc: 0.972 - ETA: 5s - loss: 0.0938 - acc: 0.972 - ETA: 5s - loss: 0.0934 - acc: 0.972 - ETA: 5s - loss: 0.0932 - acc: 0.972 - ETA: 5s - loss: 0.0934 - acc: 0.972 - ETA: 5s - loss: 0.0940 - acc: 0.972 - ETA: 5s - loss: 0.0940 - acc: 0.972 - ETA: 5s - loss: 0.0943 - acc: 0.972 - ETA: 5s - loss: 0.0940 - acc: 0.972 - ETA: 5s - loss: 0.0938 - acc: 0.972 - ETA: 5s - loss: 0.0937 - acc: 0.972 - ETA: 5s - loss: 0.0938 - acc: 0.972 - ETA: 5s - loss: 0.0939 - acc: 0.972 - ETA: 5s - loss: 0.0942 - acc: 0.972 - ETA: 5s - loss: 0.0937 - acc: 0.972 - ETA: 5s - loss: 0.0936 - acc: 0.972 - ETA: 4s - loss: 0.0934 - acc: 0.972 - ETA: 4s - loss: 0.0933 - acc: 0.972 - ETA: 4s - loss: 0.0930 - acc: 0.972 - ETA: 4s - loss: 0.0928 - acc: 0.973 - ETA: 4s - loss: 0.0927 - acc: 0.973 - ETA: 4s - loss: 0.0924 - acc: 0.973 - ETA: 4s - loss: 0.0927 - acc: 0.973 - ETA: 4s - loss: 0.0925 - acc: 0.973 - ETA: 4s - loss: 0.0926 - acc: 0.973 - ETA: 4s - loss: 0.0927 - acc: 0.972 - ETA: 4s - loss: 0.0926 - acc: 0.972 - ETA: 4s - loss: 0.0925 - acc: 0.972 - ETA: 4s - loss: 0.0922 - acc: 0.973 - ETA: 4s - loss: 0.0923 - acc: 0.973 - ETA: 4s - loss: 0.0921 - acc: 0.973 - ETA: 4s - loss: 0.0919 - acc: 0.973 - ETA: 4s - loss: 0.0916 - acc: 0.973 - ETA: 4s - loss: 0.0921 - acc: 0.973 - ETA: 3s - loss: 0.0919 - acc: 0.973 - ETA: 3s - loss: 0.0916 - acc: 0.973 - ETA: 3s - loss: 0.0915 - acc: 0.973 - ETA: 3s - loss: 0.0914 - acc: 0.973 - ETA: 3s - loss: 0.0914 - acc: 0.973 - ETA: 3s - loss: 0.0917 - acc: 0.973 - ETA: 3s - loss: 0.0918 - acc: 0.973 - ETA: 3s - loss: 0.0915 - acc: 0.973 - ETA: 3s - loss: 0.0912 - acc: 0.973 - ETA: 3s - loss: 0.0910 - acc: 0.973 - ETA: 3s - loss: 0.0907 - acc: 0.973 - ETA: 3s - loss: 0.0907 - acc: 0.973 - ETA: 3s - loss: 0.0904 - acc: 0.973 - ETA: 3s - loss: 0.0905 - acc: 0.973 - ETA: 3s - loss: 0.0905 - acc: 0.973 - ETA: 3s - loss: 0.0906 - acc: 0.973 - ETA: 3s - loss: 0.0909 - acc: 0.973 - ETA: 2s - loss: 0.0908 - acc: 0.973 - ETA: 2s - loss: 0.0908 - acc: 0.973 - ETA: 2s - loss: 0.0907 - acc: 0.973 - ETA: 2s - loss: 0.0907 - acc: 0.973 - ETA: 2s - loss: 0.0907 - acc: 0.973 - ETA: 2s - loss: 0.0909 - acc: 0.973 - ETA: 2s - loss: 0.0907 - acc: 0.973 - ETA: 2s - loss: 0.0904 - acc: 0.973 - ETA: 2s - loss: 0.0908 - acc: 0.973 - ETA: 2s - loss: 0.0906 - acc: 0.973 - ETA: 2s - loss: 0.0904 - acc: 0.973 - ETA: 2s - loss: 0.0900 - acc: 0.973 - ETA: 2s - loss: 0.0899 - acc: 0.973 - ETA: 2s - loss: 0.0897 - acc: 0.973 - ETA: 2s - loss: 0.0896 - acc: 0.973 - ETA: 2s - loss: 0.0894 - acc: 0.973 - ETA: 2s - loss: 0.0891 - acc: 0.973 - ETA: 2s - loss: 0.0892 - acc: 0.973 - ETA: 1s - loss: 0.0892 - acc: 0.973 - ETA: 1s - loss: 0.0893 - acc: 0.973 - ETA: 1s - loss: 0.0891 - acc: 0.973 - ETA: 1s - loss: 0.0890 - acc: 0.973 - ETA: 1s - loss: 0.0889 - acc: 0.973 - ETA: 1s - loss: 0.0887 - acc: 0.974 - ETA: 1s - loss: 0.0885 - acc: 0.974 - ETA: 1s - loss: 0.0882 - acc: 0.974 - ETA: 1s - loss: 0.0880 - acc: 0.974 - ETA: 1s - loss: 0.0879 - acc: 0.974 - ETA: 1s - loss: 0.0877 - acc: 0.974 - ETA: 1s - loss: 0.0878 - acc: 0.974 - ETA: 1s - loss: 0.0876 - acc: 0.974 - ETA: 1s - loss: 0.0876 - acc: 0.974 - ETA: 1s - loss: 0.0874 - acc: 0.9744"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 1s - loss: 0.0872 - acc: 0.974 - ETA: 1s - loss: 0.0872 - acc: 0.974 - ETA: 0s - loss: 0.0871 - acc: 0.974 - ETA: 0s - loss: 0.0871 - acc: 0.974 - ETA: 0s - loss: 0.0874 - acc: 0.974 - ETA: 0s - loss: 0.0876 - acc: 0.974 - ETA: 0s - loss: 0.0875 - acc: 0.974 - ETA: 0s - loss: 0.0878 - acc: 0.974 - ETA: 0s - loss: 0.0878 - acc: 0.974 - ETA: 0s - loss: 0.0878 - acc: 0.974 - ETA: 0s - loss: 0.0880 - acc: 0.974 - ETA: 0s - loss: 0.0877 - acc: 0.974 - ETA: 0s - loss: 0.0879 - acc: 0.974 - ETA: 0s - loss: 0.0880 - acc: 0.974 - ETA: 0s - loss: 0.0883 - acc: 0.974 - ETA: 0s - loss: 0.0883 - acc: 0.974 - ETA: 0s - loss: 0.0883 - acc: 0.974 - ETA: 0s - loss: 0.0882 - acc: 0.974 - ETA: 0s - loss: 0.0880 - acc: 0.974 - 14s 234us/step - loss: 0.0880 - acc: 0.9745 - val_loss: 0.0411 - val_acc: 0.9860\n",
      "Epoch 3/12\n",
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0427 - acc: 0.98 - ETA: 13s - loss: 0.0435 - acc: 0.98 - ETA: 13s - loss: 0.0372 - acc: 0.98 - ETA: 13s - loss: 0.0512 - acc: 0.98 - ETA: 13s - loss: 0.0578 - acc: 0.98 - ETA: 12s - loss: 0.0593 - acc: 0.98 - ETA: 12s - loss: 0.0652 - acc: 0.97 - ETA: 12s - loss: 0.0633 - acc: 0.97 - ETA: 12s - loss: 0.0705 - acc: 0.97 - ETA: 12s - loss: 0.0693 - acc: 0.97 - ETA: 12s - loss: 0.0715 - acc: 0.97 - ETA: 12s - loss: 0.0700 - acc: 0.97 - ETA: 12s - loss: 0.0697 - acc: 0.97 - ETA: 12s - loss: 0.0676 - acc: 0.97 - ETA: 12s - loss: 0.0669 - acc: 0.97 - ETA: 12s - loss: 0.0659 - acc: 0.98 - ETA: 12s - loss: 0.0666 - acc: 0.97 - ETA: 12s - loss: 0.0677 - acc: 0.97 - ETA: 12s - loss: 0.0671 - acc: 0.97 - ETA: 12s - loss: 0.0661 - acc: 0.98 - ETA: 12s - loss: 0.0675 - acc: 0.97 - ETA: 12s - loss: 0.0671 - acc: 0.97 - ETA: 12s - loss: 0.0663 - acc: 0.98 - ETA: 12s - loss: 0.0658 - acc: 0.98 - ETA: 11s - loss: 0.0661 - acc: 0.97 - ETA: 11s - loss: 0.0658 - acc: 0.97 - ETA: 11s - loss: 0.0662 - acc: 0.97 - ETA: 11s - loss: 0.0656 - acc: 0.97 - ETA: 11s - loss: 0.0643 - acc: 0.97 - ETA: 11s - loss: 0.0640 - acc: 0.97 - ETA: 11s - loss: 0.0635 - acc: 0.98 - ETA: 11s - loss: 0.0635 - acc: 0.97 - ETA: 11s - loss: 0.0636 - acc: 0.97 - ETA: 11s - loss: 0.0645 - acc: 0.97 - ETA: 11s - loss: 0.0660 - acc: 0.97 - ETA: 11s - loss: 0.0650 - acc: 0.98 - ETA: 11s - loss: 0.0643 - acc: 0.98 - ETA: 11s - loss: 0.0646 - acc: 0.98 - ETA: 11s - loss: 0.0640 - acc: 0.98 - ETA: 11s - loss: 0.0649 - acc: 0.98 - ETA: 11s - loss: 0.0646 - acc: 0.98 - ETA: 11s - loss: 0.0646 - acc: 0.98 - ETA: 10s - loss: 0.0648 - acc: 0.98 - ETA: 10s - loss: 0.0653 - acc: 0.97 - ETA: 10s - loss: 0.0665 - acc: 0.97 - ETA: 10s - loss: 0.0666 - acc: 0.97 - ETA: 10s - loss: 0.0667 - acc: 0.97 - ETA: 10s - loss: 0.0666 - acc: 0.97 - ETA: 10s - loss: 0.0669 - acc: 0.97 - ETA: 10s - loss: 0.0666 - acc: 0.97 - ETA: 10s - loss: 0.0670 - acc: 0.97 - ETA: 10s - loss: 0.0678 - acc: 0.97 - ETA: 10s - loss: 0.0678 - acc: 0.97 - ETA: 10s - loss: 0.0685 - acc: 0.97 - ETA: 10s - loss: 0.0681 - acc: 0.97 - ETA: 10s - loss: 0.0684 - acc: 0.97 - ETA: 10s - loss: 0.0685 - acc: 0.97 - ETA: 10s - loss: 0.0690 - acc: 0.97 - ETA: 10s - loss: 0.0683 - acc: 0.97 - ETA: 9s - loss: 0.0680 - acc: 0.9791 - ETA: 9s - loss: 0.0680 - acc: 0.979 - ETA: 9s - loss: 0.0681 - acc: 0.979 - ETA: 9s - loss: 0.0680 - acc: 0.979 - ETA: 9s - loss: 0.0683 - acc: 0.978 - ETA: 9s - loss: 0.0691 - acc: 0.978 - ETA: 9s - loss: 0.0697 - acc: 0.978 - ETA: 9s - loss: 0.0695 - acc: 0.978 - ETA: 9s - loss: 0.0699 - acc: 0.978 - ETA: 9s - loss: 0.0695 - acc: 0.978 - ETA: 9s - loss: 0.0705 - acc: 0.978 - ETA: 9s - loss: 0.0708 - acc: 0.978 - ETA: 9s - loss: 0.0702 - acc: 0.978 - ETA: 9s - loss: 0.0700 - acc: 0.978 - ETA: 9s - loss: 0.0697 - acc: 0.978 - ETA: 9s - loss: 0.0697 - acc: 0.978 - ETA: 9s - loss: 0.0693 - acc: 0.978 - ETA: 9s - loss: 0.0692 - acc: 0.978 - ETA: 8s - loss: 0.0690 - acc: 0.978 - ETA: 8s - loss: 0.0694 - acc: 0.978 - ETA: 8s - loss: 0.0698 - acc: 0.978 - ETA: 8s - loss: 0.0694 - acc: 0.978 - ETA: 8s - loss: 0.0694 - acc: 0.978 - ETA: 8s - loss: 0.0692 - acc: 0.978 - ETA: 8s - loss: 0.0690 - acc: 0.978 - ETA: 8s - loss: 0.0691 - acc: 0.978 - ETA: 8s - loss: 0.0691 - acc: 0.978 - ETA: 8s - loss: 0.0697 - acc: 0.978 - ETA: 8s - loss: 0.0703 - acc: 0.978 - ETA: 8s - loss: 0.0704 - acc: 0.978 - ETA: 8s - loss: 0.0705 - acc: 0.978 - ETA: 8s - loss: 0.0708 - acc: 0.978 - ETA: 8s - loss: 0.0711 - acc: 0.978 - ETA: 8s - loss: 0.0708 - acc: 0.978 - ETA: 8s - loss: 0.0703 - acc: 0.978 - ETA: 8s - loss: 0.0698 - acc: 0.978 - ETA: 7s - loss: 0.0693 - acc: 0.979 - ETA: 7s - loss: 0.0690 - acc: 0.979 - ETA: 7s - loss: 0.0692 - acc: 0.979 - ETA: 7s - loss: 0.0690 - acc: 0.979 - ETA: 7s - loss: 0.0688 - acc: 0.979 - ETA: 7s - loss: 0.0687 - acc: 0.979 - ETA: 7s - loss: 0.0686 - acc: 0.979 - ETA: 7s - loss: 0.0687 - acc: 0.979 - ETA: 7s - loss: 0.0691 - acc: 0.979 - ETA: 7s - loss: 0.0690 - acc: 0.979 - ETA: 7s - loss: 0.0686 - acc: 0.979 - ETA: 7s - loss: 0.0690 - acc: 0.979 - ETA: 7s - loss: 0.0689 - acc: 0.979 - ETA: 7s - loss: 0.0687 - acc: 0.979 - ETA: 7s - loss: 0.0686 - acc: 0.979 - ETA: 7s - loss: 0.0684 - acc: 0.979 - ETA: 7s - loss: 0.0681 - acc: 0.979 - ETA: 6s - loss: 0.0683 - acc: 0.979 - ETA: 6s - loss: 0.0687 - acc: 0.979 - ETA: 6s - loss: 0.0685 - acc: 0.979 - ETA: 6s - loss: 0.0684 - acc: 0.979 - ETA: 6s - loss: 0.0683 - acc: 0.979 - ETA: 6s - loss: 0.0685 - acc: 0.979 - ETA: 6s - loss: 0.0685 - acc: 0.979 - ETA: 6s - loss: 0.0681 - acc: 0.979 - ETA: 6s - loss: 0.0678 - acc: 0.979 - ETA: 6s - loss: 0.0675 - acc: 0.979 - ETA: 6s - loss: 0.0674 - acc: 0.979 - ETA: 6s - loss: 0.0674 - acc: 0.979 - ETA: 6s - loss: 0.0671 - acc: 0.979 - ETA: 6s - loss: 0.0673 - acc: 0.979 - ETA: 6s - loss: 0.0672 - acc: 0.979 - ETA: 6s - loss: 0.0671 - acc: 0.979 - ETA: 6s - loss: 0.0670 - acc: 0.979 - ETA: 6s - loss: 0.0670 - acc: 0.979 - ETA: 5s - loss: 0.0670 - acc: 0.979 - ETA: 5s - loss: 0.0669 - acc: 0.979 - ETA: 5s - loss: 0.0665 - acc: 0.980 - ETA: 5s - loss: 0.0669 - acc: 0.979 - ETA: 5s - loss: 0.0670 - acc: 0.979 - ETA: 5s - loss: 0.0667 - acc: 0.979 - ETA: 5s - loss: 0.0667 - acc: 0.979 - ETA: 5s - loss: 0.0668 - acc: 0.979 - ETA: 5s - loss: 0.0666 - acc: 0.979 - ETA: 5s - loss: 0.0667 - acc: 0.979 - ETA: 5s - loss: 0.0668 - acc: 0.979 - ETA: 5s - loss: 0.0670 - acc: 0.979 - ETA: 5s - loss: 0.0671 - acc: 0.979 - ETA: 5s - loss: 0.0669 - acc: 0.979 - ETA: 5s - loss: 0.0669 - acc: 0.979 - ETA: 5s - loss: 0.0670 - acc: 0.979 - ETA: 5s - loss: 0.0674 - acc: 0.979 - ETA: 4s - loss: 0.0678 - acc: 0.979 - ETA: 4s - loss: 0.0677 - acc: 0.979 - ETA: 4s - loss: 0.0675 - acc: 0.979 - ETA: 4s - loss: 0.0674 - acc: 0.979 - ETA: 4s - loss: 0.0673 - acc: 0.979 - ETA: 4s - loss: 0.0671 - acc: 0.979 - ETA: 4s - loss: 0.0670 - acc: 0.979 - ETA: 4s - loss: 0.0668 - acc: 0.979 - ETA: 4s - loss: 0.0668 - acc: 0.979 - ETA: 4s - loss: 0.0668 - acc: 0.979 - ETA: 4s - loss: 0.0666 - acc: 0.979 - ETA: 4s - loss: 0.0663 - acc: 0.980 - ETA: 4s - loss: 0.0666 - acc: 0.980 - ETA: 4s - loss: 0.0664 - acc: 0.980 - ETA: 4s - loss: 0.0665 - acc: 0.980 - ETA: 4s - loss: 0.0665 - acc: 0.980 - ETA: 4s - loss: 0.0666 - acc: 0.980 - ETA: 4s - loss: 0.0665 - acc: 0.980 - ETA: 3s - loss: 0.0664 - acc: 0.980 - ETA: 3s - loss: 0.0664 - acc: 0.980 - ETA: 3s - loss: 0.0662 - acc: 0.980 - ETA: 3s - loss: 0.0660 - acc: 0.980 - ETA: 3s - loss: 0.0662 - acc: 0.980 - ETA: 3s - loss: 0.0660 - acc: 0.980 - ETA: 3s - loss: 0.0663 - acc: 0.980 - ETA: 3s - loss: 0.0663 - acc: 0.980 - ETA: 3s - loss: 0.0664 - acc: 0.980 - ETA: 3s - loss: 0.0662 - acc: 0.980 - ETA: 3s - loss: 0.0662 - acc: 0.980 - ETA: 3s - loss: 0.0660 - acc: 0.980 - ETA: 3s - loss: 0.0660 - acc: 0.980 - ETA: 3s - loss: 0.0660 - acc: 0.980 - ETA: 3s - loss: 0.0661 - acc: 0.980 - ETA: 3s - loss: 0.0660 - acc: 0.980 - ETA: 3s - loss: 0.0659 - acc: 0.980 - ETA: 2s - loss: 0.0663 - acc: 0.980 - ETA: 2s - loss: 0.0665 - acc: 0.979 - ETA: 2s - loss: 0.0663 - acc: 0.979 - ETA: 2s - loss: 0.0664 - acc: 0.979 - ETA: 2s - loss: 0.0662 - acc: 0.980 - ETA: 2s - loss: 0.0664 - acc: 0.979 - ETA: 2s - loss: 0.0664 - acc: 0.979 - ETA: 2s - loss: 0.0666 - acc: 0.979 - ETA: 2s - loss: 0.0668 - acc: 0.979 - ETA: 2s - loss: 0.0666 - acc: 0.979 - ETA: 2s - loss: 0.0671 - acc: 0.979 - ETA: 2s - loss: 0.0674 - acc: 0.979 - ETA: 2s - loss: 0.0673 - acc: 0.979 - ETA: 2s - loss: 0.0672 - acc: 0.979 - ETA: 2s - loss: 0.0672 - acc: 0.979 - ETA: 2s - loss: 0.0671 - acc: 0.979 - ETA: 2s - loss: 0.0672 - acc: 0.979 - ETA: 2s - loss: 0.0670 - acc: 0.979 - ETA: 1s - loss: 0.0670 - acc: 0.979 - ETA: 1s - loss: 0.0672 - acc: 0.979 - ETA: 1s - loss: 0.0671 - acc: 0.979 - ETA: 1s - loss: 0.0669 - acc: 0.979 - ETA: 1s - loss: 0.0668 - acc: 0.979 - ETA: 1s - loss: 0.0669 - acc: 0.979 - ETA: 1s - loss: 0.0670 - acc: 0.979 - ETA: 1s - loss: 0.0671 - acc: 0.979 - ETA: 1s - loss: 0.0670 - acc: 0.979 - ETA: 1s - loss: 0.0669 - acc: 0.979 - ETA: 1s - loss: 0.0669 - acc: 0.979 - ETA: 1s - loss: 0.0668 - acc: 0.979 - ETA: 1s - loss: 0.0668 - acc: 0.979 - ETA: 1s - loss: 0.0668 - acc: 0.979 - ETA: 1s - loss: 0.0667 - acc: 0.979960000/60000 [==============================] - ETA: 1s - loss: 0.0665 - acc: 0.980 - ETA: 1s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0663 - acc: 0.980 - ETA: 0s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0666 - acc: 0.979 - ETA: 0s - loss: 0.0665 - acc: 0.980 - ETA: 0s - loss: 0.0665 - acc: 0.980 - ETA: 0s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0665 - acc: 0.980 - ETA: 0s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0663 - acc: 0.980 - ETA: 0s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0663 - acc: 0.980 - ETA: 0s - loss: 0.0663 - acc: 0.980 - ETA: 0s - loss: 0.0665 - acc: 0.980 - ETA: 0s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0663 - acc: 0.980 - 14s 234us/step - loss: 0.0664 - acc: 0.9802 - val_loss: 0.0369 - val_acc: 0.9875\n",
      "Epoch 4/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0481 - acc: 0.98 - ETA: 13s - loss: 0.0657 - acc: 0.98 - ETA: 13s - loss: 0.0697 - acc: 0.97 - ETA: 13s - loss: 0.0576 - acc: 0.98 - ETA: 13s - loss: 0.0490 - acc: 0.98 - ETA: 13s - loss: 0.0537 - acc: 0.98 - ETA: 12s - loss: 0.0550 - acc: 0.98 - ETA: 12s - loss: 0.0572 - acc: 0.98 - ETA: 12s - loss: 0.0608 - acc: 0.98 - ETA: 12s - loss: 0.0577 - acc: 0.98 - ETA: 12s - loss: 0.0604 - acc: 0.98 - ETA: 12s - loss: 0.0599 - acc: 0.98 - ETA: 12s - loss: 0.0596 - acc: 0.98 - ETA: 12s - loss: 0.0579 - acc: 0.98 - ETA: 12s - loss: 0.0583 - acc: 0.98 - ETA: 12s - loss: 0.0571 - acc: 0.98 - ETA: 12s - loss: 0.0589 - acc: 0.98 - ETA: 12s - loss: 0.0572 - acc: 0.98 - ETA: 12s - loss: 0.0578 - acc: 0.98 - ETA: 12s - loss: 0.0574 - acc: 0.98 - ETA: 12s - loss: 0.0563 - acc: 0.98 - ETA: 12s - loss: 0.0557 - acc: 0.98 - ETA: 12s - loss: 0.0556 - acc: 0.98 - ETA: 12s - loss: 0.0546 - acc: 0.98 - ETA: 12s - loss: 0.0571 - acc: 0.98 - ETA: 11s - loss: 0.0564 - acc: 0.98 - ETA: 11s - loss: 0.0559 - acc: 0.98 - ETA: 11s - loss: 0.0554 - acc: 0.98 - ETA: 11s - loss: 0.0539 - acc: 0.98 - ETA: 11s - loss: 0.0539 - acc: 0.98 - ETA: 11s - loss: 0.0548 - acc: 0.98 - ETA: 11s - loss: 0.0537 - acc: 0.98 - ETA: 11s - loss: 0.0542 - acc: 0.98 - ETA: 11s - loss: 0.0540 - acc: 0.98 - ETA: 11s - loss: 0.0540 - acc: 0.98 - ETA: 11s - loss: 0.0529 - acc: 0.98 - ETA: 11s - loss: 0.0534 - acc: 0.98 - ETA: 11s - loss: 0.0533 - acc: 0.98 - ETA: 11s - loss: 0.0525 - acc: 0.98 - ETA: 11s - loss: 0.0532 - acc: 0.98 - ETA: 11s - loss: 0.0524 - acc: 0.98 - ETA: 11s - loss: 0.0531 - acc: 0.98 - ETA: 10s - loss: 0.0524 - acc: 0.98 - ETA: 10s - loss: 0.0535 - acc: 0.98 - ETA: 10s - loss: 0.0529 - acc: 0.98 - ETA: 10s - loss: 0.0525 - acc: 0.98 - ETA: 10s - loss: 0.0522 - acc: 0.98 - ETA: 10s - loss: 0.0525 - acc: 0.98 - ETA: 10s - loss: 0.0521 - acc: 0.98 - ETA: 10s - loss: 0.0525 - acc: 0.98 - ETA: 10s - loss: 0.0524 - acc: 0.98 - ETA: 10s - loss: 0.0520 - acc: 0.98 - ETA: 10s - loss: 0.0517 - acc: 0.98 - ETA: 10s - loss: 0.0516 - acc: 0.98 - ETA: 10s - loss: 0.0517 - acc: 0.98 - ETA: 10s - loss: 0.0513 - acc: 0.98 - ETA: 10s - loss: 0.0510 - acc: 0.98 - ETA: 10s - loss: 0.0517 - acc: 0.98 - ETA: 10s - loss: 0.0519 - acc: 0.98 - ETA: 10s - loss: 0.0518 - acc: 0.98 - ETA: 9s - loss: 0.0515 - acc: 0.9851 - ETA: 9s - loss: 0.0514 - acc: 0.985 - ETA: 9s - loss: 0.0517 - acc: 0.985 - ETA: 9s - loss: 0.0521 - acc: 0.985 - ETA: 9s - loss: 0.0533 - acc: 0.985 - ETA: 9s - loss: 0.0537 - acc: 0.985 - ETA: 9s - loss: 0.0540 - acc: 0.984 - ETA: 9s - loss: 0.0547 - acc: 0.984 - ETA: 9s - loss: 0.0549 - acc: 0.984 - ETA: 9s - loss: 0.0548 - acc: 0.984 - ETA: 9s - loss: 0.0558 - acc: 0.984 - ETA: 9s - loss: 0.0553 - acc: 0.984 - ETA: 9s - loss: 0.0550 - acc: 0.984 - ETA: 9s - loss: 0.0554 - acc: 0.984 - ETA: 9s - loss: 0.0553 - acc: 0.984 - ETA: 9s - loss: 0.0559 - acc: 0.984 - ETA: 9s - loss: 0.0559 - acc: 0.984 - ETA: 8s - loss: 0.0563 - acc: 0.984 - ETA: 8s - loss: 0.0562 - acc: 0.984 - ETA: 8s - loss: 0.0562 - acc: 0.984 - ETA: 8s - loss: 0.0559 - acc: 0.984 - ETA: 8s - loss: 0.0560 - acc: 0.984 - ETA: 8s - loss: 0.0559 - acc: 0.984 - ETA: 8s - loss: 0.0562 - acc: 0.984 - ETA: 8s - loss: 0.0564 - acc: 0.984 - ETA: 8s - loss: 0.0565 - acc: 0.984 - ETA: 8s - loss: 0.0568 - acc: 0.984 - ETA: 8s - loss: 0.0567 - acc: 0.984 - ETA: 8s - loss: 0.0573 - acc: 0.983 - ETA: 8s - loss: 0.0572 - acc: 0.983 - ETA: 8s - loss: 0.0575 - acc: 0.983 - ETA: 8s - loss: 0.0575 - acc: 0.983 - ETA: 8s - loss: 0.0576 - acc: 0.983 - ETA: 8s - loss: 0.0574 - acc: 0.983 - ETA: 8s - loss: 0.0572 - acc: 0.983 - ETA: 7s - loss: 0.0568 - acc: 0.984 - ETA: 7s - loss: 0.0567 - acc: 0.984 - ETA: 7s - loss: 0.0567 - acc: 0.984 - ETA: 7s - loss: 0.0567 - acc: 0.984 - ETA: 7s - loss: 0.0568 - acc: 0.983 - ETA: 7s - loss: 0.0569 - acc: 0.983 - ETA: 7s - loss: 0.0571 - acc: 0.983 - ETA: 7s - loss: 0.0576 - acc: 0.983 - ETA: 7s - loss: 0.0578 - acc: 0.983 - ETA: 7s - loss: 0.0577 - acc: 0.983 - ETA: 7s - loss: 0.0580 - acc: 0.983 - ETA: 7s - loss: 0.0580 - acc: 0.983 - ETA: 7s - loss: 0.0577 - acc: 0.983 - ETA: 7s - loss: 0.0577 - acc: 0.983 - ETA: 7s - loss: 0.0573 - acc: 0.983 - ETA: 7s - loss: 0.0572 - acc: 0.983 - ETA: 7s - loss: 0.0571 - acc: 0.983 - ETA: 6s - loss: 0.0572 - acc: 0.983 - ETA: 6s - loss: 0.0571 - acc: 0.983 - ETA: 6s - loss: 0.0573 - acc: 0.983 - ETA: 6s - loss: 0.0571 - acc: 0.983 - ETA: 6s - loss: 0.0572 - acc: 0.983 - ETA: 6s - loss: 0.0575 - acc: 0.983 - ETA: 6s - loss: 0.0572 - acc: 0.983 - ETA: 6s - loss: 0.0570 - acc: 0.983 - ETA: 6s - loss: 0.0568 - acc: 0.983 - ETA: 6s - loss: 0.0565 - acc: 0.984 - ETA: 6s - loss: 0.0564 - acc: 0.984 - ETA: 6s - loss: 0.0564 - acc: 0.984 - ETA: 6s - loss: 0.0564 - acc: 0.983 - ETA: 6s - loss: 0.0563 - acc: 0.984 - ETA: 6s - loss: 0.0560 - acc: 0.984 - ETA: 6s - loss: 0.0560 - acc: 0.984 - ETA: 6s - loss: 0.0558 - acc: 0.984 - ETA: 6s - loss: 0.0558 - acc: 0.984 - ETA: 5s - loss: 0.0557 - acc: 0.984 - ETA: 5s - loss: 0.0555 - acc: 0.984 - ETA: 5s - loss: 0.0554 - acc: 0.984 - ETA: 5s - loss: 0.0555 - acc: 0.984 - ETA: 5s - loss: 0.0555 - acc: 0.984 - ETA: 5s - loss: 0.0555 - acc: 0.984 - ETA: 5s - loss: 0.0554 - acc: 0.984 - ETA: 5s - loss: 0.0554 - acc: 0.984 - ETA: 5s - loss: 0.0556 - acc: 0.984 - ETA: 5s - loss: 0.0554 - acc: 0.984 - ETA: 5s - loss: 0.0557 - acc: 0.984 - ETA: 5s - loss: 0.0558 - acc: 0.984 - ETA: 5s - loss: 0.0560 - acc: 0.983 - ETA: 5s - loss: 0.0559 - acc: 0.984 - ETA: 5s - loss: 0.0562 - acc: 0.983 - ETA: 5s - loss: 0.0562 - acc: 0.983 - ETA: 5s - loss: 0.0561 - acc: 0.983 - ETA: 4s - loss: 0.0559 - acc: 0.984 - ETA: 4s - loss: 0.0559 - acc: 0.983 - ETA: 4s - loss: 0.0558 - acc: 0.983 - ETA: 4s - loss: 0.0556 - acc: 0.984 - ETA: 4s - loss: 0.0557 - acc: 0.984 - ETA: 4s - loss: 0.0554 - acc: 0.984 - ETA: 4s - loss: 0.0554 - acc: 0.984 - ETA: 4s - loss: 0.0553 - acc: 0.984 - ETA: 4s - loss: 0.0556 - acc: 0.984 - ETA: 4s - loss: 0.0554 - acc: 0.984 - ETA: 4s - loss: 0.0552 - acc: 0.984 - ETA: 4s - loss: 0.0553 - acc: 0.984 - ETA: 4s - loss: 0.0555 - acc: 0.984 - ETA: 4s - loss: 0.0556 - acc: 0.984 - ETA: 4s - loss: 0.0555 - acc: 0.984 - ETA: 4s - loss: 0.0557 - acc: 0.984 - ETA: 4s - loss: 0.0559 - acc: 0.983 - ETA: 4s - loss: 0.0557 - acc: 0.984 - ETA: 3s - loss: 0.0557 - acc: 0.984 - ETA: 3s - loss: 0.0555 - acc: 0.984 - ETA: 3s - loss: 0.0555 - acc: 0.984 - ETA: 3s - loss: 0.0554 - acc: 0.984 - ETA: 3s - loss: 0.0552 - acc: 0.984 - ETA: 3s - loss: 0.0552 - acc: 0.984 - ETA: 3s - loss: 0.0550 - acc: 0.984 - ETA: 3s - loss: 0.0549 - acc: 0.984 - ETA: 3s - loss: 0.0548 - acc: 0.984 - ETA: 3s - loss: 0.0548 - acc: 0.984 - ETA: 3s - loss: 0.0548 - acc: 0.984 - ETA: 3s - loss: 0.0547 - acc: 0.984 - ETA: 3s - loss: 0.0548 - acc: 0.984 - ETA: 3s - loss: 0.0547 - acc: 0.984 - ETA: 3s - loss: 0.0546 - acc: 0.984 - ETA: 3s - loss: 0.0546 - acc: 0.984 - ETA: 3s - loss: 0.0549 - acc: 0.984 - ETA: 2s - loss: 0.0549 - acc: 0.984 - ETA: 2s - loss: 0.0549 - acc: 0.984 - ETA: 2s - loss: 0.0547 - acc: 0.984 - ETA: 2s - loss: 0.0546 - acc: 0.984 - ETA: 2s - loss: 0.0547 - acc: 0.984 - ETA: 2s - loss: 0.0545 - acc: 0.984 - ETA: 2s - loss: 0.0544 - acc: 0.984 - ETA: 2s - loss: 0.0542 - acc: 0.984 - ETA: 2s - loss: 0.0542 - acc: 0.984 - ETA: 2s - loss: 0.0541 - acc: 0.984 - ETA: 2s - loss: 0.0546 - acc: 0.984 - ETA: 2s - loss: 0.0547 - acc: 0.984 - ETA: 2s - loss: 0.0547 - acc: 0.984 - ETA: 2s - loss: 0.0547 - acc: 0.984 - ETA: 2s - loss: 0.0547 - acc: 0.984 - ETA: 2s - loss: 0.0546 - acc: 0.984 - ETA: 2s - loss: 0.0546 - acc: 0.984 - ETA: 2s - loss: 0.0548 - acc: 0.984 - ETA: 1s - loss: 0.0550 - acc: 0.984 - ETA: 1s - loss: 0.0549 - acc: 0.984 - ETA: 1s - loss: 0.0548 - acc: 0.984 - ETA: 1s - loss: 0.0550 - acc: 0.984 - ETA: 1s - loss: 0.0549 - acc: 0.984 - ETA: 1s - loss: 0.0549 - acc: 0.984 - ETA: 1s - loss: 0.0549 - acc: 0.984 - ETA: 1s - loss: 0.0550 - acc: 0.984 - ETA: 1s - loss: 0.0552 - acc: 0.984 - ETA: 1s - loss: 0.0552 - acc: 0.984 - ETA: 1s - loss: 0.0552 - acc: 0.984 - ETA: 1s - loss: 0.0553 - acc: 0.984 - ETA: 1s - loss: 0.0552 - acc: 0.984 - ETA: 1s - loss: 0.0551 - acc: 0.984 - ETA: 1s - loss: 0.0550 - acc: 0.9842"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 1s - loss: 0.0550 - acc: 0.984 - ETA: 1s - loss: 0.0553 - acc: 0.984 - ETA: 0s - loss: 0.0554 - acc: 0.984 - ETA: 0s - loss: 0.0554 - acc: 0.984 - ETA: 0s - loss: 0.0554 - acc: 0.984 - ETA: 0s - loss: 0.0554 - acc: 0.984 - ETA: 0s - loss: 0.0554 - acc: 0.984 - ETA: 0s - loss: 0.0553 - acc: 0.984 - ETA: 0s - loss: 0.0551 - acc: 0.984 - ETA: 0s - loss: 0.0550 - acc: 0.984 - ETA: 0s - loss: 0.0551 - acc: 0.984 - ETA: 0s - loss: 0.0551 - acc: 0.984 - ETA: 0s - loss: 0.0552 - acc: 0.984 - ETA: 0s - loss: 0.0553 - acc: 0.984 - ETA: 0s - loss: 0.0552 - acc: 0.984 - ETA: 0s - loss: 0.0551 - acc: 0.984 - ETA: 0s - loss: 0.0550 - acc: 0.984 - ETA: 0s - loss: 0.0549 - acc: 0.984 - ETA: 0s - loss: 0.0549 - acc: 0.984 - 14s 234us/step - loss: 0.0548 - acc: 0.9843 - val_loss: 0.0320 - val_acc: 0.9891\n",
      "Epoch 5/12\n",
      "54912/60000 [==========================>...] - ETA: 14s - loss: 0.1232 - acc: 0.97 - ETA: 13s - loss: 0.0952 - acc: 0.97 - ETA: 13s - loss: 0.0794 - acc: 0.98 - ETA: 13s - loss: 0.0701 - acc: 0.97 - ETA: 13s - loss: 0.0639 - acc: 0.98 - ETA: 13s - loss: 0.0568 - acc: 0.98 - ETA: 13s - loss: 0.0573 - acc: 0.98 - ETA: 13s - loss: 0.0611 - acc: 0.98 - ETA: 12s - loss: 0.0587 - acc: 0.98 - ETA: 12s - loss: 0.0590 - acc: 0.98 - ETA: 12s - loss: 0.0565 - acc: 0.98 - ETA: 12s - loss: 0.0596 - acc: 0.98 - ETA: 12s - loss: 0.0560 - acc: 0.98 - ETA: 12s - loss: 0.0548 - acc: 0.98 - ETA: 12s - loss: 0.0541 - acc: 0.98 - ETA: 12s - loss: 0.0534 - acc: 0.98 - ETA: 12s - loss: 0.0528 - acc: 0.98 - ETA: 12s - loss: 0.0522 - acc: 0.98 - ETA: 12s - loss: 0.0519 - acc: 0.98 - ETA: 12s - loss: 0.0514 - acc: 0.98 - ETA: 12s - loss: 0.0500 - acc: 0.98 - ETA: 12s - loss: 0.0495 - acc: 0.98 - ETA: 12s - loss: 0.0491 - acc: 0.98 - ETA: 12s - loss: 0.0494 - acc: 0.98 - ETA: 12s - loss: 0.0480 - acc: 0.98 - ETA: 11s - loss: 0.0480 - acc: 0.98 - ETA: 11s - loss: 0.0479 - acc: 0.98 - ETA: 11s - loss: 0.0469 - acc: 0.98 - ETA: 11s - loss: 0.0479 - acc: 0.98 - ETA: 11s - loss: 0.0498 - acc: 0.98 - ETA: 11s - loss: 0.0503 - acc: 0.98 - ETA: 11s - loss: 0.0520 - acc: 0.98 - ETA: 11s - loss: 0.0539 - acc: 0.98 - ETA: 11s - loss: 0.0537 - acc: 0.98 - ETA: 11s - loss: 0.0539 - acc: 0.98 - ETA: 11s - loss: 0.0553 - acc: 0.98 - ETA: 11s - loss: 0.0547 - acc: 0.98 - ETA: 11s - loss: 0.0539 - acc: 0.98 - ETA: 11s - loss: 0.0549 - acc: 0.98 - ETA: 11s - loss: 0.0544 - acc: 0.98 - ETA: 11s - loss: 0.0537 - acc: 0.98 - ETA: 11s - loss: 0.0538 - acc: 0.98 - ETA: 10s - loss: 0.0540 - acc: 0.98 - ETA: 10s - loss: 0.0538 - acc: 0.98 - ETA: 10s - loss: 0.0540 - acc: 0.98 - ETA: 10s - loss: 0.0540 - acc: 0.98 - ETA: 10s - loss: 0.0536 - acc: 0.98 - ETA: 10s - loss: 0.0530 - acc: 0.98 - ETA: 10s - loss: 0.0533 - acc: 0.98 - ETA: 10s - loss: 0.0532 - acc: 0.98 - ETA: 10s - loss: 0.0532 - acc: 0.98 - ETA: 10s - loss: 0.0528 - acc: 0.98 - ETA: 10s - loss: 0.0529 - acc: 0.98 - ETA: 10s - loss: 0.0527 - acc: 0.98 - ETA: 10s - loss: 0.0529 - acc: 0.98 - ETA: 10s - loss: 0.0535 - acc: 0.98 - ETA: 10s - loss: 0.0534 - acc: 0.98 - ETA: 10s - loss: 0.0535 - acc: 0.98 - ETA: 10s - loss: 0.0530 - acc: 0.98 - ETA: 10s - loss: 0.0539 - acc: 0.98 - ETA: 9s - loss: 0.0537 - acc: 0.9833 - ETA: 9s - loss: 0.0532 - acc: 0.983 - ETA: 9s - loss: 0.0531 - acc: 0.983 - ETA: 9s - loss: 0.0531 - acc: 0.983 - ETA: 9s - loss: 0.0530 - acc: 0.983 - ETA: 9s - loss: 0.0533 - acc: 0.983 - ETA: 9s - loss: 0.0530 - acc: 0.983 - ETA: 9s - loss: 0.0529 - acc: 0.983 - ETA: 9s - loss: 0.0528 - acc: 0.983 - ETA: 9s - loss: 0.0524 - acc: 0.983 - ETA: 9s - loss: 0.0524 - acc: 0.984 - ETA: 9s - loss: 0.0524 - acc: 0.984 - ETA: 9s - loss: 0.0519 - acc: 0.984 - ETA: 9s - loss: 0.0521 - acc: 0.984 - ETA: 9s - loss: 0.0516 - acc: 0.984 - ETA: 9s - loss: 0.0513 - acc: 0.984 - ETA: 9s - loss: 0.0508 - acc: 0.984 - ETA: 8s - loss: 0.0507 - acc: 0.984 - ETA: 8s - loss: 0.0507 - acc: 0.984 - ETA: 8s - loss: 0.0506 - acc: 0.984 - ETA: 8s - loss: 0.0507 - acc: 0.984 - ETA: 8s - loss: 0.0507 - acc: 0.984 - ETA: 8s - loss: 0.0503 - acc: 0.984 - ETA: 8s - loss: 0.0503 - acc: 0.984 - ETA: 8s - loss: 0.0506 - acc: 0.984 - ETA: 8s - loss: 0.0503 - acc: 0.984 - ETA: 8s - loss: 0.0506 - acc: 0.984 - ETA: 8s - loss: 0.0510 - acc: 0.984 - ETA: 8s - loss: 0.0507 - acc: 0.984 - ETA: 8s - loss: 0.0510 - acc: 0.984 - ETA: 8s - loss: 0.0510 - acc: 0.984 - ETA: 8s - loss: 0.0507 - acc: 0.984 - ETA: 8s - loss: 0.0507 - acc: 0.984 - ETA: 8s - loss: 0.0504 - acc: 0.984 - ETA: 8s - loss: 0.0503 - acc: 0.984 - ETA: 7s - loss: 0.0501 - acc: 0.984 - ETA: 7s - loss: 0.0503 - acc: 0.984 - ETA: 7s - loss: 0.0507 - acc: 0.984 - ETA: 7s - loss: 0.0506 - acc: 0.984 - ETA: 7s - loss: 0.0504 - acc: 0.984 - ETA: 7s - loss: 0.0501 - acc: 0.984 - ETA: 7s - loss: 0.0500 - acc: 0.984 - ETA: 7s - loss: 0.0498 - acc: 0.984 - ETA: 7s - loss: 0.0499 - acc: 0.984 - ETA: 7s - loss: 0.0500 - acc: 0.984 - ETA: 7s - loss: 0.0500 - acc: 0.984 - ETA: 7s - loss: 0.0499 - acc: 0.984 - ETA: 7s - loss: 0.0501 - acc: 0.984 - ETA: 7s - loss: 0.0500 - acc: 0.984 - ETA: 7s - loss: 0.0500 - acc: 0.984 - ETA: 7s - loss: 0.0499 - acc: 0.984 - ETA: 7s - loss: 0.0496 - acc: 0.984 - ETA: 6s - loss: 0.0496 - acc: 0.984 - ETA: 6s - loss: 0.0495 - acc: 0.984 - ETA: 6s - loss: 0.0496 - acc: 0.984 - ETA: 6s - loss: 0.0496 - acc: 0.984 - ETA: 6s - loss: 0.0495 - acc: 0.984 - ETA: 6s - loss: 0.0495 - acc: 0.984 - ETA: 6s - loss: 0.0496 - acc: 0.984 - ETA: 6s - loss: 0.0496 - acc: 0.984 - ETA: 6s - loss: 0.0498 - acc: 0.984 - ETA: 6s - loss: 0.0500 - acc: 0.984 - ETA: 6s - loss: 0.0499 - acc: 0.984 - ETA: 6s - loss: 0.0498 - acc: 0.984 - ETA: 6s - loss: 0.0503 - acc: 0.984 - ETA: 6s - loss: 0.0502 - acc: 0.984 - ETA: 6s - loss: 0.0503 - acc: 0.984 - ETA: 6s - loss: 0.0503 - acc: 0.984 - ETA: 6s - loss: 0.0502 - acc: 0.984 - ETA: 6s - loss: 0.0501 - acc: 0.984 - ETA: 5s - loss: 0.0499 - acc: 0.984 - ETA: 5s - loss: 0.0501 - acc: 0.984 - ETA: 5s - loss: 0.0504 - acc: 0.984 - ETA: 5s - loss: 0.0503 - acc: 0.984 - ETA: 5s - loss: 0.0502 - acc: 0.984 - ETA: 5s - loss: 0.0501 - acc: 0.984 - ETA: 5s - loss: 0.0501 - acc: 0.984 - ETA: 5s - loss: 0.0502 - acc: 0.984 - ETA: 5s - loss: 0.0502 - acc: 0.984 - ETA: 5s - loss: 0.0502 - acc: 0.984 - ETA: 5s - loss: 0.0503 - acc: 0.984 - ETA: 5s - loss: 0.0502 - acc: 0.984 - ETA: 5s - loss: 0.0501 - acc: 0.984 - ETA: 5s - loss: 0.0504 - acc: 0.984 - ETA: 5s - loss: 0.0505 - acc: 0.984 - ETA: 5s - loss: 0.0504 - acc: 0.984 - ETA: 5s - loss: 0.0503 - acc: 0.984 - ETA: 4s - loss: 0.0502 - acc: 0.984 - ETA: 4s - loss: 0.0501 - acc: 0.984 - ETA: 4s - loss: 0.0502 - acc: 0.984 - ETA: 4s - loss: 0.0501 - acc: 0.984 - ETA: 4s - loss: 0.0500 - acc: 0.984 - ETA: 4s - loss: 0.0500 - acc: 0.984 - ETA: 4s - loss: 0.0500 - acc: 0.984 - ETA: 4s - loss: 0.0499 - acc: 0.984 - ETA: 4s - loss: 0.0496 - acc: 0.984 - ETA: 4s - loss: 0.0494 - acc: 0.984 - ETA: 4s - loss: 0.0497 - acc: 0.984 - ETA: 4s - loss: 0.0494 - acc: 0.984 - ETA: 4s - loss: 0.0496 - acc: 0.984 - ETA: 4s - loss: 0.0495 - acc: 0.984 - ETA: 4s - loss: 0.0496 - acc: 0.984 - ETA: 4s - loss: 0.0496 - acc: 0.984 - ETA: 4s - loss: 0.0497 - acc: 0.984 - ETA: 4s - loss: 0.0495 - acc: 0.984 - ETA: 3s - loss: 0.0493 - acc: 0.984 - ETA: 3s - loss: 0.0492 - acc: 0.984 - ETA: 3s - loss: 0.0491 - acc: 0.984 - ETA: 3s - loss: 0.0493 - acc: 0.984 - ETA: 3s - loss: 0.0492 - acc: 0.984 - ETA: 3s - loss: 0.0490 - acc: 0.984 - ETA: 3s - loss: 0.0490 - acc: 0.984 - ETA: 3s - loss: 0.0490 - acc: 0.984 - ETA: 3s - loss: 0.0490 - acc: 0.984 - ETA: 3s - loss: 0.0489 - acc: 0.984 - ETA: 3s - loss: 0.0492 - acc: 0.984 - ETA: 3s - loss: 0.0492 - acc: 0.984 - ETA: 3s - loss: 0.0492 - acc: 0.984 - ETA: 3s - loss: 0.0492 - acc: 0.984 - ETA: 3s - loss: 0.0491 - acc: 0.984 - ETA: 3s - loss: 0.0491 - acc: 0.984 - ETA: 3s - loss: 0.0491 - acc: 0.984 - ETA: 2s - loss: 0.0491 - acc: 0.984 - ETA: 2s - loss: 0.0491 - acc: 0.984 - ETA: 2s - loss: 0.0491 - acc: 0.984 - ETA: 2s - loss: 0.0491 - acc: 0.984 - ETA: 2s - loss: 0.0489 - acc: 0.984 - ETA: 2s - loss: 0.0487 - acc: 0.984 - ETA: 2s - loss: 0.0487 - acc: 0.984 - ETA: 2s - loss: 0.0486 - acc: 0.984 - ETA: 2s - loss: 0.0488 - acc: 0.984 - ETA: 2s - loss: 0.0487 - acc: 0.984 - ETA: 2s - loss: 0.0486 - acc: 0.984 - ETA: 2s - loss: 0.0485 - acc: 0.984 - ETA: 2s - loss: 0.0484 - acc: 0.984 - ETA: 2s - loss: 0.0483 - acc: 0.984 - ETA: 2s - loss: 0.0485 - acc: 0.984 - ETA: 2s - loss: 0.0485 - acc: 0.984 - ETA: 2s - loss: 0.0485 - acc: 0.984 - ETA: 1s - loss: 0.0484 - acc: 0.984 - ETA: 1s - loss: 0.0485 - acc: 0.984 - ETA: 1s - loss: 0.0484 - acc: 0.984 - ETA: 1s - loss: 0.0484 - acc: 0.984 - ETA: 1s - loss: 0.0487 - acc: 0.984 - ETA: 1s - loss: 0.0489 - acc: 0.984 - ETA: 1s - loss: 0.0489 - acc: 0.984 - ETA: 1s - loss: 0.0491 - acc: 0.984 - ETA: 1s - loss: 0.0489 - acc: 0.984 - ETA: 1s - loss: 0.0489 - acc: 0.984 - ETA: 1s - loss: 0.0488 - acc: 0.984 - ETA: 1s - loss: 0.0490 - acc: 0.984 - ETA: 1s - loss: 0.0489 - acc: 0.984 - ETA: 1s - loss: 0.0488 - acc: 0.984 - ETA: 1s - loss: 0.0488 - acc: 0.984 - ETA: 1s - loss: 0.0486 - acc: 0.984760000/60000 [==============================] - ETA: 1s - loss: 0.0485 - acc: 0.984 - ETA: 1s - loss: 0.0484 - acc: 0.984 - ETA: 0s - loss: 0.0483 - acc: 0.984 - ETA: 0s - loss: 0.0481 - acc: 0.984 - ETA: 0s - loss: 0.0483 - acc: 0.984 - ETA: 0s - loss: 0.0485 - acc: 0.984 - ETA: 0s - loss: 0.0485 - acc: 0.984 - ETA: 0s - loss: 0.0484 - acc: 0.984 - ETA: 0s - loss: 0.0483 - acc: 0.984 - ETA: 0s - loss: 0.0484 - acc: 0.984 - ETA: 0s - loss: 0.0483 - acc: 0.984 - ETA: 0s - loss: 0.0482 - acc: 0.984 - ETA: 0s - loss: 0.0484 - acc: 0.984 - ETA: 0s - loss: 0.0485 - acc: 0.984 - ETA: 0s - loss: 0.0484 - acc: 0.984 - ETA: 0s - loss: 0.0484 - acc: 0.984 - ETA: 0s - loss: 0.0484 - acc: 0.984 - ETA: 0s - loss: 0.0483 - acc: 0.984 - ETA: 0s - loss: 0.0482 - acc: 0.985 - 14s 234us/step - loss: 0.0481 - acc: 0.9850 - val_loss: 0.0322 - val_acc: 0.9898\n",
      "Epoch 6/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0087 - acc: 1.00 - ETA: 13s - loss: 0.0102 - acc: 0.99 - ETA: 13s - loss: 0.0214 - acc: 0.99 - ETA: 13s - loss: 0.0307 - acc: 0.99 - ETA: 13s - loss: 0.0374 - acc: 0.98 - ETA: 13s - loss: 0.0461 - acc: 0.98 - ETA: 13s - loss: 0.0478 - acc: 0.98 - ETA: 12s - loss: 0.0463 - acc: 0.98 - ETA: 12s - loss: 0.0440 - acc: 0.98 - ETA: 12s - loss: 0.0444 - acc: 0.98 - ETA: 12s - loss: 0.0441 - acc: 0.98 - ETA: 12s - loss: 0.0460 - acc: 0.98 - ETA: 12s - loss: 0.0436 - acc: 0.98 - ETA: 12s - loss: 0.0419 - acc: 0.98 - ETA: 12s - loss: 0.0406 - acc: 0.98 - ETA: 12s - loss: 0.0398 - acc: 0.98 - ETA: 12s - loss: 0.0403 - acc: 0.98 - ETA: 12s - loss: 0.0392 - acc: 0.98 - ETA: 12s - loss: 0.0374 - acc: 0.98 - ETA: 12s - loss: 0.0374 - acc: 0.98 - ETA: 12s - loss: 0.0384 - acc: 0.98 - ETA: 12s - loss: 0.0376 - acc: 0.98 - ETA: 12s - loss: 0.0383 - acc: 0.98 - ETA: 12s - loss: 0.0389 - acc: 0.98 - ETA: 12s - loss: 0.0386 - acc: 0.98 - ETA: 11s - loss: 0.0379 - acc: 0.98 - ETA: 11s - loss: 0.0388 - acc: 0.98 - ETA: 11s - loss: 0.0384 - acc: 0.98 - ETA: 11s - loss: 0.0399 - acc: 0.98 - ETA: 11s - loss: 0.0403 - acc: 0.98 - ETA: 11s - loss: 0.0394 - acc: 0.98 - ETA: 11s - loss: 0.0402 - acc: 0.98 - ETA: 11s - loss: 0.0405 - acc: 0.98 - ETA: 11s - loss: 0.0401 - acc: 0.98 - ETA: 11s - loss: 0.0398 - acc: 0.98 - ETA: 11s - loss: 0.0403 - acc: 0.98 - ETA: 11s - loss: 0.0404 - acc: 0.98 - ETA: 11s - loss: 0.0406 - acc: 0.98 - ETA: 11s - loss: 0.0401 - acc: 0.98 - ETA: 11s - loss: 0.0395 - acc: 0.98 - ETA: 11s - loss: 0.0396 - acc: 0.98 - ETA: 11s - loss: 0.0395 - acc: 0.98 - ETA: 11s - loss: 0.0391 - acc: 0.98 - ETA: 10s - loss: 0.0391 - acc: 0.98 - ETA: 10s - loss: 0.0387 - acc: 0.98 - ETA: 10s - loss: 0.0384 - acc: 0.98 - ETA: 10s - loss: 0.0397 - acc: 0.98 - ETA: 10s - loss: 0.0390 - acc: 0.98 - ETA: 10s - loss: 0.0391 - acc: 0.98 - ETA: 10s - loss: 0.0388 - acc: 0.98 - ETA: 10s - loss: 0.0386 - acc: 0.98 - ETA: 10s - loss: 0.0387 - acc: 0.98 - ETA: 10s - loss: 0.0390 - acc: 0.98 - ETA: 10s - loss: 0.0388 - acc: 0.98 - ETA: 10s - loss: 0.0387 - acc: 0.98 - ETA: 10s - loss: 0.0383 - acc: 0.98 - ETA: 10s - loss: 0.0383 - acc: 0.98 - ETA: 10s - loss: 0.0383 - acc: 0.98 - ETA: 10s - loss: 0.0385 - acc: 0.98 - ETA: 10s - loss: 0.0385 - acc: 0.98 - ETA: 9s - loss: 0.0383 - acc: 0.9879 - ETA: 9s - loss: 0.0392 - acc: 0.987 - ETA: 9s - loss: 0.0394 - acc: 0.987 - ETA: 9s - loss: 0.0394 - acc: 0.987 - ETA: 9s - loss: 0.0392 - acc: 0.987 - ETA: 9s - loss: 0.0397 - acc: 0.987 - ETA: 9s - loss: 0.0396 - acc: 0.987 - ETA: 9s - loss: 0.0396 - acc: 0.987 - ETA: 9s - loss: 0.0396 - acc: 0.987 - ETA: 9s - loss: 0.0398 - acc: 0.987 - ETA: 9s - loss: 0.0396 - acc: 0.987 - ETA: 9s - loss: 0.0395 - acc: 0.987 - ETA: 9s - loss: 0.0395 - acc: 0.987 - ETA: 9s - loss: 0.0399 - acc: 0.987 - ETA: 9s - loss: 0.0398 - acc: 0.987 - ETA: 9s - loss: 0.0395 - acc: 0.987 - ETA: 9s - loss: 0.0393 - acc: 0.987 - ETA: 9s - loss: 0.0393 - acc: 0.987 - ETA: 8s - loss: 0.0392 - acc: 0.987 - ETA: 8s - loss: 0.0394 - acc: 0.987 - ETA: 8s - loss: 0.0394 - acc: 0.987 - ETA: 8s - loss: 0.0401 - acc: 0.987 - ETA: 8s - loss: 0.0400 - acc: 0.987 - ETA: 8s - loss: 0.0399 - acc: 0.987 - ETA: 8s - loss: 0.0400 - acc: 0.987 - ETA: 8s - loss: 0.0402 - acc: 0.987 - ETA: 8s - loss: 0.0405 - acc: 0.987 - ETA: 8s - loss: 0.0405 - acc: 0.987 - ETA: 8s - loss: 0.0412 - acc: 0.987 - ETA: 8s - loss: 0.0410 - acc: 0.987 - ETA: 8s - loss: 0.0408 - acc: 0.987 - ETA: 8s - loss: 0.0411 - acc: 0.987 - ETA: 8s - loss: 0.0413 - acc: 0.987 - ETA: 8s - loss: 0.0414 - acc: 0.987 - ETA: 8s - loss: 0.0412 - acc: 0.987 - ETA: 7s - loss: 0.0412 - acc: 0.987 - ETA: 7s - loss: 0.0414 - acc: 0.987 - ETA: 7s - loss: 0.0414 - acc: 0.986 - ETA: 7s - loss: 0.0415 - acc: 0.986 - ETA: 7s - loss: 0.0415 - acc: 0.986 - ETA: 7s - loss: 0.0414 - acc: 0.986 - ETA: 7s - loss: 0.0415 - acc: 0.986 - ETA: 7s - loss: 0.0414 - acc: 0.986 - ETA: 7s - loss: 0.0413 - acc: 0.986 - ETA: 7s - loss: 0.0413 - acc: 0.986 - ETA: 7s - loss: 0.0411 - acc: 0.986 - ETA: 7s - loss: 0.0411 - acc: 0.986 - ETA: 7s - loss: 0.0408 - acc: 0.987 - ETA: 7s - loss: 0.0407 - acc: 0.987 - ETA: 7s - loss: 0.0410 - acc: 0.986 - ETA: 7s - loss: 0.0411 - acc: 0.986 - ETA: 7s - loss: 0.0411 - acc: 0.986 - ETA: 6s - loss: 0.0410 - acc: 0.986 - ETA: 6s - loss: 0.0410 - acc: 0.987 - ETA: 6s - loss: 0.0410 - acc: 0.987 - ETA: 6s - loss: 0.0409 - acc: 0.987 - ETA: 6s - loss: 0.0409 - acc: 0.987 - ETA: 6s - loss: 0.0406 - acc: 0.987 - ETA: 6s - loss: 0.0412 - acc: 0.986 - ETA: 6s - loss: 0.0410 - acc: 0.986 - ETA: 6s - loss: 0.0410 - acc: 0.987 - ETA: 6s - loss: 0.0410 - acc: 0.987 - ETA: 6s - loss: 0.0413 - acc: 0.987 - ETA: 6s - loss: 0.0416 - acc: 0.986 - ETA: 6s - loss: 0.0415 - acc: 0.986 - ETA: 6s - loss: 0.0414 - acc: 0.987 - ETA: 6s - loss: 0.0414 - acc: 0.987 - ETA: 6s - loss: 0.0414 - acc: 0.986 - ETA: 6s - loss: 0.0413 - acc: 0.986 - ETA: 6s - loss: 0.0411 - acc: 0.987 - ETA: 5s - loss: 0.0409 - acc: 0.987 - ETA: 5s - loss: 0.0412 - acc: 0.987 - ETA: 5s - loss: 0.0411 - acc: 0.987 - ETA: 5s - loss: 0.0409 - acc: 0.987 - ETA: 5s - loss: 0.0407 - acc: 0.987 - ETA: 5s - loss: 0.0407 - acc: 0.987 - ETA: 5s - loss: 0.0406 - acc: 0.987 - ETA: 5s - loss: 0.0408 - acc: 0.987 - ETA: 5s - loss: 0.0407 - acc: 0.987 - ETA: 5s - loss: 0.0407 - acc: 0.987 - ETA: 5s - loss: 0.0408 - acc: 0.987 - ETA: 5s - loss: 0.0408 - acc: 0.987 - ETA: 5s - loss: 0.0406 - acc: 0.987 - ETA: 5s - loss: 0.0405 - acc: 0.987 - ETA: 5s - loss: 0.0407 - acc: 0.987 - ETA: 5s - loss: 0.0410 - acc: 0.987 - ETA: 5s - loss: 0.0410 - acc: 0.987 - ETA: 4s - loss: 0.0409 - acc: 0.987 - ETA: 4s - loss: 0.0411 - acc: 0.987 - ETA: 4s - loss: 0.0414 - acc: 0.986 - ETA: 4s - loss: 0.0412 - acc: 0.986 - ETA: 4s - loss: 0.0411 - acc: 0.987 - ETA: 4s - loss: 0.0409 - acc: 0.987 - ETA: 4s - loss: 0.0409 - acc: 0.987 - ETA: 4s - loss: 0.0407 - acc: 0.987 - ETA: 4s - loss: 0.0407 - acc: 0.987 - ETA: 4s - loss: 0.0406 - acc: 0.987 - ETA: 4s - loss: 0.0405 - acc: 0.987 - ETA: 4s - loss: 0.0404 - acc: 0.987 - ETA: 4s - loss: 0.0403 - acc: 0.987 - ETA: 4s - loss: 0.0402 - acc: 0.987 - ETA: 4s - loss: 0.0403 - acc: 0.987 - ETA: 4s - loss: 0.0402 - acc: 0.987 - ETA: 4s - loss: 0.0401 - acc: 0.987 - ETA: 4s - loss: 0.0400 - acc: 0.987 - ETA: 3s - loss: 0.0399 - acc: 0.987 - ETA: 3s - loss: 0.0400 - acc: 0.987 - ETA: 3s - loss: 0.0400 - acc: 0.987 - ETA: 3s - loss: 0.0399 - acc: 0.987 - ETA: 3s - loss: 0.0398 - acc: 0.987 - ETA: 3s - loss: 0.0397 - acc: 0.987 - ETA: 3s - loss: 0.0399 - acc: 0.987 - ETA: 3s - loss: 0.0398 - acc: 0.987 - ETA: 3s - loss: 0.0401 - acc: 0.987 - ETA: 3s - loss: 0.0399 - acc: 0.987 - ETA: 3s - loss: 0.0399 - acc: 0.987 - ETA: 3s - loss: 0.0398 - acc: 0.987 - ETA: 3s - loss: 0.0399 - acc: 0.987 - ETA: 3s - loss: 0.0399 - acc: 0.987 - ETA: 3s - loss: 0.0397 - acc: 0.987 - ETA: 3s - loss: 0.0396 - acc: 0.987 - ETA: 3s - loss: 0.0398 - acc: 0.987 - ETA: 2s - loss: 0.0397 - acc: 0.987 - ETA: 2s - loss: 0.0400 - acc: 0.987 - ETA: 2s - loss: 0.0404 - acc: 0.987 - ETA: 2s - loss: 0.0405 - acc: 0.987 - ETA: 2s - loss: 0.0408 - acc: 0.987 - ETA: 2s - loss: 0.0408 - acc: 0.987 - ETA: 2s - loss: 0.0408 - acc: 0.987 - ETA: 2s - loss: 0.0408 - acc: 0.987 - ETA: 2s - loss: 0.0408 - acc: 0.987 - ETA: 2s - loss: 0.0408 - acc: 0.987 - ETA: 2s - loss: 0.0409 - acc: 0.987 - ETA: 2s - loss: 0.0409 - acc: 0.987 - ETA: 2s - loss: 0.0409 - acc: 0.987 - ETA: 2s - loss: 0.0409 - acc: 0.987 - ETA: 2s - loss: 0.0410 - acc: 0.987 - ETA: 2s - loss: 0.0411 - acc: 0.987 - ETA: 2s - loss: 0.0410 - acc: 0.987 - ETA: 2s - loss: 0.0409 - acc: 0.987 - ETA: 1s - loss: 0.0409 - acc: 0.987 - ETA: 1s - loss: 0.0410 - acc: 0.987 - ETA: 1s - loss: 0.0409 - acc: 0.987 - ETA: 1s - loss: 0.0410 - acc: 0.987 - ETA: 1s - loss: 0.0411 - acc: 0.987 - ETA: 1s - loss: 0.0411 - acc: 0.987 - ETA: 1s - loss: 0.0412 - acc: 0.987 - ETA: 1s - loss: 0.0411 - acc: 0.987 - ETA: 1s - loss: 0.0412 - acc: 0.987 - ETA: 1s - loss: 0.0411 - acc: 0.987 - ETA: 1s - loss: 0.0412 - acc: 0.987 - ETA: 1s - loss: 0.0412 - acc: 0.987 - ETA: 1s - loss: 0.0412 - acc: 0.987 - ETA: 1s - loss: 0.0412 - acc: 0.987 - ETA: 1s - loss: 0.0413 - acc: 0.9870"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 1s - loss: 0.0414 - acc: 0.986 - ETA: 1s - loss: 0.0415 - acc: 0.987 - ETA: 0s - loss: 0.0415 - acc: 0.986 - ETA: 0s - loss: 0.0415 - acc: 0.986 - ETA: 0s - loss: 0.0417 - acc: 0.986 - ETA: 0s - loss: 0.0418 - acc: 0.986 - ETA: 0s - loss: 0.0417 - acc: 0.986 - ETA: 0s - loss: 0.0421 - acc: 0.986 - ETA: 0s - loss: 0.0420 - acc: 0.986 - ETA: 0s - loss: 0.0420 - acc: 0.986 - ETA: 0s - loss: 0.0421 - acc: 0.986 - ETA: 0s - loss: 0.0422 - acc: 0.986 - ETA: 0s - loss: 0.0421 - acc: 0.986 - ETA: 0s - loss: 0.0421 - acc: 0.986 - ETA: 0s - loss: 0.0420 - acc: 0.986 - ETA: 0s - loss: 0.0420 - acc: 0.987 - ETA: 0s - loss: 0.0419 - acc: 0.987 - ETA: 0s - loss: 0.0419 - acc: 0.987 - ETA: 0s - loss: 0.0421 - acc: 0.987 - 14s 236us/step - loss: 0.0422 - acc: 0.9869 - val_loss: 0.0336 - val_acc: 0.9894\n",
      "Epoch 7/12\n",
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0223 - acc: 0.99 - ETA: 13s - loss: 0.0323 - acc: 0.98 - ETA: 13s - loss: 0.0353 - acc: 0.98 - ETA: 13s - loss: 0.0357 - acc: 0.98 - ETA: 12s - loss: 0.0332 - acc: 0.98 - ETA: 12s - loss: 0.0308 - acc: 0.98 - ETA: 12s - loss: 0.0324 - acc: 0.98 - ETA: 12s - loss: 0.0319 - acc: 0.98 - ETA: 12s - loss: 0.0292 - acc: 0.98 - ETA: 12s - loss: 0.0336 - acc: 0.98 - ETA: 12s - loss: 0.0361 - acc: 0.98 - ETA: 12s - loss: 0.0396 - acc: 0.98 - ETA: 12s - loss: 0.0394 - acc: 0.98 - ETA: 12s - loss: 0.0405 - acc: 0.98 - ETA: 12s - loss: 0.0406 - acc: 0.98 - ETA: 12s - loss: 0.0387 - acc: 0.98 - ETA: 12s - loss: 0.0379 - acc: 0.98 - ETA: 12s - loss: 0.0432 - acc: 0.98 - ETA: 12s - loss: 0.0421 - acc: 0.98 - ETA: 12s - loss: 0.0413 - acc: 0.98 - ETA: 12s - loss: 0.0408 - acc: 0.98 - ETA: 12s - loss: 0.0427 - acc: 0.98 - ETA: 12s - loss: 0.0430 - acc: 0.98 - ETA: 12s - loss: 0.0417 - acc: 0.98 - ETA: 12s - loss: 0.0417 - acc: 0.98 - ETA: 11s - loss: 0.0408 - acc: 0.98 - ETA: 11s - loss: 0.0399 - acc: 0.98 - ETA: 11s - loss: 0.0409 - acc: 0.98 - ETA: 11s - loss: 0.0407 - acc: 0.98 - ETA: 11s - loss: 0.0403 - acc: 0.98 - ETA: 11s - loss: 0.0395 - acc: 0.98 - ETA: 11s - loss: 0.0389 - acc: 0.98 - ETA: 11s - loss: 0.0385 - acc: 0.98 - ETA: 11s - loss: 0.0389 - acc: 0.98 - ETA: 11s - loss: 0.0383 - acc: 0.98 - ETA: 11s - loss: 0.0386 - acc: 0.98 - ETA: 11s - loss: 0.0378 - acc: 0.98 - ETA: 11s - loss: 0.0380 - acc: 0.98 - ETA: 11s - loss: 0.0374 - acc: 0.98 - ETA: 11s - loss: 0.0377 - acc: 0.98 - ETA: 11s - loss: 0.0373 - acc: 0.98 - ETA: 11s - loss: 0.0371 - acc: 0.98 - ETA: 11s - loss: 0.0370 - acc: 0.98 - ETA: 10s - loss: 0.0363 - acc: 0.98 - ETA: 10s - loss: 0.0367 - acc: 0.98 - ETA: 10s - loss: 0.0366 - acc: 0.98 - ETA: 10s - loss: 0.0369 - acc: 0.98 - ETA: 10s - loss: 0.0375 - acc: 0.98 - ETA: 10s - loss: 0.0376 - acc: 0.98 - ETA: 10s - loss: 0.0373 - acc: 0.98 - ETA: 10s - loss: 0.0372 - acc: 0.98 - ETA: 10s - loss: 0.0370 - acc: 0.98 - ETA: 10s - loss: 0.0371 - acc: 0.98 - ETA: 10s - loss: 0.0370 - acc: 0.98 - ETA: 10s - loss: 0.0367 - acc: 0.98 - ETA: 10s - loss: 0.0367 - acc: 0.98 - ETA: 10s - loss: 0.0365 - acc: 0.98 - ETA: 10s - loss: 0.0364 - acc: 0.98 - ETA: 10s - loss: 0.0368 - acc: 0.98 - ETA: 10s - loss: 0.0368 - acc: 0.98 - ETA: 10s - loss: 0.0367 - acc: 0.98 - ETA: 9s - loss: 0.0373 - acc: 0.9878 - ETA: 9s - loss: 0.0375 - acc: 0.987 - ETA: 9s - loss: 0.0379 - acc: 0.987 - ETA: 9s - loss: 0.0376 - acc: 0.987 - ETA: 9s - loss: 0.0376 - acc: 0.988 - ETA: 9s - loss: 0.0373 - acc: 0.988 - ETA: 9s - loss: 0.0373 - acc: 0.988 - ETA: 9s - loss: 0.0371 - acc: 0.988 - ETA: 9s - loss: 0.0367 - acc: 0.988 - ETA: 9s - loss: 0.0364 - acc: 0.988 - ETA: 9s - loss: 0.0360 - acc: 0.988 - ETA: 9s - loss: 0.0357 - acc: 0.988 - ETA: 9s - loss: 0.0358 - acc: 0.988 - ETA: 9s - loss: 0.0356 - acc: 0.988 - ETA: 9s - loss: 0.0356 - acc: 0.988 - ETA: 9s - loss: 0.0355 - acc: 0.988 - ETA: 9s - loss: 0.0360 - acc: 0.988 - ETA: 8s - loss: 0.0364 - acc: 0.988 - ETA: 8s - loss: 0.0361 - acc: 0.988 - ETA: 8s - loss: 0.0363 - acc: 0.988 - ETA: 8s - loss: 0.0364 - acc: 0.988 - ETA: 8s - loss: 0.0366 - acc: 0.988 - ETA: 8s - loss: 0.0366 - acc: 0.988 - ETA: 8s - loss: 0.0364 - acc: 0.988 - ETA: 8s - loss: 0.0363 - acc: 0.988 - ETA: 8s - loss: 0.0361 - acc: 0.988 - ETA: 8s - loss: 0.0360 - acc: 0.988 - ETA: 8s - loss: 0.0360 - acc: 0.988 - ETA: 8s - loss: 0.0361 - acc: 0.988 - ETA: 8s - loss: 0.0361 - acc: 0.988 - ETA: 8s - loss: 0.0360 - acc: 0.988 - ETA: 8s - loss: 0.0361 - acc: 0.988 - ETA: 8s - loss: 0.0359 - acc: 0.988 - ETA: 8s - loss: 0.0361 - acc: 0.988 - ETA: 7s - loss: 0.0360 - acc: 0.988 - ETA: 7s - loss: 0.0358 - acc: 0.988 - ETA: 7s - loss: 0.0363 - acc: 0.988 - ETA: 7s - loss: 0.0364 - acc: 0.988 - ETA: 7s - loss: 0.0363 - acc: 0.988 - ETA: 7s - loss: 0.0366 - acc: 0.988 - ETA: 7s - loss: 0.0366 - acc: 0.988 - ETA: 7s - loss: 0.0368 - acc: 0.988 - ETA: 7s - loss: 0.0368 - acc: 0.988 - ETA: 7s - loss: 0.0366 - acc: 0.988 - ETA: 7s - loss: 0.0366 - acc: 0.988 - ETA: 7s - loss: 0.0369 - acc: 0.988 - ETA: 7s - loss: 0.0368 - acc: 0.988 - ETA: 7s - loss: 0.0367 - acc: 0.988 - ETA: 7s - loss: 0.0370 - acc: 0.988 - ETA: 7s - loss: 0.0369 - acc: 0.988 - ETA: 7s - loss: 0.0367 - acc: 0.988 - ETA: 7s - loss: 0.0366 - acc: 0.988 - ETA: 6s - loss: 0.0364 - acc: 0.988 - ETA: 6s - loss: 0.0370 - acc: 0.988 - ETA: 6s - loss: 0.0369 - acc: 0.988 - ETA: 6s - loss: 0.0375 - acc: 0.988 - ETA: 6s - loss: 0.0375 - acc: 0.988 - ETA: 6s - loss: 0.0376 - acc: 0.988 - ETA: 6s - loss: 0.0375 - acc: 0.988 - ETA: 6s - loss: 0.0376 - acc: 0.988 - ETA: 6s - loss: 0.0378 - acc: 0.988 - ETA: 6s - loss: 0.0378 - acc: 0.988 - ETA: 6s - loss: 0.0378 - acc: 0.988 - ETA: 6s - loss: 0.0378 - acc: 0.988 - ETA: 6s - loss: 0.0377 - acc: 0.988 - ETA: 6s - loss: 0.0380 - acc: 0.988 - ETA: 6s - loss: 0.0379 - acc: 0.988 - ETA: 6s - loss: 0.0376 - acc: 0.988 - ETA: 6s - loss: 0.0376 - acc: 0.988 - ETA: 5s - loss: 0.0376 - acc: 0.988 - ETA: 5s - loss: 0.0374 - acc: 0.988 - ETA: 5s - loss: 0.0375 - acc: 0.988 - ETA: 5s - loss: 0.0374 - acc: 0.988 - ETA: 5s - loss: 0.0377 - acc: 0.988 - ETA: 5s - loss: 0.0376 - acc: 0.988 - ETA: 5s - loss: 0.0374 - acc: 0.988 - ETA: 5s - loss: 0.0375 - acc: 0.988 - ETA: 5s - loss: 0.0373 - acc: 0.988 - ETA: 5s - loss: 0.0378 - acc: 0.988 - ETA: 5s - loss: 0.0377 - acc: 0.988 - ETA: 5s - loss: 0.0378 - acc: 0.988 - ETA: 5s - loss: 0.0379 - acc: 0.988 - ETA: 5s - loss: 0.0378 - acc: 0.988 - ETA: 5s - loss: 0.0380 - acc: 0.988 - ETA: 5s - loss: 0.0378 - acc: 0.988 - ETA: 5s - loss: 0.0380 - acc: 0.988 - ETA: 5s - loss: 0.0381 - acc: 0.988 - ETA: 4s - loss: 0.0381 - acc: 0.988 - ETA: 4s - loss: 0.0383 - acc: 0.988 - ETA: 4s - loss: 0.0383 - acc: 0.988 - ETA: 4s - loss: 0.0383 - acc: 0.988 - ETA: 4s - loss: 0.0381 - acc: 0.988 - ETA: 4s - loss: 0.0380 - acc: 0.988 - ETA: 4s - loss: 0.0381 - acc: 0.988 - ETA: 4s - loss: 0.0380 - acc: 0.988 - ETA: 4s - loss: 0.0381 - acc: 0.988 - ETA: 4s - loss: 0.0382 - acc: 0.988 - ETA: 4s - loss: 0.0384 - acc: 0.988 - ETA: 4s - loss: 0.0384 - acc: 0.988 - ETA: 4s - loss: 0.0383 - acc: 0.988 - ETA: 4s - loss: 0.0385 - acc: 0.988 - ETA: 4s - loss: 0.0384 - acc: 0.988 - ETA: 4s - loss: 0.0384 - acc: 0.988 - ETA: 4s - loss: 0.0383 - acc: 0.988 - ETA: 3s - loss: 0.0383 - acc: 0.988 - ETA: 3s - loss: 0.0384 - acc: 0.988 - ETA: 3s - loss: 0.0383 - acc: 0.988 - ETA: 3s - loss: 0.0384 - acc: 0.988 - ETA: 3s - loss: 0.0383 - acc: 0.988 - ETA: 3s - loss: 0.0382 - acc: 0.988 - ETA: 3s - loss: 0.0381 - acc: 0.988 - ETA: 3s - loss: 0.0382 - acc: 0.988 - ETA: 3s - loss: 0.0383 - acc: 0.988 - ETA: 3s - loss: 0.0382 - acc: 0.988 - ETA: 3s - loss: 0.0383 - acc: 0.988 - ETA: 3s - loss: 0.0382 - acc: 0.988 - ETA: 3s - loss: 0.0381 - acc: 0.988 - ETA: 3s - loss: 0.0380 - acc: 0.988 - ETA: 3s - loss: 0.0379 - acc: 0.988 - ETA: 3s - loss: 0.0379 - acc: 0.988 - ETA: 3s - loss: 0.0377 - acc: 0.988 - ETA: 2s - loss: 0.0378 - acc: 0.988 - ETA: 2s - loss: 0.0380 - acc: 0.988 - ETA: 2s - loss: 0.0379 - acc: 0.988 - ETA: 2s - loss: 0.0378 - acc: 0.988 - ETA: 2s - loss: 0.0380 - acc: 0.988 - ETA: 2s - loss: 0.0379 - acc: 0.988 - ETA: 2s - loss: 0.0379 - acc: 0.988 - ETA: 2s - loss: 0.0378 - acc: 0.988 - ETA: 2s - loss: 0.0378 - acc: 0.988 - ETA: 2s - loss: 0.0382 - acc: 0.988 - ETA: 2s - loss: 0.0382 - acc: 0.988 - ETA: 2s - loss: 0.0383 - acc: 0.988 - ETA: 2s - loss: 0.0383 - acc: 0.988 - ETA: 2s - loss: 0.0382 - acc: 0.988 - ETA: 2s - loss: 0.0381 - acc: 0.988 - ETA: 2s - loss: 0.0379 - acc: 0.988 - ETA: 2s - loss: 0.0379 - acc: 0.988 - ETA: 2s - loss: 0.0381 - acc: 0.988 - ETA: 1s - loss: 0.0382 - acc: 0.988 - ETA: 1s - loss: 0.0382 - acc: 0.988 - ETA: 1s - loss: 0.0381 - acc: 0.988 - ETA: 1s - loss: 0.0379 - acc: 0.988 - ETA: 1s - loss: 0.0378 - acc: 0.988 - ETA: 1s - loss: 0.0379 - acc: 0.988 - ETA: 1s - loss: 0.0378 - acc: 0.988 - ETA: 1s - loss: 0.0378 - acc: 0.988 - ETA: 1s - loss: 0.0381 - acc: 0.988 - ETA: 1s - loss: 0.0382 - acc: 0.988 - ETA: 1s - loss: 0.0381 - acc: 0.988 - ETA: 1s - loss: 0.0382 - acc: 0.988 - ETA: 1s - loss: 0.0382 - acc: 0.988 - ETA: 1s - loss: 0.0381 - acc: 0.988 - ETA: 1s - loss: 0.0383 - acc: 0.988760000/60000 [==============================] - ETA: 1s - loss: 0.0383 - acc: 0.988 - ETA: 1s - loss: 0.0384 - acc: 0.988 - ETA: 0s - loss: 0.0384 - acc: 0.988 - ETA: 0s - loss: 0.0384 - acc: 0.988 - ETA: 0s - loss: 0.0385 - acc: 0.988 - ETA: 0s - loss: 0.0386 - acc: 0.988 - ETA: 0s - loss: 0.0385 - acc: 0.988 - ETA: 0s - loss: 0.0385 - acc: 0.988 - ETA: 0s - loss: 0.0384 - acc: 0.988 - ETA: 0s - loss: 0.0384 - acc: 0.988 - ETA: 0s - loss: 0.0384 - acc: 0.988 - ETA: 0s - loss: 0.0382 - acc: 0.988 - ETA: 0s - loss: 0.0382 - acc: 0.988 - ETA: 0s - loss: 0.0382 - acc: 0.988 - ETA: 0s - loss: 0.0382 - acc: 0.988 - ETA: 0s - loss: 0.0382 - acc: 0.988 - ETA: 0s - loss: 0.0382 - acc: 0.988 - ETA: 0s - loss: 0.0383 - acc: 0.988 - ETA: 0s - loss: 0.0383 - acc: 0.988 - 14s 235us/step - loss: 0.0383 - acc: 0.9884 - val_loss: 0.0307 - val_acc: 0.9888\n",
      "Epoch 8/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0574 - acc: 0.98 - ETA: 13s - loss: 0.0652 - acc: 0.98 - ETA: 13s - loss: 0.0488 - acc: 0.98 - ETA: 13s - loss: 0.0485 - acc: 0.98 - ETA: 13s - loss: 0.0439 - acc: 0.98 - ETA: 13s - loss: 0.0399 - acc: 0.98 - ETA: 13s - loss: 0.0418 - acc: 0.98 - ETA: 13s - loss: 0.0399 - acc: 0.98 - ETA: 12s - loss: 0.0405 - acc: 0.98 - ETA: 12s - loss: 0.0385 - acc: 0.98 - ETA: 12s - loss: 0.0398 - acc: 0.98 - ETA: 12s - loss: 0.0382 - acc: 0.98 - ETA: 12s - loss: 0.0394 - acc: 0.98 - ETA: 12s - loss: 0.0393 - acc: 0.98 - ETA: 12s - loss: 0.0375 - acc: 0.98 - ETA: 12s - loss: 0.0376 - acc: 0.98 - ETA: 12s - loss: 0.0360 - acc: 0.98 - ETA: 12s - loss: 0.0349 - acc: 0.98 - ETA: 12s - loss: 0.0338 - acc: 0.98 - ETA: 12s - loss: 0.0372 - acc: 0.98 - ETA: 12s - loss: 0.0374 - acc: 0.98 - ETA: 12s - loss: 0.0364 - acc: 0.98 - ETA: 12s - loss: 0.0365 - acc: 0.98 - ETA: 12s - loss: 0.0357 - acc: 0.98 - ETA: 12s - loss: 0.0346 - acc: 0.98 - ETA: 11s - loss: 0.0340 - acc: 0.98 - ETA: 11s - loss: 0.0340 - acc: 0.98 - ETA: 11s - loss: 0.0365 - acc: 0.98 - ETA: 11s - loss: 0.0368 - acc: 0.98 - ETA: 11s - loss: 0.0364 - acc: 0.98 - ETA: 11s - loss: 0.0363 - acc: 0.98 - ETA: 11s - loss: 0.0361 - acc: 0.98 - ETA: 11s - loss: 0.0358 - acc: 0.98 - ETA: 11s - loss: 0.0355 - acc: 0.98 - ETA: 11s - loss: 0.0359 - acc: 0.98 - ETA: 11s - loss: 0.0360 - acc: 0.98 - ETA: 11s - loss: 0.0357 - acc: 0.98 - ETA: 11s - loss: 0.0356 - acc: 0.98 - ETA: 11s - loss: 0.0363 - acc: 0.98 - ETA: 11s - loss: 0.0356 - acc: 0.98 - ETA: 11s - loss: 0.0353 - acc: 0.98 - ETA: 11s - loss: 0.0347 - acc: 0.98 - ETA: 11s - loss: 0.0354 - acc: 0.98 - ETA: 10s - loss: 0.0353 - acc: 0.98 - ETA: 10s - loss: 0.0353 - acc: 0.98 - ETA: 10s - loss: 0.0358 - acc: 0.98 - ETA: 10s - loss: 0.0359 - acc: 0.98 - ETA: 10s - loss: 0.0366 - acc: 0.98 - ETA: 10s - loss: 0.0367 - acc: 0.98 - ETA: 10s - loss: 0.0366 - acc: 0.98 - ETA: 10s - loss: 0.0362 - acc: 0.98 - ETA: 10s - loss: 0.0359 - acc: 0.98 - ETA: 10s - loss: 0.0360 - acc: 0.98 - ETA: 10s - loss: 0.0356 - acc: 0.98 - ETA: 10s - loss: 0.0358 - acc: 0.98 - ETA: 10s - loss: 0.0357 - acc: 0.98 - ETA: 10s - loss: 0.0355 - acc: 0.98 - ETA: 10s - loss: 0.0353 - acc: 0.98 - ETA: 10s - loss: 0.0350 - acc: 0.98 - ETA: 10s - loss: 0.0346 - acc: 0.98 - ETA: 9s - loss: 0.0341 - acc: 0.9890 - ETA: 9s - loss: 0.0339 - acc: 0.989 - ETA: 9s - loss: 0.0341 - acc: 0.989 - ETA: 9s - loss: 0.0346 - acc: 0.988 - ETA: 9s - loss: 0.0344 - acc: 0.989 - ETA: 9s - loss: 0.0342 - acc: 0.989 - ETA: 9s - loss: 0.0340 - acc: 0.989 - ETA: 9s - loss: 0.0342 - acc: 0.989 - ETA: 9s - loss: 0.0343 - acc: 0.989 - ETA: 9s - loss: 0.0346 - acc: 0.989 - ETA: 9s - loss: 0.0346 - acc: 0.989 - ETA: 9s - loss: 0.0350 - acc: 0.988 - ETA: 9s - loss: 0.0349 - acc: 0.988 - ETA: 9s - loss: 0.0347 - acc: 0.988 - ETA: 9s - loss: 0.0347 - acc: 0.988 - ETA: 9s - loss: 0.0347 - acc: 0.988 - ETA: 9s - loss: 0.0344 - acc: 0.989 - ETA: 8s - loss: 0.0344 - acc: 0.989 - ETA: 8s - loss: 0.0344 - acc: 0.989 - ETA: 8s - loss: 0.0342 - acc: 0.989 - ETA: 8s - loss: 0.0345 - acc: 0.989 - ETA: 8s - loss: 0.0342 - acc: 0.989 - ETA: 8s - loss: 0.0339 - acc: 0.989 - ETA: 8s - loss: 0.0341 - acc: 0.989 - ETA: 8s - loss: 0.0338 - acc: 0.989 - ETA: 8s - loss: 0.0337 - acc: 0.989 - ETA: 8s - loss: 0.0337 - acc: 0.989 - ETA: 8s - loss: 0.0335 - acc: 0.989 - ETA: 8s - loss: 0.0336 - acc: 0.989 - ETA: 8s - loss: 0.0333 - acc: 0.989 - ETA: 8s - loss: 0.0334 - acc: 0.989 - ETA: 8s - loss: 0.0332 - acc: 0.989 - ETA: 8s - loss: 0.0334 - acc: 0.989 - ETA: 8s - loss: 0.0332 - acc: 0.989 - ETA: 8s - loss: 0.0333 - acc: 0.989 - ETA: 7s - loss: 0.0333 - acc: 0.989 - ETA: 7s - loss: 0.0332 - acc: 0.989 - ETA: 7s - loss: 0.0330 - acc: 0.989 - ETA: 7s - loss: 0.0330 - acc: 0.989 - ETA: 7s - loss: 0.0333 - acc: 0.989 - ETA: 7s - loss: 0.0331 - acc: 0.989 - ETA: 7s - loss: 0.0331 - acc: 0.989 - ETA: 7s - loss: 0.0331 - acc: 0.989 - ETA: 7s - loss: 0.0331 - acc: 0.989 - ETA: 7s - loss: 0.0329 - acc: 0.989 - ETA: 7s - loss: 0.0328 - acc: 0.989 - ETA: 7s - loss: 0.0328 - acc: 0.989 - ETA: 7s - loss: 0.0326 - acc: 0.989 - ETA: 7s - loss: 0.0331 - acc: 0.989 - ETA: 7s - loss: 0.0332 - acc: 0.989 - ETA: 7s - loss: 0.0332 - acc: 0.989 - ETA: 7s - loss: 0.0330 - acc: 0.989 - ETA: 6s - loss: 0.0334 - acc: 0.989 - ETA: 6s - loss: 0.0333 - acc: 0.989 - ETA: 6s - loss: 0.0333 - acc: 0.989 - ETA: 6s - loss: 0.0331 - acc: 0.989 - ETA: 6s - loss: 0.0330 - acc: 0.989 - ETA: 6s - loss: 0.0330 - acc: 0.989 - ETA: 6s - loss: 0.0328 - acc: 0.989 - ETA: 6s - loss: 0.0328 - acc: 0.989 - ETA: 6s - loss: 0.0328 - acc: 0.989 - ETA: 6s - loss: 0.0326 - acc: 0.989 - ETA: 6s - loss: 0.0327 - acc: 0.989 - ETA: 6s - loss: 0.0325 - acc: 0.989 - ETA: 6s - loss: 0.0327 - acc: 0.989 - ETA: 6s - loss: 0.0327 - acc: 0.989 - ETA: 6s - loss: 0.0330 - acc: 0.989 - ETA: 6s - loss: 0.0329 - acc: 0.989 - ETA: 6s - loss: 0.0327 - acc: 0.989 - ETA: 6s - loss: 0.0326 - acc: 0.989 - ETA: 5s - loss: 0.0324 - acc: 0.989 - ETA: 5s - loss: 0.0325 - acc: 0.989 - ETA: 5s - loss: 0.0329 - acc: 0.989 - ETA: 5s - loss: 0.0328 - acc: 0.989 - ETA: 5s - loss: 0.0328 - acc: 0.989 - ETA: 5s - loss: 0.0328 - acc: 0.989 - ETA: 5s - loss: 0.0327 - acc: 0.989 - ETA: 5s - loss: 0.0327 - acc: 0.989 - ETA: 5s - loss: 0.0327 - acc: 0.989 - ETA: 5s - loss: 0.0329 - acc: 0.989 - ETA: 5s - loss: 0.0328 - acc: 0.989 - ETA: 5s - loss: 0.0329 - acc: 0.989 - ETA: 5s - loss: 0.0332 - acc: 0.989 - ETA: 5s - loss: 0.0333 - acc: 0.989 - ETA: 5s - loss: 0.0333 - acc: 0.989 - ETA: 5s - loss: 0.0336 - acc: 0.989 - ETA: 5s - loss: 0.0336 - acc: 0.989 - ETA: 4s - loss: 0.0338 - acc: 0.989 - ETA: 4s - loss: 0.0338 - acc: 0.989 - ETA: 4s - loss: 0.0339 - acc: 0.989 - ETA: 4s - loss: 0.0340 - acc: 0.989 - ETA: 4s - loss: 0.0340 - acc: 0.989 - ETA: 4s - loss: 0.0338 - acc: 0.989 - ETA: 4s - loss: 0.0338 - acc: 0.989 - ETA: 4s - loss: 0.0337 - acc: 0.989 - ETA: 4s - loss: 0.0336 - acc: 0.989 - ETA: 4s - loss: 0.0335 - acc: 0.989 - ETA: 4s - loss: 0.0337 - acc: 0.989 - ETA: 4s - loss: 0.0337 - acc: 0.989 - ETA: 4s - loss: 0.0337 - acc: 0.989 - ETA: 4s - loss: 0.0337 - acc: 0.989 - ETA: 4s - loss: 0.0336 - acc: 0.989 - ETA: 4s - loss: 0.0336 - acc: 0.989 - ETA: 4s - loss: 0.0338 - acc: 0.989 - ETA: 4s - loss: 0.0338 - acc: 0.989 - ETA: 3s - loss: 0.0337 - acc: 0.989 - ETA: 3s - loss: 0.0337 - acc: 0.989 - ETA: 3s - loss: 0.0336 - acc: 0.989 - ETA: 3s - loss: 0.0337 - acc: 0.989 - ETA: 3s - loss: 0.0336 - acc: 0.989 - ETA: 3s - loss: 0.0335 - acc: 0.989 - ETA: 3s - loss: 0.0336 - acc: 0.989 - ETA: 3s - loss: 0.0337 - acc: 0.989 - ETA: 3s - loss: 0.0336 - acc: 0.989 - ETA: 3s - loss: 0.0336 - acc: 0.989 - ETA: 3s - loss: 0.0337 - acc: 0.989 - ETA: 3s - loss: 0.0336 - acc: 0.989 - ETA: 3s - loss: 0.0334 - acc: 0.989 - ETA: 3s - loss: 0.0335 - acc: 0.989 - ETA: 3s - loss: 0.0334 - acc: 0.989 - ETA: 3s - loss: 0.0335 - acc: 0.989 - ETA: 3s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0337 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0337 - acc: 0.989 - ETA: 2s - loss: 0.0339 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0337 - acc: 0.989 - ETA: 2s - loss: 0.0337 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0339 - acc: 0.989 - ETA: 2s - loss: 0.0340 - acc: 0.989 - ETA: 2s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0338 - acc: 0.989 - ETA: 1s - loss: 0.0337 - acc: 0.989 - ETA: 1s - loss: 0.0337 - acc: 0.989 - ETA: 1s - loss: 0.0336 - acc: 0.989 - ETA: 1s - loss: 0.0337 - acc: 0.989 - ETA: 1s - loss: 0.0338 - acc: 0.989 - ETA: 1s - loss: 0.0338 - acc: 0.989 - ETA: 1s - loss: 0.0338 - acc: 0.989 - ETA: 1s - loss: 0.0338 - acc: 0.9898"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 1s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0340 - acc: 0.989 - ETA: 0s - loss: 0.0340 - acc: 0.989 - ETA: 0s - loss: 0.0341 - acc: 0.989 - ETA: 0s - loss: 0.0340 - acc: 0.989 - ETA: 0s - loss: 0.0340 - acc: 0.989 - ETA: 0s - loss: 0.0339 - acc: 0.989 - ETA: 0s - loss: 0.0340 - acc: 0.989 - ETA: 0s - loss: 0.0339 - acc: 0.989 - ETA: 0s - loss: 0.0338 - acc: 0.989 - ETA: 0s - loss: 0.0337 - acc: 0.989 - ETA: 0s - loss: 0.0336 - acc: 0.989 - ETA: 0s - loss: 0.0339 - acc: 0.989 - ETA: 0s - loss: 0.0338 - acc: 0.989 - ETA: 0s - loss: 0.0338 - acc: 0.989 - ETA: 0s - loss: 0.0338 - acc: 0.989 - ETA: 0s - loss: 0.0337 - acc: 0.989 - ETA: 0s - loss: 0.0336 - acc: 0.989 - ETA: 0s - loss: 0.0335 - acc: 0.989 - 14s 234us/step - loss: 0.0334 - acc: 0.9899 - val_loss: 0.0274 - val_acc: 0.9907\n",
      "Epoch 9/12\n",
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0046 - acc: 1.00 - ETA: 13s - loss: 0.0120 - acc: 0.99 - ETA: 13s - loss: 0.0098 - acc: 0.99 - ETA: 13s - loss: 0.0114 - acc: 0.99 - ETA: 13s - loss: 0.0126 - acc: 0.99 - ETA: 13s - loss: 0.0135 - acc: 0.99 - ETA: 13s - loss: 0.0187 - acc: 0.99 - ETA: 13s - loss: 0.0241 - acc: 0.99 - ETA: 13s - loss: 0.0223 - acc: 0.99 - ETA: 12s - loss: 0.0227 - acc: 0.99 - ETA: 12s - loss: 0.0219 - acc: 0.99 - ETA: 12s - loss: 0.0219 - acc: 0.99 - ETA: 12s - loss: 0.0211 - acc: 0.99 - ETA: 12s - loss: 0.0237 - acc: 0.99 - ETA: 12s - loss: 0.0280 - acc: 0.99 - ETA: 12s - loss: 0.0289 - acc: 0.99 - ETA: 12s - loss: 0.0288 - acc: 0.99 - ETA: 12s - loss: 0.0316 - acc: 0.99 - ETA: 12s - loss: 0.0313 - acc: 0.99 - ETA: 12s - loss: 0.0313 - acc: 0.99 - ETA: 12s - loss: 0.0316 - acc: 0.99 - ETA: 12s - loss: 0.0329 - acc: 0.98 - ETA: 12s - loss: 0.0328 - acc: 0.98 - ETA: 12s - loss: 0.0335 - acc: 0.98 - ETA: 12s - loss: 0.0336 - acc: 0.98 - ETA: 12s - loss: 0.0342 - acc: 0.98 - ETA: 11s - loss: 0.0336 - acc: 0.98 - ETA: 11s - loss: 0.0328 - acc: 0.99 - ETA: 11s - loss: 0.0323 - acc: 0.99 - ETA: 11s - loss: 0.0316 - acc: 0.99 - ETA: 11s - loss: 0.0316 - acc: 0.99 - ETA: 11s - loss: 0.0308 - acc: 0.99 - ETA: 11s - loss: 0.0312 - acc: 0.99 - ETA: 11s - loss: 0.0311 - acc: 0.99 - ETA: 11s - loss: 0.0312 - acc: 0.99 - ETA: 11s - loss: 0.0325 - acc: 0.99 - ETA: 11s - loss: 0.0322 - acc: 0.99 - ETA: 11s - loss: 0.0324 - acc: 0.99 - ETA: 11s - loss: 0.0325 - acc: 0.99 - ETA: 11s - loss: 0.0323 - acc: 0.99 - ETA: 11s - loss: 0.0322 - acc: 0.99 - ETA: 11s - loss: 0.0320 - acc: 0.99 - ETA: 11s - loss: 0.0317 - acc: 0.99 - ETA: 10s - loss: 0.0314 - acc: 0.99 - ETA: 10s - loss: 0.0314 - acc: 0.99 - ETA: 10s - loss: 0.0313 - acc: 0.99 - ETA: 10s - loss: 0.0311 - acc: 0.99 - ETA: 10s - loss: 0.0308 - acc: 0.99 - ETA: 10s - loss: 0.0304 - acc: 0.99 - ETA: 10s - loss: 0.0298 - acc: 0.99 - ETA: 10s - loss: 0.0296 - acc: 0.99 - ETA: 10s - loss: 0.0294 - acc: 0.99 - ETA: 10s - loss: 0.0298 - acc: 0.99 - ETA: 10s - loss: 0.0308 - acc: 0.99 - ETA: 10s - loss: 0.0306 - acc: 0.99 - ETA: 10s - loss: 0.0303 - acc: 0.99 - ETA: 10s - loss: 0.0306 - acc: 0.99 - ETA: 10s - loss: 0.0302 - acc: 0.99 - ETA: 10s - loss: 0.0304 - acc: 0.99 - ETA: 10s - loss: 0.0301 - acc: 0.99 - ETA: 9s - loss: 0.0305 - acc: 0.9908 - ETA: 9s - loss: 0.0305 - acc: 0.990 - ETA: 9s - loss: 0.0304 - acc: 0.990 - ETA: 9s - loss: 0.0308 - acc: 0.990 - ETA: 9s - loss: 0.0310 - acc: 0.990 - ETA: 9s - loss: 0.0306 - acc: 0.990 - ETA: 9s - loss: 0.0309 - acc: 0.990 - ETA: 9s - loss: 0.0307 - acc: 0.990 - ETA: 9s - loss: 0.0305 - acc: 0.990 - ETA: 9s - loss: 0.0305 - acc: 0.990 - ETA: 9s - loss: 0.0306 - acc: 0.990 - ETA: 9s - loss: 0.0305 - acc: 0.990 - ETA: 9s - loss: 0.0306 - acc: 0.990 - ETA: 9s - loss: 0.0310 - acc: 0.990 - ETA: 9s - loss: 0.0311 - acc: 0.990 - ETA: 9s - loss: 0.0311 - acc: 0.990 - ETA: 9s - loss: 0.0314 - acc: 0.990 - ETA: 9s - loss: 0.0313 - acc: 0.990 - ETA: 8s - loss: 0.0312 - acc: 0.990 - ETA: 8s - loss: 0.0309 - acc: 0.990 - ETA: 8s - loss: 0.0307 - acc: 0.990 - ETA: 8s - loss: 0.0310 - acc: 0.990 - ETA: 8s - loss: 0.0312 - acc: 0.990 - ETA: 8s - loss: 0.0312 - acc: 0.990 - ETA: 8s - loss: 0.0313 - acc: 0.990 - ETA: 8s - loss: 0.0313 - acc: 0.990 - ETA: 8s - loss: 0.0312 - acc: 0.990 - ETA: 8s - loss: 0.0313 - acc: 0.990 - ETA: 8s - loss: 0.0311 - acc: 0.990 - ETA: 8s - loss: 0.0309 - acc: 0.990 - ETA: 8s - loss: 0.0310 - acc: 0.990 - ETA: 8s - loss: 0.0310 - acc: 0.990 - ETA: 8s - loss: 0.0310 - acc: 0.990 - ETA: 8s - loss: 0.0312 - acc: 0.990 - ETA: 8s - loss: 0.0312 - acc: 0.990 - ETA: 7s - loss: 0.0311 - acc: 0.990 - ETA: 7s - loss: 0.0310 - acc: 0.990 - ETA: 7s - loss: 0.0310 - acc: 0.990 - ETA: 7s - loss: 0.0314 - acc: 0.990 - ETA: 7s - loss: 0.0313 - acc: 0.990 - ETA: 7s - loss: 0.0314 - acc: 0.989 - ETA: 7s - loss: 0.0314 - acc: 0.990 - ETA: 7s - loss: 0.0313 - acc: 0.990 - ETA: 7s - loss: 0.0317 - acc: 0.989 - ETA: 7s - loss: 0.0318 - acc: 0.989 - ETA: 7s - loss: 0.0319 - acc: 0.989 - ETA: 7s - loss: 0.0317 - acc: 0.989 - ETA: 7s - loss: 0.0316 - acc: 0.989 - ETA: 7s - loss: 0.0319 - acc: 0.989 - ETA: 7s - loss: 0.0317 - acc: 0.989 - ETA: 7s - loss: 0.0322 - acc: 0.989 - ETA: 7s - loss: 0.0322 - acc: 0.989 - ETA: 7s - loss: 0.0321 - acc: 0.989 - ETA: 6s - loss: 0.0322 - acc: 0.989 - ETA: 6s - loss: 0.0320 - acc: 0.989 - ETA: 6s - loss: 0.0322 - acc: 0.989 - ETA: 6s - loss: 0.0321 - acc: 0.989 - ETA: 6s - loss: 0.0323 - acc: 0.989 - ETA: 6s - loss: 0.0324 - acc: 0.989 - ETA: 6s - loss: 0.0323 - acc: 0.989 - ETA: 6s - loss: 0.0322 - acc: 0.989 - ETA: 6s - loss: 0.0325 - acc: 0.989 - ETA: 6s - loss: 0.0325 - acc: 0.989 - ETA: 6s - loss: 0.0326 - acc: 0.989 - ETA: 6s - loss: 0.0325 - acc: 0.989 - ETA: 6s - loss: 0.0326 - acc: 0.989 - ETA: 6s - loss: 0.0325 - acc: 0.989 - ETA: 6s - loss: 0.0323 - acc: 0.989 - ETA: 6s - loss: 0.0323 - acc: 0.989 - ETA: 6s - loss: 0.0322 - acc: 0.989 - ETA: 6s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0322 - acc: 0.989 - ETA: 5s - loss: 0.0322 - acc: 0.989 - ETA: 5s - loss: 0.0322 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0322 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0326 - acc: 0.989 - ETA: 5s - loss: 0.0325 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0324 - acc: 0.989 - ETA: 5s - loss: 0.0322 - acc: 0.989 - ETA: 5s - loss: 0.0321 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0322 - acc: 0.989 - ETA: 4s - loss: 0.0322 - acc: 0.989 - ETA: 4s - loss: 0.0321 - acc: 0.989 - ETA: 4s - loss: 0.0321 - acc: 0.989 - ETA: 4s - loss: 0.0320 - acc: 0.989 - ETA: 4s - loss: 0.0320 - acc: 0.989 - ETA: 4s - loss: 0.0320 - acc: 0.989 - ETA: 4s - loss: 0.0320 - acc: 0.989 - ETA: 4s - loss: 0.0320 - acc: 0.989 - ETA: 4s - loss: 0.0321 - acc: 0.989 - ETA: 4s - loss: 0.0321 - acc: 0.989 - ETA: 4s - loss: 0.0321 - acc: 0.989 - ETA: 4s - loss: 0.0320 - acc: 0.989 - ETA: 4s - loss: 0.0320 - acc: 0.989 - ETA: 4s - loss: 0.0319 - acc: 0.989 - ETA: 4s - loss: 0.0321 - acc: 0.989 - ETA: 4s - loss: 0.0324 - acc: 0.989 - ETA: 4s - loss: 0.0326 - acc: 0.989 - ETA: 3s - loss: 0.0326 - acc: 0.989 - ETA: 3s - loss: 0.0327 - acc: 0.989 - ETA: 3s - loss: 0.0329 - acc: 0.989 - ETA: 3s - loss: 0.0328 - acc: 0.989 - ETA: 3s - loss: 0.0328 - acc: 0.989 - ETA: 3s - loss: 0.0328 - acc: 0.989 - ETA: 3s - loss: 0.0327 - acc: 0.989 - ETA: 3s - loss: 0.0326 - acc: 0.989 - ETA: 3s - loss: 0.0328 - acc: 0.989 - ETA: 3s - loss: 0.0328 - acc: 0.989 - ETA: 3s - loss: 0.0328 - acc: 0.989 - ETA: 3s - loss: 0.0327 - acc: 0.989 - ETA: 3s - loss: 0.0326 - acc: 0.989 - ETA: 3s - loss: 0.0325 - acc: 0.989 - ETA: 3s - loss: 0.0326 - acc: 0.989 - ETA: 3s - loss: 0.0325 - acc: 0.989 - ETA: 3s - loss: 0.0325 - acc: 0.989 - ETA: 2s - loss: 0.0327 - acc: 0.989 - ETA: 2s - loss: 0.0327 - acc: 0.989 - ETA: 2s - loss: 0.0326 - acc: 0.989 - ETA: 2s - loss: 0.0325 - acc: 0.989 - ETA: 2s - loss: 0.0324 - acc: 0.989 - ETA: 2s - loss: 0.0324 - acc: 0.989 - ETA: 2s - loss: 0.0323 - acc: 0.989 - ETA: 2s - loss: 0.0323 - acc: 0.989 - ETA: 2s - loss: 0.0323 - acc: 0.989 - ETA: 2s - loss: 0.0323 - acc: 0.989 - ETA: 2s - loss: 0.0324 - acc: 0.989 - ETA: 2s - loss: 0.0325 - acc: 0.989 - ETA: 2s - loss: 0.0324 - acc: 0.989 - ETA: 2s - loss: 0.0325 - acc: 0.989 - ETA: 2s - loss: 0.0325 - acc: 0.989 - ETA: 2s - loss: 0.0324 - acc: 0.989 - ETA: 2s - loss: 0.0324 - acc: 0.989 - ETA: 2s - loss: 0.0324 - acc: 0.989 - ETA: 1s - loss: 0.0324 - acc: 0.989 - ETA: 1s - loss: 0.0324 - acc: 0.989 - ETA: 1s - loss: 0.0324 - acc: 0.989 - ETA: 1s - loss: 0.0325 - acc: 0.989 - ETA: 1s - loss: 0.0325 - acc: 0.989 - ETA: 1s - loss: 0.0324 - acc: 0.989 - ETA: 1s - loss: 0.0324 - acc: 0.989 - ETA: 1s - loss: 0.0323 - acc: 0.989 - ETA: 1s - loss: 0.0323 - acc: 0.989 - ETA: 1s - loss: 0.0322 - acc: 0.989 - ETA: 1s - loss: 0.0322 - acc: 0.989 - ETA: 1s - loss: 0.0321 - acc: 0.989 - ETA: 1s - loss: 0.0321 - acc: 0.989 - ETA: 1s - loss: 0.0321 - acc: 0.989460000/60000 [==============================] - ETA: 1s - loss: 0.0322 - acc: 0.989 - ETA: 1s - loss: 0.0323 - acc: 0.989 - ETA: 0s - loss: 0.0322 - acc: 0.989 - ETA: 0s - loss: 0.0322 - acc: 0.989 - ETA: 0s - loss: 0.0321 - acc: 0.989 - ETA: 0s - loss: 0.0320 - acc: 0.989 - ETA: 0s - loss: 0.0321 - acc: 0.989 - ETA: 0s - loss: 0.0324 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0324 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0324 - acc: 0.989 - ETA: 0s - loss: 0.0327 - acc: 0.989 - ETA: 0s - loss: 0.0326 - acc: 0.989 - 14s 241us/step - loss: 0.0327 - acc: 0.9894 - val_loss: 0.0281 - val_acc: 0.9910\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0215 - acc: 0.99 - ETA: 13s - loss: 0.0460 - acc: 0.98 - ETA: 13s - loss: 0.0475 - acc: 0.98 - ETA: 13s - loss: 0.0402 - acc: 0.98 - ETA: 13s - loss: 0.0394 - acc: 0.98 - ETA: 13s - loss: 0.0355 - acc: 0.98 - ETA: 13s - loss: 0.0351 - acc: 0.98 - ETA: 13s - loss: 0.0352 - acc: 0.98 - ETA: 13s - loss: 0.0342 - acc: 0.98 - ETA: 12s - loss: 0.0325 - acc: 0.98 - ETA: 12s - loss: 0.0339 - acc: 0.98 - ETA: 12s - loss: 0.0334 - acc: 0.98 - ETA: 12s - loss: 0.0317 - acc: 0.98 - ETA: 12s - loss: 0.0306 - acc: 0.98 - ETA: 12s - loss: 0.0297 - acc: 0.98 - ETA: 12s - loss: 0.0289 - acc: 0.98 - ETA: 12s - loss: 0.0298 - acc: 0.99 - ETA: 12s - loss: 0.0293 - acc: 0.99 - ETA: 12s - loss: 0.0289 - acc: 0.98 - ETA: 12s - loss: 0.0280 - acc: 0.99 - ETA: 12s - loss: 0.0272 - acc: 0.99 - ETA: 12s - loss: 0.0268 - acc: 0.99 - ETA: 12s - loss: 0.0270 - acc: 0.99 - ETA: 12s - loss: 0.0261 - acc: 0.99 - ETA: 12s - loss: 0.0258 - acc: 0.99 - ETA: 12s - loss: 0.0290 - acc: 0.99 - ETA: 11s - loss: 0.0294 - acc: 0.99 - ETA: 11s - loss: 0.0290 - acc: 0.99 - ETA: 11s - loss: 0.0295 - acc: 0.99 - ETA: 11s - loss: 0.0298 - acc: 0.99 - ETA: 11s - loss: 0.0297 - acc: 0.99 - ETA: 11s - loss: 0.0304 - acc: 0.99 - ETA: 11s - loss: 0.0301 - acc: 0.99 - ETA: 11s - loss: 0.0302 - acc: 0.99 - ETA: 11s - loss: 0.0298 - acc: 0.99 - ETA: 11s - loss: 0.0313 - acc: 0.99 - ETA: 11s - loss: 0.0312 - acc: 0.98 - ETA: 11s - loss: 0.0310 - acc: 0.99 - ETA: 11s - loss: 0.0309 - acc: 0.99 - ETA: 11s - loss: 0.0311 - acc: 0.99 - ETA: 11s - loss: 0.0316 - acc: 0.99 - ETA: 11s - loss: 0.0320 - acc: 0.98 - ETA: 11s - loss: 0.0322 - acc: 0.98 - ETA: 11s - loss: 0.0324 - acc: 0.98 - ETA: 10s - loss: 0.0324 - acc: 0.98 - ETA: 10s - loss: 0.0320 - acc: 0.98 - ETA: 10s - loss: 0.0317 - acc: 0.98 - ETA: 10s - loss: 0.0313 - acc: 0.99 - ETA: 10s - loss: 0.0309 - acc: 0.99 - ETA: 10s - loss: 0.0311 - acc: 0.99 - ETA: 10s - loss: 0.0319 - acc: 0.98 - ETA: 10s - loss: 0.0327 - acc: 0.99 - ETA: 10s - loss: 0.0325 - acc: 0.99 - ETA: 10s - loss: 0.0321 - acc: 0.99 - ETA: 10s - loss: 0.0318 - acc: 0.99 - ETA: 10s - loss: 0.0318 - acc: 0.99 - ETA: 10s - loss: 0.0319 - acc: 0.99 - ETA: 10s - loss: 0.0323 - acc: 0.99 - ETA: 10s - loss: 0.0325 - acc: 0.99 - ETA: 10s - loss: 0.0321 - acc: 0.99 - ETA: 10s - loss: 0.0323 - acc: 0.99 - ETA: 9s - loss: 0.0320 - acc: 0.9903 - ETA: 9s - loss: 0.0320 - acc: 0.990 - ETA: 9s - loss: 0.0316 - acc: 0.990 - ETA: 9s - loss: 0.0313 - acc: 0.990 - ETA: 9s - loss: 0.0313 - acc: 0.990 - ETA: 9s - loss: 0.0310 - acc: 0.990 - ETA: 9s - loss: 0.0308 - acc: 0.990 - ETA: 9s - loss: 0.0306 - acc: 0.990 - ETA: 9s - loss: 0.0307 - acc: 0.990 - ETA: 9s - loss: 0.0306 - acc: 0.990 - ETA: 9s - loss: 0.0303 - acc: 0.990 - ETA: 9s - loss: 0.0299 - acc: 0.990 - ETA: 9s - loss: 0.0298 - acc: 0.990 - ETA: 9s - loss: 0.0296 - acc: 0.990 - ETA: 9s - loss: 0.0294 - acc: 0.990 - ETA: 9s - loss: 0.0296 - acc: 0.990 - ETA: 9s - loss: 0.0296 - acc: 0.990 - ETA: 8s - loss: 0.0295 - acc: 0.990 - ETA: 8s - loss: 0.0296 - acc: 0.990 - ETA: 8s - loss: 0.0296 - acc: 0.990 - ETA: 8s - loss: 0.0294 - acc: 0.990 - ETA: 8s - loss: 0.0292 - acc: 0.991 - ETA: 8s - loss: 0.0293 - acc: 0.991 - ETA: 8s - loss: 0.0291 - acc: 0.991 - ETA: 8s - loss: 0.0293 - acc: 0.991 - ETA: 8s - loss: 0.0291 - acc: 0.991 - ETA: 8s - loss: 0.0288 - acc: 0.991 - ETA: 8s - loss: 0.0286 - acc: 0.991 - ETA: 8s - loss: 0.0286 - acc: 0.991 - ETA: 8s - loss: 0.0285 - acc: 0.991 - ETA: 8s - loss: 0.0293 - acc: 0.991 - ETA: 8s - loss: 0.0293 - acc: 0.991 - ETA: 8s - loss: 0.0291 - acc: 0.991 - ETA: 8s - loss: 0.0290 - acc: 0.991 - ETA: 8s - loss: 0.0293 - acc: 0.991 - ETA: 7s - loss: 0.0295 - acc: 0.991 - ETA: 7s - loss: 0.0293 - acc: 0.991 - ETA: 7s - loss: 0.0292 - acc: 0.991 - ETA: 7s - loss: 0.0293 - acc: 0.991 - ETA: 7s - loss: 0.0291 - acc: 0.991 - ETA: 7s - loss: 0.0300 - acc: 0.990 - ETA: 7s - loss: 0.0300 - acc: 0.990 - ETA: 7s - loss: 0.0298 - acc: 0.990 - ETA: 7s - loss: 0.0296 - acc: 0.991 - ETA: 7s - loss: 0.0296 - acc: 0.990 - ETA: 7s - loss: 0.0296 - acc: 0.990 - ETA: 7s - loss: 0.0296 - acc: 0.990 - ETA: 7s - loss: 0.0297 - acc: 0.990 - ETA: 7s - loss: 0.0297 - acc: 0.990 - ETA: 7s - loss: 0.0297 - acc: 0.990 - ETA: 7s - loss: 0.0296 - acc: 0.990 - ETA: 7s - loss: 0.0296 - acc: 0.990 - ETA: 6s - loss: 0.0296 - acc: 0.991 - ETA: 6s - loss: 0.0294 - acc: 0.991 - ETA: 6s - loss: 0.0293 - acc: 0.991 - ETA: 6s - loss: 0.0292 - acc: 0.991 - ETA: 6s - loss: 0.0292 - acc: 0.991 - ETA: 6s - loss: 0.0291 - acc: 0.991 - ETA: 6s - loss: 0.0290 - acc: 0.991 - ETA: 6s - loss: 0.0289 - acc: 0.991 - ETA: 6s - loss: 0.0288 - acc: 0.991 - ETA: 6s - loss: 0.0289 - acc: 0.991 - ETA: 6s - loss: 0.0288 - acc: 0.991 - ETA: 6s - loss: 0.0288 - acc: 0.991 - ETA: 6s - loss: 0.0288 - acc: 0.991 - ETA: 6s - loss: 0.0287 - acc: 0.991 - ETA: 6s - loss: 0.0287 - acc: 0.991 - ETA: 6s - loss: 0.0291 - acc: 0.990 - ETA: 6s - loss: 0.0293 - acc: 0.990 - ETA: 5s - loss: 0.0293 - acc: 0.990 - ETA: 5s - loss: 0.0293 - acc: 0.990 - ETA: 5s - loss: 0.0293 - acc: 0.990 - ETA: 5s - loss: 0.0294 - acc: 0.990 - ETA: 5s - loss: 0.0293 - acc: 0.990 - ETA: 5s - loss: 0.0292 - acc: 0.991 - ETA: 5s - loss: 0.0290 - acc: 0.991 - ETA: 5s - loss: 0.0294 - acc: 0.990 - ETA: 5s - loss: 0.0293 - acc: 0.990 - ETA: 5s - loss: 0.0292 - acc: 0.991 - ETA: 5s - loss: 0.0291 - acc: 0.991 - ETA: 5s - loss: 0.0291 - acc: 0.991 - ETA: 5s - loss: 0.0291 - acc: 0.991 - ETA: 5s - loss: 0.0291 - acc: 0.991 - ETA: 5s - loss: 0.0293 - acc: 0.990 - ETA: 5s - loss: 0.0292 - acc: 0.991 - ETA: 5s - loss: 0.0292 - acc: 0.991 - ETA: 5s - loss: 0.0292 - acc: 0.990 - ETA: 4s - loss: 0.0292 - acc: 0.990 - ETA: 4s - loss: 0.0293 - acc: 0.990 - ETA: 4s - loss: 0.0292 - acc: 0.990 - ETA: 4s - loss: 0.0291 - acc: 0.990 - ETA: 4s - loss: 0.0291 - acc: 0.991 - ETA: 4s - loss: 0.0290 - acc: 0.991 - ETA: 4s - loss: 0.0289 - acc: 0.991 - ETA: 4s - loss: 0.0288 - acc: 0.991 - ETA: 4s - loss: 0.0287 - acc: 0.991 - ETA: 4s - loss: 0.0286 - acc: 0.991 - ETA: 4s - loss: 0.0285 - acc: 0.991 - ETA: 4s - loss: 0.0284 - acc: 0.991 - ETA: 4s - loss: 0.0286 - acc: 0.991 - ETA: 4s - loss: 0.0286 - acc: 0.991 - ETA: 4s - loss: 0.0287 - acc: 0.991 - ETA: 4s - loss: 0.0287 - acc: 0.991 - ETA: 4s - loss: 0.0287 - acc: 0.991 - ETA: 3s - loss: 0.0286 - acc: 0.991 - ETA: 3s - loss: 0.0286 - acc: 0.991 - ETA: 3s - loss: 0.0286 - acc: 0.991 - ETA: 3s - loss: 0.0285 - acc: 0.991 - ETA: 3s - loss: 0.0285 - acc: 0.991 - ETA: 3s - loss: 0.0284 - acc: 0.991 - ETA: 3s - loss: 0.0285 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0287 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0289 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0289 - acc: 0.991 - ETA: 2s - loss: 0.0288 - acc: 0.991 - ETA: 2s - loss: 0.0288 - acc: 0.991 - ETA: 2s - loss: 0.0290 - acc: 0.991 - ETA: 2s - loss: 0.0291 - acc: 0.990 - ETA: 2s - loss: 0.0290 - acc: 0.991 - ETA: 2s - loss: 0.0292 - acc: 0.990 - ETA: 2s - loss: 0.0292 - acc: 0.990 - ETA: 2s - loss: 0.0291 - acc: 0.990 - ETA: 2s - loss: 0.0292 - acc: 0.990 - ETA: 2s - loss: 0.0292 - acc: 0.990 - ETA: 2s - loss: 0.0292 - acc: 0.990 - ETA: 2s - loss: 0.0291 - acc: 0.991 - ETA: 2s - loss: 0.0290 - acc: 0.991 - ETA: 2s - loss: 0.0289 - acc: 0.991 - ETA: 2s - loss: 0.0289 - acc: 0.991 - ETA: 2s - loss: 0.0289 - acc: 0.991 - ETA: 2s - loss: 0.0289 - acc: 0.991 - ETA: 2s - loss: 0.0292 - acc: 0.991 - ETA: 1s - loss: 0.0291 - acc: 0.991 - ETA: 1s - loss: 0.0292 - acc: 0.991 - ETA: 1s - loss: 0.0292 - acc: 0.990 - ETA: 1s - loss: 0.0292 - acc: 0.990 - ETA: 1s - loss: 0.0293 - acc: 0.990 - ETA: 1s - loss: 0.0293 - acc: 0.990 - ETA: 1s - loss: 0.0294 - acc: 0.990 - ETA: 1s - loss: 0.0294 - acc: 0.990 - ETA: 1s - loss: 0.0294 - acc: 0.990 - ETA: 1s - loss: 0.0295 - acc: 0.990 - ETA: 1s - loss: 0.0296 - acc: 0.990 - ETA: 1s - loss: 0.0297 - acc: 0.990 - ETA: 1s - loss: 0.0296 - acc: 0.990 - ETA: 1s - loss: 0.0295 - acc: 0.990 - ETA: 1s - loss: 0.0294 - acc: 0.9908"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 1s - loss: 0.0294 - acc: 0.990 - ETA: 1s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0296 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0294 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0296 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0297 - acc: 0.990 - ETA: 0s - loss: 0.0298 - acc: 0.990 - ETA: 0s - loss: 0.0297 - acc: 0.990 - ETA: 0s - loss: 0.0296 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0297 - acc: 0.990 - 14s 235us/step - loss: 0.0296 - acc: 0.9909 - val_loss: 0.0274 - val_acc: 0.9914\n",
      "Epoch 11/12\n",
      "57088/60000 [===========================>..] - ETA: 13s - loss: 0.0198 - acc: 0.99 - ETA: 13s - loss: 0.0327 - acc: 0.99 - ETA: 13s - loss: 0.0267 - acc: 0.99 - ETA: 13s - loss: 0.0243 - acc: 0.99 - ETA: 13s - loss: 0.0237 - acc: 0.99 - ETA: 13s - loss: 0.0316 - acc: 0.98 - ETA: 13s - loss: 0.0338 - acc: 0.98 - ETA: 12s - loss: 0.0310 - acc: 0.99 - ETA: 12s - loss: 0.0330 - acc: 0.99 - ETA: 12s - loss: 0.0320 - acc: 0.99 - ETA: 12s - loss: 0.0318 - acc: 0.99 - ETA: 12s - loss: 0.0305 - acc: 0.99 - ETA: 12s - loss: 0.0299 - acc: 0.99 - ETA: 12s - loss: 0.0282 - acc: 0.99 - ETA: 12s - loss: 0.0292 - acc: 0.99 - ETA: 12s - loss: 0.0286 - acc: 0.99 - ETA: 12s - loss: 0.0291 - acc: 0.99 - ETA: 12s - loss: 0.0283 - acc: 0.99 - ETA: 12s - loss: 0.0277 - acc: 0.99 - ETA: 12s - loss: 0.0277 - acc: 0.99 - ETA: 12s - loss: 0.0280 - acc: 0.99 - ETA: 12s - loss: 0.0280 - acc: 0.99 - ETA: 12s - loss: 0.0270 - acc: 0.99 - ETA: 12s - loss: 0.0263 - acc: 0.99 - ETA: 12s - loss: 0.0255 - acc: 0.99 - ETA: 11s - loss: 0.0252 - acc: 0.99 - ETA: 11s - loss: 0.0246 - acc: 0.99 - ETA: 11s - loss: 0.0249 - acc: 0.99 - ETA: 11s - loss: 0.0243 - acc: 0.99 - ETA: 11s - loss: 0.0245 - acc: 0.99 - ETA: 11s - loss: 0.0240 - acc: 0.99 - ETA: 11s - loss: 0.0240 - acc: 0.99 - ETA: 11s - loss: 0.0238 - acc: 0.99 - ETA: 11s - loss: 0.0237 - acc: 0.99 - ETA: 11s - loss: 0.0249 - acc: 0.99 - ETA: 11s - loss: 0.0249 - acc: 0.99 - ETA: 11s - loss: 0.0250 - acc: 0.99 - ETA: 11s - loss: 0.0247 - acc: 0.99 - ETA: 11s - loss: 0.0246 - acc: 0.99 - ETA: 11s - loss: 0.0249 - acc: 0.99 - ETA: 11s - loss: 0.0251 - acc: 0.99 - ETA: 11s - loss: 0.0255 - acc: 0.99 - ETA: 11s - loss: 0.0262 - acc: 0.99 - ETA: 10s - loss: 0.0265 - acc: 0.99 - ETA: 10s - loss: 0.0269 - acc: 0.99 - ETA: 10s - loss: 0.0270 - acc: 0.99 - ETA: 10s - loss: 0.0267 - acc: 0.99 - ETA: 10s - loss: 0.0268 - acc: 0.99 - ETA: 10s - loss: 0.0276 - acc: 0.99 - ETA: 10s - loss: 0.0272 - acc: 0.99 - ETA: 10s - loss: 0.0272 - acc: 0.99 - ETA: 10s - loss: 0.0269 - acc: 0.99 - ETA: 10s - loss: 0.0267 - acc: 0.99 - ETA: 10s - loss: 0.0270 - acc: 0.99 - ETA: 10s - loss: 0.0271 - acc: 0.99 - ETA: 10s - loss: 0.0284 - acc: 0.99 - ETA: 10s - loss: 0.0282 - acc: 0.99 - ETA: 10s - loss: 0.0282 - acc: 0.99 - ETA: 10s - loss: 0.0280 - acc: 0.99 - ETA: 10s - loss: 0.0282 - acc: 0.99 - ETA: 10s - loss: 0.0281 - acc: 0.99 - ETA: 9s - loss: 0.0284 - acc: 0.9903 - ETA: 9s - loss: 0.0282 - acc: 0.990 - ETA: 9s - loss: 0.0279 - acc: 0.990 - ETA: 9s - loss: 0.0279 - acc: 0.990 - ETA: 9s - loss: 0.0279 - acc: 0.990 - ETA: 9s - loss: 0.0278 - acc: 0.990 - ETA: 9s - loss: 0.0277 - acc: 0.990 - ETA: 9s - loss: 0.0284 - acc: 0.990 - ETA: 9s - loss: 0.0282 - acc: 0.990 - ETA: 9s - loss: 0.0280 - acc: 0.990 - ETA: 9s - loss: 0.0279 - acc: 0.990 - ETA: 9s - loss: 0.0279 - acc: 0.990 - ETA: 9s - loss: 0.0278 - acc: 0.990 - ETA: 9s - loss: 0.0275 - acc: 0.990 - ETA: 9s - loss: 0.0273 - acc: 0.990 - ETA: 9s - loss: 0.0278 - acc: 0.990 - ETA: 9s - loss: 0.0279 - acc: 0.990 - ETA: 8s - loss: 0.0277 - acc: 0.990 - ETA: 8s - loss: 0.0274 - acc: 0.991 - ETA: 8s - loss: 0.0278 - acc: 0.990 - ETA: 8s - loss: 0.0276 - acc: 0.990 - ETA: 8s - loss: 0.0275 - acc: 0.991 - ETA: 8s - loss: 0.0274 - acc: 0.991 - ETA: 8s - loss: 0.0271 - acc: 0.991 - ETA: 8s - loss: 0.0272 - acc: 0.991 - ETA: 8s - loss: 0.0274 - acc: 0.990 - ETA: 8s - loss: 0.0273 - acc: 0.990 - ETA: 8s - loss: 0.0272 - acc: 0.990 - ETA: 8s - loss: 0.0277 - acc: 0.990 - ETA: 8s - loss: 0.0277 - acc: 0.990 - ETA: 8s - loss: 0.0276 - acc: 0.990 - ETA: 8s - loss: 0.0276 - acc: 0.990 - ETA: 8s - loss: 0.0276 - acc: 0.990 - ETA: 8s - loss: 0.0276 - acc: 0.990 - ETA: 7s - loss: 0.0276 - acc: 0.990 - ETA: 7s - loss: 0.0276 - acc: 0.990 - ETA: 7s - loss: 0.0277 - acc: 0.990 - ETA: 7s - loss: 0.0281 - acc: 0.990 - ETA: 7s - loss: 0.0282 - acc: 0.990 - ETA: 7s - loss: 0.0282 - acc: 0.990 - ETA: 7s - loss: 0.0280 - acc: 0.990 - ETA: 7s - loss: 0.0281 - acc: 0.990 - ETA: 7s - loss: 0.0284 - acc: 0.990 - ETA: 7s - loss: 0.0287 - acc: 0.990 - ETA: 7s - loss: 0.0286 - acc: 0.990 - ETA: 7s - loss: 0.0285 - acc: 0.990 - ETA: 7s - loss: 0.0284 - acc: 0.990 - ETA: 7s - loss: 0.0282 - acc: 0.990 - ETA: 7s - loss: 0.0282 - acc: 0.990 - ETA: 7s - loss: 0.0281 - acc: 0.990 - ETA: 7s - loss: 0.0279 - acc: 0.990 - ETA: 6s - loss: 0.0278 - acc: 0.990 - ETA: 6s - loss: 0.0279 - acc: 0.990 - ETA: 6s - loss: 0.0281 - acc: 0.990 - ETA: 6s - loss: 0.0280 - acc: 0.990 - ETA: 6s - loss: 0.0278 - acc: 0.990 - ETA: 6s - loss: 0.0283 - acc: 0.990 - ETA: 6s - loss: 0.0284 - acc: 0.990 - ETA: 6s - loss: 0.0285 - acc: 0.990 - ETA: 6s - loss: 0.0287 - acc: 0.990 - ETA: 6s - loss: 0.0286 - acc: 0.990 - ETA: 6s - loss: 0.0285 - acc: 0.990 - ETA: 6s - loss: 0.0284 - acc: 0.990 - ETA: 6s - loss: 0.0284 - acc: 0.990 - ETA: 6s - loss: 0.0283 - acc: 0.990 - ETA: 6s - loss: 0.0284 - acc: 0.990 - ETA: 6s - loss: 0.0282 - acc: 0.990 - ETA: 6s - loss: 0.0283 - acc: 0.990 - ETA: 6s - loss: 0.0285 - acc: 0.990 - ETA: 5s - loss: 0.0285 - acc: 0.990 - ETA: 5s - loss: 0.0284 - acc: 0.990 - ETA: 5s - loss: 0.0286 - acc: 0.990 - ETA: 5s - loss: 0.0288 - acc: 0.990 - ETA: 5s - loss: 0.0289 - acc: 0.990 - ETA: 5s - loss: 0.0288 - acc: 0.990 - ETA: 5s - loss: 0.0289 - acc: 0.990 - ETA: 5s - loss: 0.0290 - acc: 0.990 - ETA: 5s - loss: 0.0289 - acc: 0.990 - ETA: 5s - loss: 0.0288 - acc: 0.990 - ETA: 5s - loss: 0.0289 - acc: 0.990 - ETA: 5s - loss: 0.0290 - acc: 0.990 - ETA: 5s - loss: 0.0290 - acc: 0.990 - ETA: 5s - loss: 0.0289 - acc: 0.990 - ETA: 5s - loss: 0.0291 - acc: 0.990 - ETA: 5s - loss: 0.0290 - acc: 0.990 - ETA: 4s - loss: 0.0292 - acc: 0.990 - ETA: 4s - loss: 0.0292 - acc: 0.990 - ETA: 4s - loss: 0.0292 - acc: 0.990 - ETA: 4s - loss: 0.0293 - acc: 0.990 - ETA: 4s - loss: 0.0292 - acc: 0.990 - ETA: 4s - loss: 0.0290 - acc: 0.990 - ETA: 4s - loss: 0.0289 - acc: 0.990 - ETA: 4s - loss: 0.0289 - acc: 0.990 - ETA: 4s - loss: 0.0289 - acc: 0.990 - ETA: 4s - loss: 0.0290 - acc: 0.990 - ETA: 4s - loss: 0.0290 - acc: 0.990 - ETA: 4s - loss: 0.0290 - acc: 0.990 - ETA: 4s - loss: 0.0289 - acc: 0.990 - ETA: 4s - loss: 0.0288 - acc: 0.990 - ETA: 3s - loss: 0.0287 - acc: 0.990 - ETA: 3s - loss: 0.0286 - acc: 0.990 - ETA: 3s - loss: 0.0286 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.990 - ETA: 3s - loss: 0.0287 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.990 - ETA: 3s - loss: 0.0286 - acc: 0.991 - ETA: 3s - loss: 0.0285 - acc: 0.991 - ETA: 3s - loss: 0.0284 - acc: 0.991 - ETA: 3s - loss: 0.0283 - acc: 0.991 - ETA: 3s - loss: 0.0284 - acc: 0.991 - ETA: 3s - loss: 0.0287 - acc: 0.990 - ETA: 3s - loss: 0.0287 - acc: 0.990 - ETA: 2s - loss: 0.0290 - acc: 0.990 - ETA: 2s - loss: 0.0290 - acc: 0.990 - ETA: 2s - loss: 0.0289 - acc: 0.990 - ETA: 2s - loss: 0.0289 - acc: 0.990 - ETA: 2s - loss: 0.0289 - acc: 0.990 - ETA: 2s - loss: 0.0288 - acc: 0.990 - ETA: 2s - loss: 0.0288 - acc: 0.990 - ETA: 2s - loss: 0.0287 - acc: 0.990 - ETA: 2s - loss: 0.0288 - acc: 0.990 - ETA: 2s - loss: 0.0287 - acc: 0.990 - ETA: 2s - loss: 0.0286 - acc: 0.990 - ETA: 2s - loss: 0.0286 - acc: 0.990 - ETA: 2s - loss: 0.0285 - acc: 0.990 - ETA: 2s - loss: 0.0285 - acc: 0.990 - ETA: 2s - loss: 0.0286 - acc: 0.990 - ETA: 2s - loss: 0.0285 - acc: 0.991 - ETA: 2s - loss: 0.0285 - acc: 0.991 - ETA: 1s - loss: 0.0284 - acc: 0.991 - ETA: 1s - loss: 0.0283 - acc: 0.991 - ETA: 1s - loss: 0.0282 - acc: 0.991 - ETA: 1s - loss: 0.0283 - acc: 0.991 - ETA: 1s - loss: 0.0282 - acc: 0.991 - ETA: 1s - loss: 0.0283 - acc: 0.990 - ETA: 1s - loss: 0.0284 - acc: 0.990 - ETA: 1s - loss: 0.0284 - acc: 0.990 - ETA: 1s - loss: 0.0283 - acc: 0.990 - ETA: 1s - loss: 0.0283 - acc: 0.990 - ETA: 1s - loss: 0.0283 - acc: 0.990 - ETA: 1s - loss: 0.0282 - acc: 0.990 - ETA: 1s - loss: 0.0282 - acc: 0.990 - ETA: 1s - loss: 0.0281 - acc: 0.990 - ETA: 1s - loss: 0.0280 - acc: 0.991 - ETA: 1s - loss: 0.0280 - acc: 0.991 - ETA: 1s - loss: 0.0280 - acc: 0.991 - ETA: 0s - loss: 0.0280 - acc: 0.991 - ETA: 0s - loss: 0.0280 - acc: 0.991 - ETA: 0s - loss: 0.0280 - acc: 0.991 - ETA: 0s - loss: 0.0279 - acc: 0.991 - ETA: 0s - loss: 0.0281 - acc: 0.990 - ETA: 0s - loss: 0.0281 - acc: 0.990 - ETA: 0s - loss: 0.0281 - acc: 0.990960000/60000 [==============================] - ETA: 0s - loss: 0.0281 - acc: 0.990 - ETA: 0s - loss: 0.0280 - acc: 0.990 - ETA: 0s - loss: 0.0279 - acc: 0.991 - ETA: 0s - loss: 0.0278 - acc: 0.991 - ETA: 0s - loss: 0.0279 - acc: 0.991 - ETA: 0s - loss: 0.0279 - acc: 0.991 - ETA: 0s - loss: 0.0278 - acc: 0.991 - ETA: 0s - loss: 0.0278 - acc: 0.991 - ETA: 0s - loss: 0.0277 - acc: 0.991 - ETA: 0s - loss: 0.0278 - acc: 0.991 - ETA: 0s - loss: 0.0278 - acc: 0.991 - 14s 236us/step - loss: 0.0278 - acc: 0.9910 - val_loss: 0.0297 - val_acc: 0.9917\n",
      "Epoch 12/12\n",
      "12416/60000 [=====>........................] - ETA: 16s - loss: 0.0058 - acc: 1.00 - ETA: 15s - loss: 0.0169 - acc: 0.99 - ETA: 15s - loss: 0.0198 - acc: 0.99 - ETA: 15s - loss: 0.0234 - acc: 0.99 - ETA: 15s - loss: 0.0228 - acc: 0.99 - ETA: 15s - loss: 0.0240 - acc: 0.99 - ETA: 15s - loss: 0.0230 - acc: 0.99 - ETA: 15s - loss: 0.0219 - acc: 0.99 - ETA: 15s - loss: 0.0213 - acc: 0.99 - ETA: 14s - loss: 0.0243 - acc: 0.99 - ETA: 14s - loss: 0.0232 - acc: 0.99 - ETA: 14s - loss: 0.0240 - acc: 0.99 - ETA: 14s - loss: 0.0231 - acc: 0.99 - ETA: 13s - loss: 0.0249 - acc: 0.99 - ETA: 13s - loss: 0.0239 - acc: 0.99 - ETA: 13s - loss: 0.0247 - acc: 0.99 - ETA: 13s - loss: 0.0235 - acc: 0.99 - ETA: 13s - loss: 0.0226 - acc: 0.99 - ETA: 13s - loss: 0.0217 - acc: 0.99 - ETA: 13s - loss: 0.0216 - acc: 0.99 - ETA: 13s - loss: 0.0216 - acc: 0.99 - ETA: 13s - loss: 0.0217 - acc: 0.99 - ETA: 13s - loss: 0.0215 - acc: 0.99 - ETA: 13s - loss: 0.0217 - acc: 0.99 - ETA: 13s - loss: 0.0213 - acc: 0.99 - ETA: 13s - loss: 0.0216 - acc: 0.99 - ETA: 12s - loss: 0.0214 - acc: 0.99 - ETA: 12s - loss: 0.0211 - acc: 0.99 - ETA: 12s - loss: 0.0209 - acc: 0.99 - ETA: 12s - loss: 0.0204 - acc: 0.99 - ETA: 12s - loss: 0.0209 - acc: 0.99 - ETA: 12s - loss: 0.0216 - acc: 0.99 - ETA: 12s - loss: 0.0220 - acc: 0.99 - ETA: 12s - loss: 0.0224 - acc: 0.99 - ETA: 12s - loss: 0.0231 - acc: 0.99 - ETA: 12s - loss: 0.0233 - acc: 0.99 - ETA: 12s - loss: 0.0234 - acc: 0.99 - ETA: 12s - loss: 0.0232 - acc: 0.99 - ETA: 11s - loss: 0.0232 - acc: 0.99 - ETA: 11s - loss: 0.0227 - acc: 0.99 - ETA: 11s - loss: 0.0232 - acc: 0.99 - ETA: 11s - loss: 0.0229 - acc: 0.99 - ETA: 11s - loss: 0.0226 - acc: 0.99 - ETA: 11s - loss: 0.0229 - acc: 0.99 - ETA: 11s - loss: 0.0226 - acc: 0.99 - ETA: 11s - loss: 0.0228 - acc: 0.99 - ETA: 11s - loss: 0.0232 - acc: 0.99 - ETA: 11s - loss: 0.0246 - acc: 0.99 - ETA: 11s - loss: 0.0244 - acc: 0.9923"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ea19096a25ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m           validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2986\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_shape = (28,28,1)\n",
    "batch_size = 128\n",
    "epochs = 12\n",
    "\n",
    "def net(input_shape, num_classes=10):\n",
    "    in_ = tf.keras.Input(input_shape)\n",
    "    x = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(in_)\n",
    "    x = tf.keras.layers.Conv2D(64, (3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    out_ = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return tf.keras.Model(inputs=in_,outputs=out_)\n",
    "\n",
    "model = net(input_shape)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "(60000, 28, 28, 1)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "56704/60000 [===========================>..] - ETA: 12:30 - loss: 2.3254 - acc: 0.08 - ETA: 4:18 - loss: 2.1928 - acc: 0.2005 - ETA: 2:41 - loss: 2.0791 - acc: 0.270 - ETA: 1:58 - loss: 1.9699 - acc: 0.313 - ETA: 1:34 - loss: 1.8886 - acc: 0.336 - ETA: 1:19 - loss: 1.7878 - acc: 0.377 - ETA: 1:09 - loss: 1.6999 - acc: 0.416 - ETA: 1:01 - loss: 1.6207 - acc: 0.442 - ETA: 55s - loss: 1.5702 - acc: 0.463 - ETA: 50s - loss: 1.4841 - acc: 0.49 - ETA: 47s - loss: 1.4377 - acc: 0.51 - ETA: 43s - loss: 1.3923 - acc: 0.52 - ETA: 41s - loss: 1.3445 - acc: 0.54 - ETA: 39s - loss: 1.3071 - acc: 0.55 - ETA: 36s - loss: 1.2655 - acc: 0.57 - ETA: 35s - loss: 1.2186 - acc: 0.58 - ETA: 33s - loss: 1.1705 - acc: 0.60 - ETA: 31s - loss: 1.1410 - acc: 0.61 - ETA: 30s - loss: 1.1101 - acc: 0.62 - ETA: 29s - loss: 1.0781 - acc: 0.63 - ETA: 28s - loss: 1.0577 - acc: 0.64 - ETA: 28s - loss: 1.0283 - acc: 0.65 - ETA: 27s - loss: 1.0056 - acc: 0.66 - ETA: 26s - loss: 0.9818 - acc: 0.67 - ETA: 26s - loss: 0.9615 - acc: 0.67 - ETA: 25s - loss: 0.9301 - acc: 0.68 - ETA: 24s - loss: 0.9115 - acc: 0.69 - ETA: 23s - loss: 0.8930 - acc: 0.70 - ETA: 23s - loss: 0.8705 - acc: 0.70 - ETA: 22s - loss: 0.8561 - acc: 0.71 - ETA: 22s - loss: 0.8399 - acc: 0.72 - ETA: 21s - loss: 0.8255 - acc: 0.72 - ETA: 21s - loss: 0.8118 - acc: 0.73 - ETA: 21s - loss: 0.7984 - acc: 0.73 - ETA: 20s - loss: 0.7832 - acc: 0.74 - ETA: 20s - loss: 0.7706 - acc: 0.74 - ETA: 20s - loss: 0.7603 - acc: 0.75 - ETA: 19s - loss: 0.7483 - acc: 0.75 - ETA: 19s - loss: 0.7377 - acc: 0.75 - ETA: 19s - loss: 0.7286 - acc: 0.76 - ETA: 18s - loss: 0.7180 - acc: 0.76 - ETA: 18s - loss: 0.7113 - acc: 0.76 - ETA: 18s - loss: 0.7015 - acc: 0.77 - ETA: 18s - loss: 0.6924 - acc: 0.77 - ETA: 17s - loss: 0.6840 - acc: 0.77 - ETA: 17s - loss: 0.6761 - acc: 0.77 - ETA: 17s - loss: 0.6636 - acc: 0.78 - ETA: 16s - loss: 0.6527 - acc: 0.78 - ETA: 16s - loss: 0.6455 - acc: 0.79 - ETA: 16s - loss: 0.6371 - acc: 0.79 - ETA: 16s - loss: 0.6309 - acc: 0.79 - ETA: 16s - loss: 0.6249 - acc: 0.79 - ETA: 15s - loss: 0.6200 - acc: 0.79 - ETA: 15s - loss: 0.6131 - acc: 0.80 - ETA: 15s - loss: 0.6065 - acc: 0.80 - ETA: 15s - loss: 0.6018 - acc: 0.80 - ETA: 15s - loss: 0.5963 - acc: 0.80 - ETA: 15s - loss: 0.5904 - acc: 0.80 - ETA: 14s - loss: 0.5844 - acc: 0.81 - ETA: 14s - loss: 0.5794 - acc: 0.81 - ETA: 14s - loss: 0.5745 - acc: 0.81 - ETA: 14s - loss: 0.5688 - acc: 0.81 - ETA: 14s - loss: 0.5635 - acc: 0.81 - ETA: 14s - loss: 0.5585 - acc: 0.82 - ETA: 14s - loss: 0.5528 - acc: 0.82 - ETA: 13s - loss: 0.5474 - acc: 0.82 - ETA: 13s - loss: 0.5422 - acc: 0.82 - ETA: 13s - loss: 0.5376 - acc: 0.82 - ETA: 13s - loss: 0.5326 - acc: 0.82 - ETA: 13s - loss: 0.5287 - acc: 0.83 - ETA: 13s - loss: 0.5247 - acc: 0.83 - ETA: 13s - loss: 0.5213 - acc: 0.83 - ETA: 12s - loss: 0.5164 - acc: 0.83 - ETA: 12s - loss: 0.5123 - acc: 0.83 - ETA: 12s - loss: 0.5074 - acc: 0.83 - ETA: 12s - loss: 0.5041 - acc: 0.83 - ETA: 12s - loss: 0.4999 - acc: 0.84 - ETA: 12s - loss: 0.4962 - acc: 0.84 - ETA: 12s - loss: 0.4933 - acc: 0.84 - ETA: 12s - loss: 0.4900 - acc: 0.84 - ETA: 12s - loss: 0.4860 - acc: 0.84 - ETA: 11s - loss: 0.4825 - acc: 0.84 - ETA: 11s - loss: 0.4785 - acc: 0.84 - ETA: 11s - loss: 0.4754 - acc: 0.84 - ETA: 11s - loss: 0.4719 - acc: 0.85 - ETA: 11s - loss: 0.4684 - acc: 0.85 - ETA: 11s - loss: 0.4650 - acc: 0.85 - ETA: 11s - loss: 0.4624 - acc: 0.85 - ETA: 11s - loss: 0.4598 - acc: 0.85 - ETA: 11s - loss: 0.4564 - acc: 0.85 - ETA: 10s - loss: 0.4532 - acc: 0.85 - ETA: 10s - loss: 0.4502 - acc: 0.85 - ETA: 10s - loss: 0.4476 - acc: 0.85 - ETA: 10s - loss: 0.4445 - acc: 0.85 - ETA: 10s - loss: 0.4413 - acc: 0.86 - ETA: 10s - loss: 0.4400 - acc: 0.86 - ETA: 10s - loss: 0.4375 - acc: 0.86 - ETA: 10s - loss: 0.4337 - acc: 0.86 - ETA: 10s - loss: 0.4314 - acc: 0.86 - ETA: 9s - loss: 0.4291 - acc: 0.8643 - ETA: 9s - loss: 0.4264 - acc: 0.865 - ETA: 9s - loss: 0.4240 - acc: 0.865 - ETA: 9s - loss: 0.4214 - acc: 0.866 - ETA: 9s - loss: 0.4189 - acc: 0.867 - ETA: 9s - loss: 0.4163 - acc: 0.868 - ETA: 9s - loss: 0.4142 - acc: 0.868 - ETA: 9s - loss: 0.4117 - acc: 0.869 - ETA: 9s - loss: 0.4104 - acc: 0.870 - ETA: 9s - loss: 0.4081 - acc: 0.871 - ETA: 9s - loss: 0.4060 - acc: 0.871 - ETA: 8s - loss: 0.4047 - acc: 0.872 - ETA: 8s - loss: 0.4011 - acc: 0.873 - ETA: 8s - loss: 0.3991 - acc: 0.874 - ETA: 8s - loss: 0.3972 - acc: 0.874 - ETA: 8s - loss: 0.3948 - acc: 0.875 - ETA: 8s - loss: 0.3926 - acc: 0.876 - ETA: 8s - loss: 0.3902 - acc: 0.877 - ETA: 8s - loss: 0.3883 - acc: 0.877 - ETA: 8s - loss: 0.3859 - acc: 0.878 - ETA: 8s - loss: 0.3842 - acc: 0.878 - ETA: 7s - loss: 0.3824 - acc: 0.879 - ETA: 7s - loss: 0.3812 - acc: 0.879 - ETA: 7s - loss: 0.3790 - acc: 0.880 - ETA: 7s - loss: 0.3770 - acc: 0.881 - ETA: 7s - loss: 0.3750 - acc: 0.881 - ETA: 7s - loss: 0.3728 - acc: 0.882 - ETA: 7s - loss: 0.3713 - acc: 0.883 - ETA: 7s - loss: 0.3695 - acc: 0.883 - ETA: 7s - loss: 0.3674 - acc: 0.884 - ETA: 7s - loss: 0.3661 - acc: 0.884 - ETA: 7s - loss: 0.3646 - acc: 0.885 - ETA: 7s - loss: 0.3629 - acc: 0.885 - ETA: 7s - loss: 0.3615 - acc: 0.886 - ETA: 6s - loss: 0.3600 - acc: 0.886 - ETA: 6s - loss: 0.3584 - acc: 0.887 - ETA: 6s - loss: 0.3565 - acc: 0.887 - ETA: 6s - loss: 0.3552 - acc: 0.888 - ETA: 6s - loss: 0.3536 - acc: 0.888 - ETA: 6s - loss: 0.3522 - acc: 0.889 - ETA: 6s - loss: 0.3504 - acc: 0.889 - ETA: 6s - loss: 0.3489 - acc: 0.890 - ETA: 6s - loss: 0.3481 - acc: 0.890 - ETA: 6s - loss: 0.3466 - acc: 0.890 - ETA: 6s - loss: 0.3450 - acc: 0.891 - ETA: 6s - loss: 0.3442 - acc: 0.891 - ETA: 5s - loss: 0.3428 - acc: 0.892 - ETA: 5s - loss: 0.3411 - acc: 0.892 - ETA: 5s - loss: 0.3397 - acc: 0.893 - ETA: 5s - loss: 0.3380 - acc: 0.893 - ETA: 5s - loss: 0.3369 - acc: 0.894 - ETA: 5s - loss: 0.3358 - acc: 0.894 - ETA: 5s - loss: 0.3345 - acc: 0.895 - ETA: 5s - loss: 0.3330 - acc: 0.895 - ETA: 5s - loss: 0.3316 - acc: 0.895 - ETA: 5s - loss: 0.3303 - acc: 0.896 - ETA: 5s - loss: 0.3283 - acc: 0.897 - ETA: 5s - loss: 0.3267 - acc: 0.897 - ETA: 5s - loss: 0.3256 - acc: 0.897 - ETA: 4s - loss: 0.3245 - acc: 0.898 - ETA: 4s - loss: 0.3233 - acc: 0.898 - ETA: 4s - loss: 0.3221 - acc: 0.899 - ETA: 4s - loss: 0.3212 - acc: 0.899 - ETA: 4s - loss: 0.3201 - acc: 0.899 - ETA: 4s - loss: 0.3190 - acc: 0.900 - ETA: 4s - loss: 0.3178 - acc: 0.900 - ETA: 4s - loss: 0.3163 - acc: 0.901 - ETA: 4s - loss: 0.3159 - acc: 0.901 - ETA: 4s - loss: 0.3152 - acc: 0.901 - ETA: 4s - loss: 0.3139 - acc: 0.902 - ETA: 4s - loss: 0.3128 - acc: 0.902 - ETA: 4s - loss: 0.3117 - acc: 0.902 - ETA: 3s - loss: 0.3107 - acc: 0.903 - ETA: 3s - loss: 0.3098 - acc: 0.903 - ETA: 3s - loss: 0.3081 - acc: 0.903 - ETA: 3s - loss: 0.3069 - acc: 0.904 - ETA: 3s - loss: 0.3059 - acc: 0.904 - ETA: 3s - loss: 0.3051 - acc: 0.904 - ETA: 3s - loss: 0.3039 - acc: 0.905 - ETA: 3s - loss: 0.3031 - acc: 0.905 - ETA: 3s - loss: 0.3019 - acc: 0.906 - ETA: 3s - loss: 0.3010 - acc: 0.906 - ETA: 3s - loss: 0.3002 - acc: 0.906 - ETA: 3s - loss: 0.2994 - acc: 0.906 - ETA: 3s - loss: 0.2985 - acc: 0.907 - ETA: 2s - loss: 0.2977 - acc: 0.907 - ETA: 2s - loss: 0.2969 - acc: 0.907 - ETA: 2s - loss: 0.2959 - acc: 0.908 - ETA: 2s - loss: 0.2946 - acc: 0.908 - ETA: 2s - loss: 0.2933 - acc: 0.909 - ETA: 2s - loss: 0.2922 - acc: 0.909 - ETA: 2s - loss: 0.2911 - acc: 0.909 - ETA: 2s - loss: 0.2899 - acc: 0.909 - ETA: 2s - loss: 0.2891 - acc: 0.910 - ETA: 2s - loss: 0.2882 - acc: 0.910 - ETA: 2s - loss: 0.2872 - acc: 0.910 - ETA: 2s - loss: 0.2866 - acc: 0.910 - ETA: 2s - loss: 0.2860 - acc: 0.911 - ETA: 1s - loss: 0.2850 - acc: 0.911 - ETA: 1s - loss: 0.2843 - acc: 0.911 - ETA: 1s - loss: 0.2834 - acc: 0.911 - ETA: 1s - loss: 0.2824 - acc: 0.912 - ETA: 1s - loss: 0.2816 - acc: 0.912 - ETA: 1s - loss: 0.2807 - acc: 0.912 - ETA: 1s - loss: 0.2798 - acc: 0.913 - ETA: 1s - loss: 0.2788 - acc: 0.913 - ETA: 1s - loss: 0.2783 - acc: 0.913 - ETA: 1s - loss: 0.2778 - acc: 0.913 - ETA: 1s - loss: 0.2766 - acc: 0.914 - ETA: 1s - loss: 0.2761 - acc: 0.914 - ETA: 1s - loss: 0.2754 - acc: 0.914 - ETA: 1s - loss: 0.2747 - acc: 0.914 - ETA: 0s - loss: 0.2741 - acc: 0.915 - ETA: 0s - loss: 0.2735 - acc: 0.915\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-65d86327aeec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    219\u001b[0m           \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[0mt_before_callbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[0mdelta_t_median\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;31m# will be handled by on_epoch_end.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values)\u001b[0m\n\u001b[0;32m    366\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dynamic_display\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\b'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mprev_total_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__convertor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_and_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite_and_convert\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_ansi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_plain_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite_plain_text\u001b[1;34m(self, text, start, end)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Try to use the functional API\n",
    "#tf.reset_default_graph()\n",
    "def net2(inputs_shapes, emb_size=4):\n",
    "    images_shape = inputs_shapes\n",
    "    input_image = tf.keras.Input(images_shape,name='image_input')\n",
    "    x = tf.keras.layers.Conv2D(32,(3, 3), activation='relu')(input_image)\n",
    "    x = tf.keras.layers.Conv2D(64,(3, 3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    out = tf.keras.layers.Dense(10,activation='softmax', name='out')(x)\n",
    "    return tf.keras.Model(inputs=input_image, outputs=out)\n",
    "\n",
    "w, h = SIZE\n",
    "inputs_shapes = (w, h, 1)\n",
    "model = net2(inputs_shapes)\n",
    "model.compile(tf.keras.optimizers.Adadelta(),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "print(x_train.shape)\n",
    "model.fit(x_train,y_train,batch_size=128,epochs=12,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 32,874\n",
      "Trainable params: 32,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inputs:\n",
      "[[[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]]\n",
      "[5 0 4 ... 5 6 8]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "Train outputs\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "Valid inputs\n",
      "[[[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [-0.99609375]]]]\n",
      "[7 2 1 ... 4 5 6]\n",
      "[0 0 1 ... 0 1 1]\n",
      "Valid outputs\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[0 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Check the inputs\n",
    "print(\"Train inputs:\")\n",
    "print(images_train)\n",
    "print(labels_train)\n",
    "print(issame_train_in)\n",
    "\n",
    "print(\"Train outputs\")\n",
    "print(labels_train_out)\n",
    "print(issame_train_in)\n",
    "\n",
    "print(\"Valid inputs\")\n",
    "print(pairs)\n",
    "print(labels_valid)\n",
    "print(issame_in)\n",
    "\n",
    "print(\"Valid outputs\")\n",
    "print(labels_valid_out)\n",
    "print(issame_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-625f447efafb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#validation_data = ([images_valid,issame_in], labels_valid)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#validation_data=([images_valid,labels_valid,issame_in],[labels_valid,issame_out])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'../output/logs/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m )\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    219\u001b[0m           \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[0mt_before_callbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[0mdelta_t_median\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1076\u001b[0m                   \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m                   if k not in ['batch', 'size', 'num_steps']}\n\u001b[1;32m-> 1078\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write_custom_summaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_total_batches_seen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1079\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_total_batches_seen\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_write_custom_summaries\u001b[1;34m(self, step, logs)\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_summary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[0msummary_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[0msummary_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m         \u001b[0msummary_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    #[images_train,issame_train_in],\n",
    "    [images_train,labels_train,issame_train_in],\n",
    "    #[labels_train,issame_train_out],\n",
    "    #[labels_train,issame_train_out],\n",
    "    [labels_train_out,issame_train_in],\n",
    "    epochs=20,\n",
    "    batch_size=1024,\n",
    "    validation_data = ([pairs,labels_valid,issame_in], [labels_valid_out,issame_in]),\n",
    "    #validation_data = ([images_valid,issame_in], labels_valid)\n",
    "    #validation_data=([images_valid,labels_valid,issame_in],[labels_valid,issame_out])\n",
    "    callbacks=[tf.keras.callbacks.TensorBoard(log_dir='../output/logs/')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_input (InputLayer)        (None, 100, 100, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 98, 98, 10)   280         image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 49, 49, 10)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 47, 47, 20)   1820        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 23, 23, 20)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 21, 21, 40)   7240        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 40)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 40)           0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           1312        global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "input_labels (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "issame_input (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "arcface (Arcface)               (None, 1301)         41632       dense[0][0]                      \n",
      "                                                                 input_labels[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "validation (Validation)         (None, 1)            0           dense[0][0]                      \n",
      "                                                                 issame_input[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 52,284\n",
      "Trainable params: 52,284\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it on the training/validation dataset to stop the worst examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_predict,_=model.predict([images_train[0:10],labels_train[0:10],issame_train_in[0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\guillaume\\anaconda3\\lib\\site-packages\\skimage\\io\\_plugins\\matplotlib_plugin.py:80: UserWarning: Float image out of standard range; displaying image with stretched contrast.\n",
      "  warn(\"Float image out of standard range; displaying \"\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2000f3e5d30>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import skimage as sk\n",
    "sk.io.imshow(images_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6868594, 0.7891156]], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.predict([images_test[0:1],labels_test[0:1],issame_in[0:1]])\n",
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate it on the test dataset: one shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
